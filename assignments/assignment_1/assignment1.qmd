---
title: "Assignment 1: Census Data Quality for Policy Decisions"
subtitle: "Evaluating Data Reliability for Algorithmic Decision-Making"
author: "Sujan Kakumanu"
date: 09-29-2025
format: 
  html:
    code-fold: false
    toc: true
    toc-location: left
    theme: cosmo
execute:
  warning: false
  message: false
---

# Assignment Overview

## Scenario

You are a data analyst for the **Ohio Department of Human Services**. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.

Drawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.

## Learning Objectives

- Apply dplyr functions to real census data for policy analysis
- Evaluate data quality using margins of error 
- Connect technical analysis to algorithmic decision-making
- Identify potential equity implications of data reliability issues
- Create professional documentation for policy stakeholders

## Submission Instructions

**Submit by posting your updated portfolio link on Canvas.** Your assignment should be accessible at `your-portfolio-url/assignments/assignment_1/`

Make sure to update your `_quarto.yml` navigation to include this assignment under an "Assignments" menu.

# Part 1: Portfolio Integration

Create this assignment in your portfolio repository under an `assignments/assignment_1/` folder structure. Update your navigation menu to include:

```
- text: Assignments
  menu:
    - href: assignments/assignment_1/your_file_name.qmd
      text: "Assignment 1: Census Data Exploration"
```
If there is a special character like comma, you need use double quote mark so that the quarto can identify this as text

# Setup

```{r setup}
# Load required packages (hint: you need tidycensus, tidyverse, and knitr)
library(tidyverse)
library(tidycensus)
library(knitr)

# Set your Census API key
census_api_key(Sys.getenv("MY_CENSUS_API_KEY"))

# Choose your state for analysis - assign it to a variable called my_state
my_state <- "OH"
```

**State Selection:** I have chosen **Ohio** for this analysis because it is my home state! I spent most of my life there and although I am excited for my new chapter in Philadelphia, the state of Ohio will always have a place in my heart.

# Part 2: County-Level Resource Assessment

## 2.1 Data Retrieval

**Your Task:** Use `get_acs()` to retrieve county-level data for your chosen state.

**Requirements:**
- Geography: county level
- Variables: median household income (B19013_001) and total population (B01003_001)  
- Year: 2022
- Survey: acs5
- Output format: wide

**Hint:** Remember to give your variables descriptive names using the `variables = c(name = "code")` syntax.

```{r county-data}
# Write your get_acs() code here
county_data <- get_acs(
  geography = "county",
  variables = c(
    median_income = "B19013_001",   # Median household income
    total_pop = "B01003_001"       # Total population
  ),
  state = my_state,
  year = 2022,
  output = "wide"
)

# Clean the county names to remove state name and "County" 
words_to_remove <- c("County", "Ohio", ', ', ' ')
pattern <- paste0("\\b(", paste(words_to_remove, collapse = "|"), ")\\b")

county_data <- county_data %>%
  mutate(county_name = str_remove_all(NAME, pattern))

# Display the first few rows
glimpse(county_data)
```

## 2.2 Data Quality Assessment

**Your Task:** Calculate margin of error percentages and create reliability categories.

**Requirements:**
- Calculate MOE percentage: (margin of error / estimate) * 100
- Create reliability categories:
  - High Confidence: MOE < 5%
  - Moderate Confidence: MOE 5-10%  
  - Low Confidence: MOE > 10%
- Create a flag for unreliable estimates (MOE > 10%)

**Hint:** Use `mutate()` with `case_when()` for the categories.

```{r income-reliability}
# Calculate MOE percentage and reliability categories using mutate()
# Calculate MOE percentages
county_data <- county_data %>%
  mutate(
      median_income_moe_pct = (median_incomeM / median_incomeE) * 100,
  )

county_data <- county_data %>%
  mutate(
    median_income_moe_cat = case_when(
      median_income_moe_pct > 10 ~ "Low Confidence: MOE > 10%",
      median_income_moe_pct >= 5 & median_income_moe_pct <= 10 ~ "Moderate Confidence: MOE 5-10%",
      TRUE ~ "High Confidence: MOE < 5%"
    ),
  )

# Create a summary showing count of counties in each reliability category
# Hint: use count() and mutate() to add percentages

moe_cat_summary_table <- county_data %>%
  count(median_income_moe_cat, name = "number_of_counties")

moe_cat_summary_table
```

## 2.3 High Uncertainty Counties

**Your Task:** Identify the 5 counties with the highest MOE percentages.

**Requirements:**
- Sort by MOE percentage (highest first)
- Select the top 5 counties
- Display: county name, median income, margin of error, MOE percentage, reliability category
- Format as a professional table using `kable()`

**Hint:** Use `arrange()`, `slice()`, and `select()` functions.

```{r high-uncertainty}
# Create table of top 5 counties by MOE percentage
top_moe_counties <- county_data %>%
   slice_max(median_income_moe_pct, n = 5) %>%
    select(county_name, median_incomeE, median_incomeM, median_income_moe_pct, median_income_moe_cat)

# Format as table with kable() - include appropriate column names and caption
kable(
  top_moe_counties,
  caption = "Top 5 Counties in Ohio by Median Income Margin of Error (MOE)",
  col.names = c("County Name", "Median Income Estimate $", "Median Income MOE $", "Median Income MOE %", "Median Income MOE Category")
)
```

**Data Quality Commentary:**

These results are not surprising given that the counties are among the least populous in the state (Vinton, Noble, and Monroe have less than 15,000 people according to 2024 estimates). Since the ACS relies on sample data, rather than the total enumeration provided by the Census, small population groups will result in small sample sizes. 

In this scenario: Any algorithmic system should account for the data inaccuracies arising from Ohio's smaller counties.

# Part 3: Neighborhood-Level Analysis

## 3.1 Focus Area Selection

**Your Task:** Select 2-3 counties from your reliability analysis for detailed tract-level study.

**Strategy:** Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.

```{r select-counties}
# Use filter() to select 2-3 counties from your county_reliability data
# Store the selected counties in a variable called selected_counties
selected_counties <- county_data %>%
  filter(county_name %in% c("Cuyahoga", "Marion", "Vinton"))

# Display the selected counties with their key characteristics
# Show: county name, median income, MOE percentage, reliability category
kable(
  caption = "Selected Counties (1 in each reliability category)",
  selected_counties %>% select(county_name, median_incomeE, median_income_moe_pct, median_income_moe_cat),
  col.names = c("County Name", "Median Income Estimate $", "Median Income MOE %", "Median Income MOE Category")
)
```

**Comment on the output:** Cuyahoga county is one of the most populous counties in Ohio, so the sample sizes used lent to its low MOE of ~1.16%. Although this **can not** be extrapolated to all counties, it is interesting to see a decline of median income as the MOE increases among the selected counties. 

## 3.2 Tract-Level Demographics

**Your Task:** Get demographic data for census tracts in your selected counties.

**Requirements:**
- Geography: tract level
- Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001)
- Use the same state and year as before
- Output format: wide
- **Challenge:** You'll need county codes, not names. Look at the GEOID patterns in your county data for hints.

```{r tract-demographics}
# Define your race/ethnicity variables with descriptive names
race_vars <- c(
  white    = "B03002_003",
  black    = "B03002_004",
  hispanic = "B03002_012",
  total    = "B03002_001"
)

# Use get_acs() to retrieve tract-level data
# Hint: You may need to specify county codes in the county parameter

# Based on https://www.census.gov/programs-surveys/geography/guidance/geo-identifiers.html,
# GEOIDs for counties are made of two digit state codes and three digit county codes. This is visible
# in how GEOID in county_data stays constant for the first two digits, and is unique for the last 3.
# We can parse this and create a list:
selected_counties <- selected_counties %>%
  mutate(county_code = str_sub(GEOID, 3, 5))

county_codes <- selected_counties %>% pull(county_code)

tract_demo_selected_counties = get_acs(
  geography = "tract",
  state     = my_state,
  county    = county_codes,
  variables = race_vars,
  output    = "wide"
)

# Calculate percentage of each group using mutate()
# Create percentages for white, Black, and Hispanic populations
tract_demo_selected_counties <- tract_demo_selected_counties %>%
  mutate(
    pct_white    = 100 * whiteE    / totalE,
    pct_black    = 100 * blackE    / totalE,
    pct_hispanic = 100 * hispanicE / totalE
  )

# Add readable tract and county name columns using str_extract() or similar
tract_demo_selected_counties <- tract_demo_selected_counties %>%
  mutate(
    tract_name  = str_extract(NAME, "^[^;]+"),
    county_name = str_extract(NAME, "(?<=;\\s).*")
  )

# We can clean up the county name as we did before:
words_to_remove <- c("County", "Ohio", '; ', ' ')
pattern <- paste0("\\b(", paste(words_to_remove, collapse = "|"), ")\\b")
tract_demo_selected_counties <- tract_demo_selected_counties %>%
  mutate(county_name = str_remove_all(county_name, pattern))
```

## 3.3 Demographic Analysis

**Your Task:** Analyze the demographic patterns in your selected areas.

```{r demographic-analysis}
# Find the tract with the highest percentage of Hispanic/Latino residents
# Hint: use arrange() and slice() to get the top tract
top_hispanic <- tract_demo_selected_counties %>%
  slice_max(order_by = pct_hispanic, n = 1)

top_hispanic %>%
  select(county_name, tract_name, pct_white, pct_black, pct_hispanic) %>%
  kable(
    caption = "Details of Census Tract with Highest Percentage Hispanic Residents",
    col.names = c("County Name", "Tract", "% White", "% Black", "% Hispanic")
  )

# Calculate average demographics by county using group_by() and summarize()
# Show: number of tracts, average percentage for each racial/ethnic group

selected_counties_summary <- tract_demo_selected_counties %>%
  group_by(county_name) %>%
  summarize(
    n_of_tracts      = n(),
    avg_pct_white = mean(pct_white,    na.rm = TRUE),
    avg_pct_black = mean(pct_black,    na.rm = TRUE),
    avg_pct_hisp  = mean(pct_hispanic, na.rm = TRUE),
  )

# Create a nicely formatted table of your results using kable()
selected_counties_summary %>%
  kable(
    caption = "Average Demographics for Selected Ohio Counties",
    digits = 2,
    col.names = c("County Name", "Number of Tracts", "Avg % White", "Avg % Black", "Avg % Hispanic")
  )

```

# Part 4: Comprehensive Data Quality Evaluation

## 4.1 MOE Analysis for Demographic Variables

**Your Task:** Examine margins of error for demographic variables to see if some communities have less reliable data.

**Requirements:**
- Calculate MOE percentages for each demographic variable
- Flag tracts where any demographic variable has MOE > 15%
- Create summary statistics

```{r demographic-moe}
# Calculate MOE percentages for white, Black, and Hispanic variables
# Hint: use the same formula as before (margin/estimate * 100)

# I noticed some of the estimates or margins were 0s, or NaNs, so I'm accounting for that here as well:
tract_demo_selected_counties <- tract_demo_selected_counties %>%
  mutate(
    white_moe_pct    = if_else(whiteE    > 0, (whiteM    / whiteE), NA_real_) * 100,
    black_moe_pct    = if_else(blackE    > 0, (blackM    / blackE), NA_real_) * 100,
    hispanic_moe_pct = if_else(hispanicE > 0, (hispanicM / hispanicE), NA_real_) * 100
  )

# Create a flag for tracts with high MOE on any demographic variable
# Use logical operators (| for OR) in an ifelse() statement
tract_demo_selected_counties <- tract_demo_selected_counties %>%
  mutate(
    high_moe_flag = ifelse(
      white_moe_pct > 15 | black_moe_pct > 15 | hispanic_moe_pct > 15,
      TRUE,
      FALSE
    )
  )

# Create summary statistics showing how many tracts have data quality issues

tract_demo_selected_counties %>%
  group_by(county_name) %>%
  summarize(
    total_tracts    = sum(!is.na(high_moe_flag), na.rm = TRUE),
    high_moe_tracts = sum(high_moe_flag, na.rm = TRUE),
  ) %>%
  kable(
    caption = "Tract Data Quality by County",
    digits = 0,
    col.names = c("County Name", "Number of Tracts (excluding those without data)", "Tracts with Quality Issues (>15% MOE)")
  )
```
I noticed that there were some tracts with no data, so the sum of tracts excludes those.

## 4.2 Pattern Analysis

**Your Task:** Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.

```{r pattern-analysis}
# Group tracts by whether they have high MOE issues
# Calculate average characteristics for each group:
# - population size, demographic percentages

# Use group_by() and summarize() to create this comparison
# Create a professional table showing the patterns

tract_demo_selected_counties %>%
  group_by(high_moe_flag) %>%
  summarize(
    avg_pct_white  = mean(pct_white, na.rm = TRUE),
    avg_pct_black  = mean(pct_black, na.rm = TRUE),
    avg_pct_hisp   = mean(pct_hispanic, na.rm = TRUE),
    n_tracts       = n(),
    .groups = "drop"
  ) %>%
  kable(
    caption = "Average Demographics by High MOE Status",
    digits = 1,
    col.names = c(
      "High MOE Flag",
      "Avg % White",
      "Avg % Black",
      "Avg % Hispanic",
      "Number of Tracts"
    )
  )
```
This summary was simple, but when I did a visual inspection of the data, I see there's some patterns that this table analysis is obscuring. I see that the MOE for Hispanic residents is above 15% for all tracts! I'll do a second summary table, this time showing 

```{r pattern-analysis-2}
tract_demo_selected_counties %>%
  summarize(
    n_tracts          = n(),
    white_high_moe    = sum(white_moe_pct    > 15, na.rm = TRUE),
    black_high_moe    = sum(black_moe_pct    > 15, na.rm = TRUE),
    hispanic_high_moe = sum(hispanic_moe_pct > 15, na.rm = TRUE),
    pct_white_high    = 100 * white_high_moe    / n_tracts,
    pct_black_high    = 100 * black_high_moe    / n_tracts,
    pct_hispanic_high = 100 * hispanic_high_moe / n_tracts
  ) %>%
  kable(
    caption = "Tracts with MOE > 15% by Demographic Group",
    digits = 1,
    col.names = c(
      "Total Tracts",
      "White High MOE Tracts",
      "Black High MOE Tracts",
      "Hispanic High MOE Tracts",
      "% of Tracts with White High MOE",
      "% of Tracts with Black High MOE",
      "% of Tracts with Hispanic High MOE"
    )
  )
```

```{r pattern-analysis-3}
tract_demo_selected_counties %>%
  group_by(county_name) %>%
  summarize(
    n_tracts          = n(),
    white_high_moe    = sum(white_moe_pct    > 15, na.rm = TRUE),
    black_high_moe    = sum(black_moe_pct    > 15, na.rm = TRUE),
    hispanic_high_moe = sum(hispanic_moe_pct > 15, na.rm = TRUE),
    pct_white_high    = 100 * white_high_moe    / n_tracts,
    pct_black_high    = 100 * black_high_moe    / n_tracts,
    pct_hispanic_high = 100 * hispanic_high_moe / n_tracts,
    .groups = "drop"
  ) %>%
  kable(
    caption = "County-Level Breakdown of Tracts with MOE > 15%, by Demographic Group",
    digits = 1,
    col.names = c(
      "County",
      "Total Tracts",
      "White High MOE Tracts",
      "Black High MOE Tracts",
      "Hispanic High MOE Tracts",
      "% of Tracts with White High MOE",
      "% of Tracts with Black High MOE",
      "% of Tracts with Hispanic High MOE"
    )
  )
```

**Pattern Analysis:** A larger share of census tracts flagged as High MOE (>15%) are driven primarily by high median income margins of error among Black or Hispanic populations. The last chart highlights that, with both communities having a higher MOE as a percentage of tracts being marked.

For Marion and Vinton counties, this is potentially explained by Black and Hispanic populations being a smaller percentage of the total. This would make the sample sizes for both communities fairly small in these counties. In Cuyahoga county, a visual inspection reveals tracts being flagged when they have small percentages of Hispanic, Black, or White population.


# Part 5: Policy Recommendations

## 5.1 Analysis Integration and Professional Summary

**Your Task:** Write an executive summary that integrates findings from all four analyses.

**Executive Summary Requirements:**

1. **Overall Pattern Identification**: What are the systematic patterns across all your analyses?

2. **Equity Assessment**: Which communities face the greatest risk of algorithmic bias based on your findings?

3. **Root Cause Analysis**: What underlying factors drive both data quality issues and bias risk?

4. **Strategic Recommendations**: What should the Department implement to address these systematic issues?

**Executive Summary:**

Across the analyses, a pattern emerges: the MOE for demographic data is highly sensitive to both the population size of census tracts and the demographic composition within them. Small-population counties, such as Vinton County, show particularly large MOEs due to limited sample sizes in the ACS. Similarly, tracts with highly diverse populations tend to have inflated MOEs across multiple demographic groups, making nearly every group susceptible to high uncertainty in the estimates.

In examining equity implications, Black and Hispanic populations face the highest risk of unreliable data. While the largest MOE observed for White populations is around 200%, Black and Hispanic populations frequently have MOEs in the hundreds of percent and, in extreme cases, as high as 2000%. This suggests that algorithmic decisions relying on this demographic data can be highly unreliable for these communities.

We can classify the root causes of these data issues into two categories. First, small sample sizes in low-population tracts and counties amplify MOEs for all estimates. Second, demographic variability can take the form of both extremely small minority populations *and* highly heterogeneous tracts. This further inflates MOE. An example is Census Tract 1711.02, which is nearly 87% Black but flagged as “High MOE” because the MOE for the White population exceeds 40%.

To address these systematic issues, decision-makers should incorporate demographic composition into analyses, weighting MOEs appropriately when interpreting the demographic data. Underrepresented communities should be prioritized for follow-up investigations at the tract or county level to better understand them. By acknowledging and adjusting for these data limitations, we can improve the equity and reliability of policy decisions.

## 6.3 Specific Recommendations

**Your Task:** Create a decision framework for algorithm implementation.

```{r recommendations-data}
# Create a summary table using your county reliability data
# Include: county name, median income, MOE percentage, reliability category

# Add a new column with algorithm recommendations using case_when():
# - High Confidence: "Safe for algorithmic decisions"
# - Moderate Confidence: "Use with caution - monitor outcomes"  
# - Low Confidence: "Requires manual review or additional data"

# Format as a professional table with kable()

county_data %>%
  mutate(
    algorithm_recommendation = case_when(
      median_income_moe_cat == "High Confidence: MOE < 5%"  ~ "Safe for algorithmic decisions",
      median_income_moe_cat == "Moderate Confidence: MOE 5-10%" ~ "Use with caution - monitor outcomes",
      median_income_moe_cat == "Low Confidence: MOE > 10%" ~ "Requires manual review or additional data",
      TRUE ~ "Check data"
    )
  ) %>%
  select(county_name, median_incomeE, median_income_moe_pct, median_income_moe_cat, algorithm_recommendation) %>%
  kable(
    caption = "County-Level Median Income Reliability and Algorithmic Recommendations",
    digits = 1,
    col.names = c(
      "County Name",
      "Median Income",
      "MOE % Median Income",
      "Reliability Category",
      "Algorithm Recommendation"
    )
  )



```

**Key Recommendations:**

**Your Task:** Use your analysis results to provide specific guidance to the department.

1. **Counties suitable for immediate algorithmic implementation:** Most Ohio counties are appropriate for immediate algorithmic use; however, some caveats apply. In counties with small or underrepresented populations, certain communities may face higher margins of error, and in highly diverse counties, careful attention or additional algorithmic analysis is needed.

2. **Counties requiring additional oversight:** Adams, Athens, Carroll, Champaign, Clinton, Coshocton, Darke, Fayette, Gallia, Guernsey, Hardin, Harrison, Henry, Highland, Jackson, Knox, Lawrence, Marion, Meigs, Monroe, Morgan, Morrow, Paulding, Perry, Pike, Preble, Sandusky, Van Wert, Wyandot.
- These counties can be further categorized based on population and demographic characteristics. In counties with very small populations, targeted local outreach may be sufficient to understand community needs. In larger or more diverse counties, more granular analysis at sub-county levels may be necessary to ensure accurate insights and equitable decision-making.

3. **Counties needing alternative approaches:** Noble, Vinton. 
- These counties have some of the lowest populations in Ohio, making traditional ACS estimates highly uncertain. Given the small population size, targeted local outreach—allowing for larger effective sample sizes than the ACS—may be the most feasible approach to accurately assess community needs.

## Questions for Further Investigation

The relationship between demographics and population size in determining MOE warrants further investigation. The current analysis, which applies MOE cutoffs based on either demographic or county-level data, involves tradeoffs and may exclude or underrepresent certain populations. Developing a cutoff that incorporates both population size and demographic characteristics could provide a more accurate and equitable assessment, rather than relying single, uniform metrics.

# Technical Notes

**Data Sources:** 
- U.S. Census Bureau, American Community Survey 2018-2022 5-Year Estimates
- Retrieved via tidycensus R package in September of 2025 (final run conducted on September 28th 2025)

**Reproducibility:** 
- All analysis conducted in R version 4.5.1 (2025-06-13)
- Census API key required for replication
- Complete code and documentation available at: https://musa-5080-fall-2025.github.io/portfolio-setup-kakumanus/

**Methodology Notes:**
Nothing of note in this analysis.

**Limitations:**
For algorithmic decision-making at the state government level, this analysis serves as an initial assessment, providing key considerations rather than providing a final solution. 
Further exploration is needed, particularly because relying on ACS-based estimates can inadvertently exclude populations—especially in counties where demographic characteristics make sampling approaches less reliable.

---

## Submission Checklist

Before submitting your portfolio link on Canvas:

- [x] All code chunks run without errors
- [x] All "[Fill this in]" prompts have been completed
- [x] Tables are properly formatted and readable
- [x] Executive summary addresses all four required components
- [x] Portfolio navigation includes this assignment
- [x] Census API key is properly set 
- [x] Document renders correctly to HTML

**Remember:** Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at `your-portfolio-url/assignments/assignment_1/your_file_name.html`