[
  {
    "objectID": "instructions_week1.html",
    "href": "instructions_week1.html",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Welcome to MUSA 5080! This guide will help you set up your personal portfolio repository for the semester.\n\n\nBy the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey\n\n\n\n\nThis is what you are building: Dr. Delmelle’s sample portfolio\n\n\n\n\nBefore starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed\n\n\n\n\n\nYou should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL\n\n\n\n\n\nIf you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!\n\n\n\n\nEach week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes\n\n\n\n\n\n\nWait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version\n\n\n\n\n\n\nCommit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis\n\n\n\n\n\nQuarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial\n\n\n\n\nDuring Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too\n\n\n\nBefore submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "instructions_week1.html#what-youre-building",
    "href": "instructions_week1.html#what-youre-building",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "By the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey"
  },
  {
    "objectID": "instructions_week1.html#example",
    "href": "instructions_week1.html#example",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "This is what you are building: Dr. Delmelle’s sample portfolio"
  },
  {
    "objectID": "instructions_week1.html#prerequisites",
    "href": "instructions_week1.html#prerequisites",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed"
  },
  {
    "objectID": "instructions_week1.html#step-by-step-setup",
    "href": "instructions_week1.html#step-by-step-setup",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "You should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL"
  },
  {
    "objectID": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "href": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "If you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!"
  },
  {
    "objectID": "instructions_week1.html#weekly-workflow",
    "href": "instructions_week1.html#weekly-workflow",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Each week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes"
  },
  {
    "objectID": "instructions_week1.html#troubleshooting",
    "href": "instructions_week1.html#troubleshooting",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Wait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version"
  },
  {
    "objectID": "instructions_week1.html#pro-tips",
    "href": "instructions_week1.html#pro-tips",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Commit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis"
  },
  {
    "objectID": "instructions_week1.html#additional-resources",
    "href": "instructions_week1.html#additional-resources",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Quarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial"
  },
  {
    "objectID": "instructions_week1.html#getting-help",
    "href": "instructions_week1.html#getting-help",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "During Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too"
  },
  {
    "objectID": "instructions_week1.html#checklist",
    "href": "instructions_week1.html#checklist",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html",
    "href": "weekly-notes/week-01-notes.html",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "An overview of Git + GitHub\nQuarto basics\nAn overview of R\ntidyverse and dplyr"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "An overview of Git + GitHub\nQuarto basics\nAn overview of R\ntidyverse and dplyr"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#coding-techniques",
    "href": "weekly-notes/week-01-notes.html#coding-techniques",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nWhy we use tidyverse (tibbles vs dataframes)\nA review of basic data manipulation using dplyr in R (select, filter, mutate, summarize, groupby)\nQuarto and how to generate good looking reports using markdown"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#questions-challenges",
    "href": "weekly-notes/week-01-notes.html#questions-challenges",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nNo major challenges this week."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#connections-to-policy",
    "href": "weekly-notes/week-01-notes.html#connections-to-policy",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nProf. Delmelle mentioned how the course, as opposed to the statistical analysis class, is focused on prediction.\nIn addition, public sector data science has very different goals than the private sector. Tradeoffs exist, and we’re not trying to simply optimize profit."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#reflection",
    "href": "weekly-notes/week-01-notes.html#reflection",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\nI hadn’t used Quarto so it was cool to see how the code could be deployed using GitHub Pages."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "This portfolio documents my learning journey in Public Policy Analytics (MUSA 5080).\n\n\nAdvanced spatial analysis and data science for urban planning and public policy.\n\n\n\n\nWeekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge\n\n\n\n\nI recently moved to Philadelphia to pursue my Master of Urban Spatial Analytics. Prior to this, I spent most of my life in Columbus, OH where I graduated with a Computer Science & Engineering degree from The Ohio State University, and spent time as a software engineer for JPMorgan Chase. My goal by pursuing this masters program is to shift my career into being able to do spatial data science for public benefit, particularly around housing and transportation access issues.\nOutside of my academic and professional pursuits, I love cooking and trying new restaurants, exploring both urban and natural spaces, and spending time with friends/family.\n\n\n\n\nEmail: kakumanu@upenn.edu\nGitHub: @kakumanus"
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Advanced spatial analysis and data science for urban planning and public policy."
  },
  {
    "objectID": "index.html#portfolio-structure",
    "href": "index.html#portfolio-structure",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Weekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "I recently moved to Philadelphia to pursue my Master of Urban Spatial Analytics. Prior to this, I spent most of my life in Columbus, OH where I graduated with a Computer Science & Engineering degree from The Ohio State University, and spent time as a software engineer for JPMorgan Chase. My goal by pursuing this masters program is to shift my career into being able to do spatial data science for public benefit, particularly around housing and transportation access issues.\nOutside of my academic and professional pursuits, I love cooking and trying new restaurants, exploring both urban and natural spaces, and spending time with friends/family."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Email: kakumanu@upenn.edu\nGitHub: @kakumanus"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#coding-techniques",
    "href": "weekly-notes/week-02-notes.html#coding-techniques",
    "title": "Week 2 Notes",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\ntidycensus\n\nRather than downloading csv files from the Census website, we will use the tidycensus R package to programatically access this data! Allows us to get the latest data and with automatic geographic boundaries.\nTables contain estimates (recall, ACS is based on a sample) and MOEs\nAlways report MOEs\n\nTIGER/Line Files\n\nShapefiles (geographic boundaries) for census tracts, counties, states.\n\nHistorical Data Sources\n\nNHGIS, Longitudinal Tract DB\nWe need this because boundaries change!"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#questions-challenges",
    "href": "weekly-notes/week-02-notes.html#questions-challenges",
    "title": "Week 2 Notes",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nN/A"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#connections-to-policy",
    "href": "weekly-notes/week-02-notes.html#connections-to-policy",
    "title": "Week 2 Notes",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nNoted above."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#reflection",
    "href": "weekly-notes/week-02-notes.html#reflection",
    "title": "Week 2 Notes",
    "section": "Reflection",
    "text": "Reflection\n\nN/A"
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html",
    "href": "labs/lab0/scripts/lab0_template.html",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "",
    "text": "Welcome to your first lab! In this (not graded) assignment, you’ll practice the fundamental dplyr operations I overviewed in class using car sales data. This lab will help you get comfortable with:\n\nBasic data exploration\nColumn selection and manipulation\n\nCreating new variables\nFiltering data\nGrouping and summarizing\n\nInstructions: Copy this template into your portfolio repository under a lab_0/ folder, then complete each section with your code and answers. You will write the code under the comment section in each chunk. Be sure to also copy the data folder into your lab_0 folder."
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#data-structure-exploration",
    "href": "labs/lab0/scripts/lab0_template.html#data-structure-exploration",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "1.1 Data Structure Exploration",
    "text": "1.1 Data Structure Exploration\nExplore the structure of your data and answer these questions:\n\n# Use glimpse() to see the data structure\n\n\n# Check the column names\n\n\n# Look at the first few rows\nhead(car_data)\n\n# A tibble: 6 × 7\n  Manufacturer Model     `Engine size` `Fuel type` `Year of manufacture` Mileage\n  &lt;chr&gt;        &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n1 Ford         Fiesta              1   Petrol                       2002  127300\n2 Porsche      718 Caym…           4   Petrol                       2016   57850\n3 Ford         Mondeo              1.6 Diesel                       2014   39190\n4 Toyota       RAV4                1.8 Hybrid                       1988  210814\n5 VW           Polo                1   Petrol                       2006  127869\n6 Ford         Focus               1.4 Petrol                       2018   33603\n# ℹ 1 more variable: Price &lt;dbl&gt;\n\n\nQuestions to answer: - How many rows and columns does the dataset have? - What types of variables do you see (numeric, character, etc.)? - Are there any column names that might cause problems? Why?\nYour answers: - Rows: 50000 - Columns: 7\n- Variable types: Strings and Numbers - Problematic names: Year of manufacture is stored as a number rather than in a date format (date would be easier to do time based calculations on). Also, the column name is way too long."
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#tibble-vs-data-frame",
    "href": "labs/lab0/scripts/lab0_template.html#tibble-vs-data-frame",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "1.2 Tibble vs Data Frame",
    "text": "1.2 Tibble vs Data Frame\nCompare how tibbles and data frames display:\n\n# Look at the tibble version (what we have)\ncar_data\n\n# A tibble: 50,000 × 7\n   Manufacturer Model    `Engine size` `Fuel type` `Year of manufacture` Mileage\n   &lt;chr&gt;        &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n 1 Ford         Fiesta             1   Petrol                       2002  127300\n 2 Porsche      718 Cay…           4   Petrol                       2016   57850\n 3 Ford         Mondeo             1.6 Diesel                       2014   39190\n 4 Toyota       RAV4               1.8 Hybrid                       1988  210814\n 5 VW           Polo               1   Petrol                       2006  127869\n 6 Ford         Focus              1.4 Petrol                       2018   33603\n 7 Ford         Mondeo             1.8 Diesel                       2010   86686\n 8 Toyota       Prius              1.4 Hybrid                       2015   30663\n 9 VW           Polo               1.2 Petrol                       2012   73470\n10 Ford         Focus              2   Diesel                       1992  262514\n# ℹ 49,990 more rows\n# ℹ 1 more variable: Price &lt;dbl&gt;\n\n# Convert to regular data frame and display\ncar_df &lt;- as.data.frame(car_data)\nhead(car_df)\n\n  Manufacturer      Model Engine size Fuel type Year of manufacture Mileage\n1         Ford     Fiesta         1.0    Petrol                2002  127300\n2      Porsche 718 Cayman         4.0    Petrol                2016   57850\n3         Ford     Mondeo         1.6    Diesel                2014   39190\n4       Toyota       RAV4         1.8    Hybrid                1988  210814\n5           VW       Polo         1.0    Petrol                2006  127869\n6         Ford      Focus         1.4    Petrol                2018   33603\n  Price\n1  3074\n2 49704\n3 24072\n4  1705\n5  4101\n6 29204\n\n\nQuestion: What differences do you notice in how they print?\nYour answer: In the rendered q markdown view, the tibble is much cleaner. It prints the size of the tibble, but only shows 10 rows and however many columns fit properly in the view. The dataframe prints the all 5000 rows! I’ve edited the code to only print the head() to avoid this."
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#selecting-columns",
    "href": "labs/lab0/scripts/lab0_template.html#selecting-columns",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "2.1 Selecting Columns",
    "text": "2.1 Selecting Columns\nPractice selecting different combinations of columns:\n\n# Select just Model and Mileage columns\nmodel_mileage &lt;- car_data %&gt;%\n  select(Model, Mileage)\n\n\n# Select Manufacturer, Price, and Fuel type\nmanufacturer_price_fuel &lt;- car_data %&gt;%\n  select(Manufacturer, Price, `Fuel type`)\n\n\n# Challenge: Select all columns EXCEPT Engine Size\ncar_data_sans_engine &lt;- car_data %&gt;%\n  select(!`Engine size`)"
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#renaming-columns",
    "href": "labs/lab0/scripts/lab0_template.html#renaming-columns",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "2.2 Renaming Columns",
    "text": "2.2 Renaming Columns\nLet’s fix a problematic column name:\n\n# Rename 'Year of manufacture' to year\ncar_data &lt;- car_data %&gt;%\n  rename(year = `Year of manufacture`)\n\n\n# Check that it worked\nnames(car_data)\n\n[1] \"Manufacturer\" \"Model\"        \"Engine size\"  \"Fuel type\"    \"year\"        \n[6] \"Mileage\"      \"Price\"       \n\n\nQuestion: Why did we need back-ticks around Year of manufacture but not around year?\nYour answer: There are spaces in the column name, so the backticks help R understand what the full name."
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#calculate-car-age",
    "href": "labs/lab0/scripts/lab0_template.html#calculate-car-age",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "3.1 Calculate Car Age",
    "text": "3.1 Calculate Car Age\n\n# Create an 'age' column (2025 minus year of manufacture)\ncar_data &lt;- car_data %&gt;%\n  mutate(age = 2025-year)\n\n# Create a mileage_per_year column  \ncar_data &lt;- car_data %&gt;%\n  mutate(mileage_per_year = Mileage/age)\n\n# Look at your new columns\n#select(car_data, Model, year, age, Mileage, mileage_per_year)\ncar_data %&gt;%\n  select(Model, year, age, Mileage, mileage_per_year)\n\n# A tibble: 50,000 × 5\n   Model       year   age Mileage mileage_per_year\n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;\n 1 Fiesta      2002    23  127300            5535.\n 2 718 Cayman  2016     9   57850            6428.\n 3 Mondeo      2014    11   39190            3563.\n 4 RAV4        1988    37  210814            5698.\n 5 Polo        2006    19  127869            6730.\n 6 Focus       2018     7   33603            4800.\n 7 Mondeo      2010    15   86686            5779.\n 8 Prius       2015    10   30663            3066.\n 9 Polo        2012    13   73470            5652.\n10 Focus       1992    33  262514            7955.\n# ℹ 49,990 more rows"
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#categorize-cars",
    "href": "labs/lab0/scripts/lab0_template.html#categorize-cars",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "3.2 Categorize Cars",
    "text": "3.2 Categorize Cars\n\n# Create a price_category column where if price is &lt; 15000, it is coded as budget, between 15000 and 30000 is midrange and greater than 30000 is luxury (use case_when)\ncar_data &lt;- car_data %&gt;%\n  mutate(price_category = case_when(\n    Price &gt; 30000 ~ \"luxury\",\n    Price &gt;= 15000 & Price &lt;= 30000 ~ \"midrange\",\n    .default = \"budget\"\n  ))\n\n\n# Check your categories select the new column and show it\ncar_data %&gt;%\n  select(Manufacturer, Model, price_category)\n\n# A tibble: 50,000 × 3\n   Manufacturer Model      price_category\n   &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;         \n 1 Ford         Fiesta     budget        \n 2 Porsche      718 Cayman luxury        \n 3 Ford         Mondeo     midrange      \n 4 Toyota       RAV4       budget        \n 5 VW           Polo       budget        \n 6 Ford         Focus      midrange      \n 7 Ford         Mondeo     budget        \n 8 Toyota       Prius      luxury        \n 9 VW           Polo       budget        \n10 Ford         Focus      budget        \n# ℹ 49,990 more rows"
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#basic-filtering",
    "href": "labs/lab0/scripts/lab0_template.html#basic-filtering",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "4.1 Basic Filtering",
    "text": "4.1 Basic Filtering\n\n# Find all Toyota cars\ncar_data %&gt;%\n  filter(Manufacturer == \"Toyota\")\n\n# A tibble: 12,554 × 10\n   Manufacturer Model `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Toyota       RAV4            1.8 Hybrid       1988  210814  1705    37\n 2 Toyota       Prius           1.4 Hybrid       2015   30663 30297    10\n 3 Toyota       RAV4            2.2 Petrol       2007   79393 16026    18\n 4 Toyota       Yaris           1.4 Petrol       1998   97286  4046    27\n 5 Toyota       RAV4            2.4 Hybrid       2003  117425 11667    22\n 6 Toyota       Yaris           1.2 Petrol       1992  245990   720    33\n 7 Toyota       RAV4            2   Hybrid       2018   28381 52671     7\n 8 Toyota       Prius           1   Hybrid       2003  115291  6512    22\n 9 Toyota       Prius           1   Hybrid       1990  238571   961    35\n10 Toyota       Prius           1.8 Hybrid       2017   31958 38961     8\n# ℹ 12,544 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find cars with mileage less than 30,000\ncar_data %&gt;%\n  filter(Mileage &lt; 30000)\n\n# A tibble: 5,402 × 10\n   Manufacturer Model      `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Toyota       RAV4                 2   Hybrid       2018   28381 52671     7\n 2 VW           Golf                 2   Petrol       2020   18985 36387     5\n 3 BMW          M5                   4   Petrol       2017   22759 97758     8\n 4 Toyota       RAV4                 2.4 Petrol       2018   24588 49125     7\n 5 VW           Golf                 2   Hybrid       2018   25017 36957     7\n 6 Porsche      718 Cayman           2.4 Petrol       2021   14070 69526     4\n 7 Ford         Focus                1.8 Petrol       2020   22371 40336     5\n 8 Ford         Mondeo               1.6 Diesel       2015   21834 28435    10\n 9 VW           Passat               1.6 Diesel       2018   22122 36634     7\n10 VW           Passat               1.4 Diesel       2020   21413 39310     5\n# ℹ 5,392 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find luxury cars (from price category) with low mileage\ncar_data %&gt;%\n  filter(Mileage &lt; 30000 & price_category == \"luxury\")\n\n# A tibble: 3,256 × 10\n   Manufacturer Model      `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Toyota       RAV4                 2   Hybrid       2018   28381 52671     7\n 2 VW           Golf                 2   Petrol       2020   18985 36387     5\n 3 BMW          M5                   4   Petrol       2017   22759 97758     8\n 4 Toyota       RAV4                 2.4 Petrol       2018   24588 49125     7\n 5 VW           Golf                 2   Hybrid       2018   25017 36957     7\n 6 Porsche      718 Cayman           2.4 Petrol       2021   14070 69526     4\n 7 Ford         Focus                1.8 Petrol       2020   22371 40336     5\n 8 VW           Passat               1.6 Diesel       2018   22122 36634     7\n 9 VW           Passat               1.4 Diesel       2020   21413 39310     5\n10 Toyota       RAV4                 2.4 Petrol       2021    6829 66031     4\n# ℹ 3,246 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;"
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#multiple-conditions",
    "href": "labs/lab0/scripts/lab0_template.html#multiple-conditions",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "4.2 Multiple Conditions",
    "text": "4.2 Multiple Conditions\n\n# Find cars that are EITHER Honda OR Nissan\n# Note: I used %in% to avoid writing Manufacturer twice.\ncar_data %&gt;%\n  filter(Manufacturer %in% c(\"Honda\", \"Nissan\"))\n\n# A tibble: 0 × 10\n# ℹ 10 variables: Manufacturer &lt;chr&gt;, Model &lt;chr&gt;, Engine size &lt;dbl&gt;,\n#   Fuel type &lt;chr&gt;, year &lt;dbl&gt;, Mileage &lt;dbl&gt;, Price &lt;dbl&gt;, age &lt;dbl&gt;,\n#   mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find cars with price between $20,000 and $35,000\n# Note: I used between() to have a more concise query.\ncar_data %&gt;%\n  filter(between(Price, 20000, 35000 ))\n\n# A tibble: 7,301 × 10\n   Manufacturer Model  `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Ford         Mondeo           1.6 Diesel       2014   39190 24072    11\n 2 Ford         Focus            1.4 Petrol       2018   33603 29204     7\n 3 Toyota       Prius            1.4 Hybrid       2015   30663 30297    10\n 4 Toyota       Prius            1.4 Hybrid       2016   43893 29946     9\n 5 Toyota       Prius            1.4 Hybrid       2016   43130 30085     9\n 6 VW           Passat           1.6 Petrol       2016   64344 23641     9\n 7 Ford         Mondeo           1.6 Diesel       2015   21834 28435    10\n 8 BMW          M5               4.4 Petrol       2008  109941 31711    17\n 9 BMW          Z4               2.2 Petrol       2014   61332 26084    11\n10 Porsche      911              3.5 Petrol       2003  107705 24378    22\n# ℹ 7,291 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find diesel cars less than 10 years old\ncar_data %&gt;%\n  filter (age &lt; 10 & `Fuel type` == \"Diesel\")\n\n# A tibble: 2,040 × 10\n   Manufacturer Model   `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Ford         Fiesta            1   Diesel       2017   38370 16257     8\n 2 VW           Passat            1.6 Diesel       2018   22122 36634     7\n 3 VW           Passat            1.4 Diesel       2020   21413 39310     5\n 4 BMW          X3                2   Diesel       2018   27389 44018     7\n 5 Ford         Mondeo            2   Diesel       2016   51724 28482     9\n 6 Porsche      Cayenne           2.6 Diesel       2019   20147 76182     6\n 7 VW           Polo              1.2 Diesel       2018   37411 19649     7\n 8 Ford         Mondeo            1.8 Diesel       2016   29439 30886     9\n 9 Ford         Mondeo            1.4 Diesel       2020   18929 37720     5\n10 Ford         Mondeo            1.4 Diesel       2018   42017 28904     7\n# ℹ 2,030 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n\nQuestion: How many diesel cars are less than 10 years old?\nYour answer: 2040"
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#basic-summaries",
    "href": "labs/lab0/scripts/lab0_template.html#basic-summaries",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "5.1 Basic Summaries",
    "text": "5.1 Basic Summaries\n\n# Calculate average price by manufacturer\navg_price_by_brand &lt;- car_data %&gt;%\n  group_by(Manufacturer) %&gt;%\n  summarize(avg_price = mean(Price, na.rm = TRUE))\n\navg_price_by_brand\n\n# A tibble: 5 × 2\n  Manufacturer avg_price\n  &lt;chr&gt;            &lt;dbl&gt;\n1 BMW             24429.\n2 Ford            10672.\n3 Porsche         29104.\n4 Toyota          14340.\n5 VW              10363.\n\n# Calculate average mileage by fuel type\navg_price_by_fuel_type &lt;- car_data %&gt;%\n  group_by(`Fuel type`) %&gt;%\n  summarize(avg_price = mean(Price, na.rm = TRUE))\n\navg_price_by_fuel_type\n\n# A tibble: 3 × 2\n  `Fuel type` avg_price\n  &lt;chr&gt;           &lt;dbl&gt;\n1 Diesel         13145.\n2 Hybrid         14949.\n3 Petrol         13691.\n\n# Count cars by manufacturer\ncount_by_manufacturer &lt;- car_data %&gt;%\n  group_by(Manufacturer) %&gt;%\n  summarize(\"Count\" = n())\n\ncount_by_manufacturer\n\n# A tibble: 5 × 2\n  Manufacturer Count\n  &lt;chr&gt;        &lt;int&gt;\n1 BMW           4965\n2 Ford         14959\n3 Porsche       2609\n4 Toyota       12554\n5 VW           14913"
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#categorical-summaries",
    "href": "labs/lab0/scripts/lab0_template.html#categorical-summaries",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "5.2 Categorical Summaries",
    "text": "5.2 Categorical Summaries\n\n# Frequency table for price categories"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html",
    "href": "weekly-notes/week-02-notes.html",
    "title": "Week 2 Notes",
    "section": "",
    "text": "Algorithms\n\nHigh level, a set of rules or instructions for solving a problem or completing a task.\n\nAlgorithmic Decision Making in the Government\n\nCan be used to assist or replace human decision makers (mitigating biases that may exist in a human only process).\nCan use historical data providing Inputs(“features”, “predictors”, “independent variables”) to determine Outputs(“labels”, “outcome”, “dependent variables”).\nData has trade-offs! Data can be inaccurate, subject to biases itself, etc.\n\nData collection is a long running practice in gov’t (civic registration, census, admin records, operations research)\n\nNow, there is an increase in official and accidental (things that can be collected from sources like social media ) data.\nThere is a shift from historical analysis to prediction.\n\nIMPORTANT: Data Analytics is Subjective!\n\nData cleaning, coding/classification, collection, interpretation, model variables. All require human choices that embody human values and biases.\nEspecially data proxies/variables that can rely on historically biased data.\n\nCensus and ACS Data:\n\nFoundational in understanding community demographics, allocating govt resources, etc.\nCensus is 10 years and sent to everyone, 9 basic questions, constitutional requirement, determines political representation.\nAmerican Community Survey (ACS) sent to ~3% of households annually, with more detailed questions (income, education, employment, housing costs). Done at Block Group level.\n\nACS has 1-year estimates (areas &gt; 65k people) and 5-year estimates (all areas with census tracts). 5-year estimates are the most reliable and based on the largest sample.\n\nHierarchy of Census Data:\nNation\n├── Regions  \n├── States\n│   ├── Counties\n│   │   ├── Census Tracts (1,500-8,000 people)\n│   │   │   ├── Block Groups (600-3,000 people)  \n│   │   │   │   └── Blocks (≈85 people, Decennial only)\nMost policy analysis happens at County, Census, Block (although Blocks have big margins of error (MOE))\n\nCensus Data in R (see below)"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "Algorithms: High level, a set of rules or instructions for solving a problem or completing a task.\nAlgorithmic Decision Making in the Government\n\nTest"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#key-concepts-learned-notes",
    "href": "weekly-notes/week-02-notes.html#key-concepts-learned-notes",
    "title": "Week 2 Notes",
    "section": "",
    "text": "Algorithms\n\nHigh level, a set of rules or instructions for solving a problem or completing a task.\n\nAlgorithmic Decision Making in the Government\n\nCan be used to assist or replace human decision makers (mitigating biases that may exist in a human only process).\nCan use historical data providing Inputs(“features”, “predictors”, “independent variables”) to determine Outputs(“labels”, “outcome”, “dependent variables”).\nData has trade-offs! Data can be inaccurate, subject to biases itself, etc.\n\nData collection is a long running practice in gov’t (civic registration, census, admin records, operations research)\n\nNow, there is an increase in official and accidental (things that can be collected from sources like social media ) data.\nThere is a shift from historical analysis to prediction.\n\nIMPORTANT: Data Analytics is Subjective!\n\nData cleaning, coding/classification, collection, interpretation, model variables. All require human choices that embody human values and biases.\nEspecially data proxies/variables that can rely on historically biased data.\n\nCensus and ACS Data:\n\nFoundational in understanding community demographics, allocating govt resources, etc.\nCensus is 10 years and sent to everyone, 9 basic questions, constitutional requirement, determines political representation.\nAmerican Community Survey (ACS) sent to ~3% of households annually, with more detailed questions (income, education, employment, housing costs). Done at Block Group level.\n\nACS has 1-year estimates (areas &gt; 65k people) and 5-year estimates (all areas with census tracts). 5-year estimates are the most reliable and based on the largest sample.\n\nHierarchy of Census Data:\nNation\n├── Regions  \n├── States\n│   ├── Counties\n│   │   ├── Census Tracts (1,500-8,000 people)\n│   │   │   ├── Block Groups (600-3,000 people)  \n│   │   │   │   └── Blocks (≈85 people, Decennial only)\nMost policy analysis happens at County, Census, Block (although Blocks have big margins of error (MOE))\n\nCensus Data in R (see below)"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#coding-techniquestechnical-notes",
    "href": "weekly-notes/week-02-notes.html#coding-techniquestechnical-notes",
    "title": "Week 2 Notes",
    "section": "Coding Techniques/Technical Notes",
    "text": "Coding Techniques/Technical Notes\n\ntidycensus\n\nRather than downloading csv files from the Census website, we will use the tidycensus R package to programatically access this data! Allows us to get the latest data and with automatic geographic boundaries.\nTables contain estimates (recall, ACS is based on a sample) and MOEs\nAlways report MOEs\n\nTIGER/Line Files\n\nShapefiles (geographic boundaries) for census tracts, counties, states.\n\nHistorical Data Sources\n\nNHGIS, Longitudinal Tract DB\nWe need this because boundaries change!"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html",
    "href": "weekly-notes/week-03-notes.html",
    "title": "Week 3 Notes",
    "section": "",
    "text": "Importance of Data Visualization and Communication\n\nExample of Anscombe’s Quartet: Charts with the same summary statistics, but the way the data is represented would suggest otherwise.\nPoor policy decisions can result from misunderstood data. Recall the Jurjevich et al. study that found 72% of Portland Census tracts having unreliable child poverty estimates. This uncertainty was never communicated by planners.\nA note that block groups, since they’re smaller, can have high margins of error! Imagine (an example) just two people getting surveyed in a block group.\n\nGrammar of Graphics Principles (ggplot2 philosophy)\n\nData\nAesthetics: aes(); connection between data variables and visual properties of the plot\nGeometries: geom_points(); how to display the data - points, lines, etc.\nVisual: Adding scales, themes, facets, annotations, etc.\n\nExploratory Data Analysis (EDA)\n\nWhat does the data looking like? (distributions, missing values)\nWhat patterns exists? (relationships, clusters, trends)\nWhat’s unusual? (outliers, anomalies, data quality issues)\nWhat questions does this raise? (hypotheses)\nReliability and data quality?"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#key-concepts-learned-notes",
    "href": "weekly-notes/week-03-notes.html#key-concepts-learned-notes",
    "title": "Week 3 Notes",
    "section": "",
    "text": "Importance of Data Visualization and Communication\n\nExample of Anscombe’s Quartet: Charts with the same summary statistics, but the way the data is represented would suggest otherwise.\nPoor policy decisions can result from misunderstood data. Recall the Jurjevich et al. study that found 72% of Portland Census tracts having unreliable child poverty estimates. This uncertainty was never communicated by planners.\nA note that block groups, since they’re smaller, can have high margins of error! Imagine (an example) just two people getting surveyed in a block group.\n\nGrammar of Graphics Principles (ggplot2 philosophy)\n\nData\nAesthetics: aes(); connection between data variables and visual properties of the plot\nGeometries: geom_points(); how to display the data - points, lines, etc.\nVisual: Adding scales, themes, facets, annotations, etc.\n\nExploratory Data Analysis (EDA)\n\nWhat does the data looking like? (distributions, missing values)\nWhat patterns exists? (relationships, clusters, trends)\nWhat’s unusual? (outliers, anomalies, data quality issues)\nWhat questions does this raise? (hypotheses)\nReliability and data quality?"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#coding-techniquestechnical-notes",
    "href": "weekly-notes/week-03-notes.html#coding-techniquestechnical-notes",
    "title": "Week 3 Notes",
    "section": "Coding Techniques/Technical Notes",
    "text": "Coding Techniques/Technical Notes\n\nggplot2\n\nggplot(data = yourdata) + aes(x = variable1, y = variable2) + geom_something() + additional_layers()\nAgain, aesthetics are used to map data to visual properties. You shouldn’t be setting themes here (for example), but if you want the color to change based on a variable, you can do it here!"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#questions-challenges",
    "href": "weekly-notes/week-03-notes.html#questions-challenges",
    "title": "Week 3 Notes",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nN/A"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#connections-to-policy",
    "href": "weekly-notes/week-03-notes.html#connections-to-policy",
    "title": "Week 3 Notes",
    "section": "## Connections to Policy",
    "text": "## Connections to Policy"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#reflection",
    "href": "weekly-notes/week-03-notes.html#reflection",
    "title": "Week 3 Notes",
    "section": "## Reflection",
    "text": "## Reflection"
  },
  {
    "objectID": "labs/lab1/assignment1.html",
    "href": "labs/lab1/assignment1.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Ohio Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders\n\n\n\n\nSubmit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "labs/lab1/assignment1.html#scenario",
    "href": "labs/lab1/assignment1.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Ohio Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "labs/lab1/assignment1.html#learning-objectives",
    "href": "labs/lab1/assignment1.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "labs/lab1/assignment1.html#submission-instructions",
    "href": "labs/lab1/assignment1.html#submission-instructions",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Submit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "labs/lab1/assignment1.html#data-retrieval",
    "href": "labs/lab1/assignment1.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\n\n# Clean the county names to remove state name and \"County\" \n# Hint: use mutate() with str_remove()\n\n# Display the first few rows"
  },
  {
    "objectID": "labs/lab1/assignment1.html#data-quality-assessment",
    "href": "labs/lab1/assignment1.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Calculate MOE percentage and reliability categories using mutate()\n\n# Create a summary showing count of counties in each reliability category\n# Hint: use count() and mutate() to add percentages"
  },
  {
    "objectID": "labs/lab1/assignment1.html#high-uncertainty-counties",
    "href": "labs/lab1/assignment1.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\n\n# Format as table with kable() - include appropriate column names and caption\n\nData Quality Commentary:\n[Write 2-3 sentences explaining what these results mean for algorithmic decision-making. Consider: Which counties might be poorly served by algorithms that rely on this income data? What factors might contribute to higher uncertainty?]"
  },
  {
    "objectID": "labs/lab1/assignment1.html#focus-area-selection",
    "href": "labs/lab1/assignment1.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\n\nComment on the output: [write something :)]"
  },
  {
    "objectID": "labs/lab1/assignment1.html#tract-level-demographics",
    "href": "labs/lab1/assignment1.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You’ll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\n\n# Use get_acs() to retrieve tract-level data\n# Hint: You may need to specify county codes in the county parameter\n\n# Calculate percentage of each group using mutate()\n# Create percentages for white, Black, and Hispanic populations\n\n# Add readable tract and county name columns using str_extract() or similar"
  },
  {
    "objectID": "labs/lab1/assignment1.html#demographic-analysis",
    "href": "labs/lab1/assignment1.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\n# Find the tract with the highest percentage of Hispanic/Latino residents\n# Hint: use arrange() and slice() to get the top tract\n\n# Calculate average demographics by county using group_by() and summarize()\n# Show: number of tracts, average percentage for each racial/ethnic group\n\n# Create a nicely formatted table of your results using kable()"
  },
  {
    "objectID": "labs/lab1/assignment1.html#moe-analysis-for-demographic-variables",
    "href": "labs/lab1/assignment1.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\n# Calculate MOE percentages for white, Black, and Hispanic variables\n# Hint: use the same formula as before (margin/estimate * 100)\n\n# Create a flag for tracts with high MOE on any demographic variable\n# Use logical operators (| for OR) in an ifelse() statement\n\n# Create summary statistics showing how many tracts have data quality issues"
  },
  {
    "objectID": "labs/lab1/assignment1.html#pattern-analysis",
    "href": "labs/lab1/assignment1.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\n\n# Use group_by() and summarize() to create this comparison\n# Create a professional table showing the patterns\n\nPattern Analysis: [Describe any patterns you observe. Do certain types of communities have less reliable data? What might explain this?]"
  },
  {
    "objectID": "labs/lab1/assignment1.html#analysis-integration-and-professional-summary",
    "href": "labs/lab1/assignment1.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements: 1. Overall Pattern Identification: What are the systematic patterns across all your analyses? 2. Equity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings? 3. Root Cause Analysis: What underlying factors drive both data quality issues and bias risk? 4. Strategic Recommendations: What should the Department implement to address these systematic issues?\nExecutive Summary:\n[Your integrated 4-paragraph summary here]"
  },
  {
    "objectID": "labs/lab1/assignment1.html#specific-recommendations",
    "href": "labs/lab1/assignment1.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\n\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\n\n# Format as a professional table with kable()\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nCounties suitable for immediate algorithmic implementation: [List counties with high confidence data and explain why they’re appropriate]\nCounties requiring additional oversight: [List counties with moderate confidence data and describe what kind of monitoring would be needed]\nCounties needing alternative approaches: [List counties with low confidence data and suggest specific alternatives - manual review, additional surveys, etc.]"
  },
  {
    "objectID": "labs/lab1/assignment1.html#questions-for-further-investigation",
    "href": "labs/lab1/assignment1.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n[List 2-3 questions that your analysis raised that you’d like to explore further in future assignments. Consider questions about spatial patterns, time trends, or other demographic factors.]"
  },
  {
    "objectID": "labs/lab1/assignment1.html#submission-checklist",
    "href": "labs/lab1/assignment1.html#submission-checklist",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\nAll code chunks run without errors\nAll “[Fill this in]” prompts have been completed\nTables are properly formatted and readable\nExecutive summary addresses all four required components\nPortfolio navigation includes this assignment\nCensus API key is properly set\nDocument renders correctly to HTML\n\nRemember: Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/your_file_name.html"
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html",
    "href": "assignments/assignment_1/assignment1.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Ohio Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders\n\n\n\n\nSubmit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#scenario",
    "href": "assignments/assignment_1/assignment1.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Ohio Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#learning-objectives",
    "href": "assignments/assignment_1/assignment1.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#submission-instructions",
    "href": "assignments/assignment_1/assignment1.html#submission-instructions",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Submit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#data-retrieval",
    "href": "assignments/assignment_1/assignment1.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\ncounty_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    median_income = \"B19013_001\",   # Median household income\n    total_pop = \"B01003_001\"       # Total population\n  ),\n  state = my_state,\n  year = 2022,\n  output = \"wide\"\n)\n\n# Clean the county names to remove state name and \"County\" \nwords_to_remove &lt;- c(\"County\", \"Ohio\", ', ', ' ')\npattern &lt;- paste0(\"\\\\b(\", paste(words_to_remove, collapse = \"|\"), \")\\\\b\")\n\ncounty_data &lt;- county_data %&gt;%\n  mutate(county_name = str_remove_all(NAME, pattern))\n\n# Display the first few rows\nglimpse(county_data)\n\nRows: 88\nColumns: 7\n$ GEOID          &lt;chr&gt; \"39001\", \"39003\", \"39005\", \"39007\", \"39009\", \"39011\", \"…\n$ NAME           &lt;chr&gt; \"Adams County, Ohio\", \"Allen County, Ohio\", \"Ashland Co…\n$ median_incomeE &lt;dbl&gt; 46234, 58976, 62254, 53663, 48750, 75231, 56943, 66677,…\n$ median_incomeM &lt;dbl&gt; 4072, 2172, 2954, 2186, 3695, 2795, 2417, 3077, 1604, 3…\n$ total_popE     &lt;dbl&gt; 27509, 102087, 52522, 97666, 61276, 46263, 66554, 43715…\n$ total_popM     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ county_name    &lt;chr&gt; \"Adams\", \"Allen\", \"Ashland\", \"Ashtabula\", \"Athens\", \"Au…"
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#data-quality-assessment",
    "href": "assignments/assignment_1/assignment1.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Calculate MOE percentage and reliability categories using mutate()\n# Calculate MOE percentages\ncounty_data &lt;- county_data %&gt;%\n  mutate(\n      median_income_moe_pct = (median_incomeM / median_incomeE) * 100,\n  )\n\ncounty_data &lt;- county_data %&gt;%\n  mutate(\n    median_income_moe_cat = case_when(\n      median_income_moe_pct &gt; 10 ~ \"Low Confidence: MOE &gt; 10%\",\n      median_income_moe_pct &gt;= 5 & median_income_moe_pct &lt;= 10 ~ \"Moderate Confidence: MOE 5-10%\",\n      TRUE ~ \"High Confidence: MOE &lt; 5%\"\n    ),\n  )\n\n# Create a summary showing count of counties in each reliability category\n# Hint: use count() and mutate() to add percentages\n\nmoe_cat_summary_table &lt;- county_data %&gt;%\n  count(median_income_moe_cat, name = \"number_of_counties\")\n\nmoe_cat_summary_table\n\n# A tibble: 3 × 2\n  median_income_moe_cat          number_of_counties\n  &lt;chr&gt;                                       &lt;int&gt;\n1 High Confidence: MOE &lt; 5%                      57\n2 Low Confidence: MOE &gt; 10%                       2\n3 Moderate Confidence: MOE 5-10%                 29"
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#high-uncertainty-counties",
    "href": "assignments/assignment_1/assignment1.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\ntop_moe_counties &lt;- county_data %&gt;%\n   slice_max(median_income_moe_pct, n = 5) %&gt;%\n    select(county_name, median_incomeE, median_incomeM, median_income_moe_pct, median_income_moe_cat)\n\n# Format as table with kable() - include appropriate column names and caption\nkable(\n  top_moe_counties,\n  caption = \"Top 5 Counties in Ohio by Median Income Margin of Error (MOE)\",\n  col.names = c(\"County Name\", \"Median Income Estimate $\", \"Median Income MOE $\", \"Median Income MOE %\", \"Median Income MOE Category\")\n)\n\n\nTop 5 Counties in Ohio by Median Income Margin of Error (MOE)\n\n\n\n\n\n\n\n\n\nCounty Name\nMedian Income Estimate $\nMedian Income MOE $\nMedian Income MOE %\nMedian Income MOE Category\n\n\n\n\nVinton\n50967\n6264\n12.290306\nLow Confidence: MOE &gt; 10%\n\n\nNoble\n51547\n5566\n10.797913\nLow Confidence: MOE &gt; 10%\n\n\nMonroe\n55381\n5415\n9.777722\nModerate Confidence: MOE 5-10%\n\n\nAdams\n46234\n4072\n8.807371\nModerate Confidence: MOE 5-10%\n\n\nPike\n50832\n4258\n8.376613\nModerate Confidence: MOE 5-10%\n\n\n\n\n\nData Quality Commentary:\nThese results are not surprising given that the counties are among the least populous in the state (Vinton, Noble, and Monroe have less than 15,000 people according to 2024 estimates). Since the ACS relies on sample data, rather than the total enumeration provided by the Census, small population groups will result in small sample sizes.\nIn this scenario: Any algorithmic system should account for the data inaccuracies arising from Ohio’s smaller counties."
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#focus-area-selection",
    "href": "assignments/assignment_1/assignment1.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\nselected_counties &lt;- county_data %&gt;%\n  filter(county_name %in% c(\"Cuyahoga\", \"Marion\", \"Vinton\"))\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\nkable(\n  caption = \"Selected Counties (1 in each reliability category)\",\n  selected_counties %&gt;% select(county_name, median_incomeE, median_income_moe_pct, median_income_moe_cat),\n  col.names = c(\"County Name\", \"Median Income Estimate $\", \"Median Income MOE %\", \"Median Income MOE Category\")\n)\n\n\nSelected Counties (1 in each reliability category)\n\n\n\n\n\n\n\n\nCounty Name\nMedian Income Estimate $\nMedian Income MOE %\nMedian Income MOE Category\n\n\n\n\nCuyahoga\n60074\n1.163565\nHigh Confidence: MOE &lt; 5%\n\n\nMarion\n55106\n8.313069\nModerate Confidence: MOE 5-10%\n\n\nVinton\n50967\n12.290306\nLow Confidence: MOE &gt; 10%\n\n\n\n\n\nComment on the output: Cuyahoga county is one of the most populous counties in Ohio, so the sample sizes used lent to its low MOE of ~1.16%. Although this can not be extrapolated to all counties, it is interesting to see a decline of median income as the MOE increases among the selected counties."
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#tract-level-demographics",
    "href": "assignments/assignment_1/assignment1.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You’ll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\nrace_vars &lt;- c(\n  white    = \"B03002_003\",\n  black    = \"B03002_004\",\n  hispanic = \"B03002_012\",\n  total    = \"B03002_001\"\n)\n\n# Use get_acs() to retrieve tract-level data\n# Hint: You may need to specify county codes in the county parameter\n\n# Based on https://www.census.gov/programs-surveys/geography/guidance/geo-identifiers.html,\n# GEOIDs for counties are made of two digit state codes and three digit county codes. This is visible\n# in how GEOID in county_data stays constant for the first two digits, and is unique for the last 3.\n# We can parse this and create a list:\nselected_counties &lt;- selected_counties %&gt;%\n  mutate(county_code = str_sub(GEOID, 3, 5))\n\ncounty_codes &lt;- selected_counties %&gt;% pull(county_code)\n\ntract_demo_selected_counties = get_acs(\n  geography = \"tract\",\n  state     = my_state,\n  county    = county_codes,\n  variables = race_vars,\n  output    = \"wide\"\n)\n\n# Calculate percentage of each group using mutate()\n# Create percentages for white, Black, and Hispanic populations\ntract_demo_selected_counties &lt;- tract_demo_selected_counties %&gt;%\n  mutate(\n    pct_white    = 100 * whiteE    / totalE,\n    pct_black    = 100 * blackE    / totalE,\n    pct_hispanic = 100 * hispanicE / totalE\n  )\n\n# Add readable tract and county name columns using str_extract() or similar\ntract_demo_selected_counties &lt;- tract_demo_selected_counties %&gt;%\n  mutate(\n    tract_name  = str_extract(NAME, \"^[^;]+\"),\n    county_name = str_extract(NAME, \"(?&lt;=;\\\\s).*\")\n  )\n\n# We can clean up the county name as we did before:\nwords_to_remove &lt;- c(\"County\", \"Ohio\", '; ', ' ')\npattern &lt;- paste0(\"\\\\b(\", paste(words_to_remove, collapse = \"|\"), \")\\\\b\")\ntract_demo_selected_counties &lt;- tract_demo_selected_counties %&gt;%\n  mutate(county_name = str_remove_all(county_name, pattern))"
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#demographic-analysis",
    "href": "assignments/assignment_1/assignment1.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\n# Find the tract with the highest percentage of Hispanic/Latino residents\n# Hint: use arrange() and slice() to get the top tract\ntop_hispanic &lt;- tract_demo_selected_counties %&gt;%\n  slice_max(order_by = pct_hispanic, n = 1)\n\ntop_hispanic %&gt;%\n  select(county_name, tract_name, pct_white, pct_black, pct_hispanic) %&gt;%\n  kable(\n    caption = \"Details of Census Tract with Highest Percentage Hispanic Residents\",\n    col.names = c(\"County Name\", \"Tract\", \"% White\", \"% Black\", \"% Hispanic\")\n  )\n\n\nDetails of Census Tract with Highest Percentage Hispanic Residents\n\n\nCounty Name\nTract\n% White\n% Black\n% Hispanic\n\n\n\n\nCuyahoga\nCensus Tract 1029\n18.89952\n19.13876\n61.48325\n\n\n\n\n# Calculate average demographics by county using group_by() and summarize()\n# Show: number of tracts, average percentage for each racial/ethnic group\n\nselected_counties_summary &lt;- tract_demo_selected_counties %&gt;%\n  group_by(county_name) %&gt;%\n  summarize(\n    n_of_tracts      = n(),\n    avg_pct_white = mean(pct_white,    na.rm = TRUE),\n    avg_pct_black = mean(pct_black,    na.rm = TRUE),\n    avg_pct_hisp  = mean(pct_hispanic, na.rm = TRUE),\n  )\n\n# Create a nicely formatted table of your results using kable()\nselected_counties_summary %&gt;%\n  kable(\n    caption = \"Average Demographics for Selected Ohio Counties\",\n    digits = 2,\n    col.names = c(\"County Name\", \"Number of Tracts\", \"Avg % White\", \"Avg % Black\", \"Avg % Hispanic\")\n  )\n\n\nAverage Demographics for Selected Ohio Counties\n\n\n\n\n\n\n\n\n\nCounty Name\nNumber of Tracts\nAvg % White\nAvg % Black\nAvg % Hispanic\n\n\n\n\nCuyahoga\n428\n51.12\n34.98\n6.77\n\n\nMarion\n18\n86.13\n4.75\n3.50\n\n\nVinton\n3\n96.42\n0.49\n0.07"
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#moe-analysis-for-demographic-variables",
    "href": "assignments/assignment_1/assignment1.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\n# Calculate MOE percentages for white, Black, and Hispanic variables\n# Hint: use the same formula as before (margin/estimate * 100)\n\n# I noticed some of the estimates or margins were 0s, or NaNs, so I'm accounting for that here as well:\ntract_demo_selected_counties &lt;- tract_demo_selected_counties %&gt;%\n  mutate(\n    white_moe_pct    = if_else(whiteE    &gt; 0, (whiteM    / whiteE), NA_real_) * 100,\n    black_moe_pct    = if_else(blackE    &gt; 0, (blackM    / blackE), NA_real_) * 100,\n    hispanic_moe_pct = if_else(hispanicE &gt; 0, (hispanicM / hispanicE), NA_real_) * 100\n  )\n\n# Create a flag for tracts with high MOE on any demographic variable\n# Use logical operators (| for OR) in an ifelse() statement\ntract_demo_selected_counties &lt;- tract_demo_selected_counties %&gt;%\n  mutate(\n    high_moe_flag = ifelse(\n      white_moe_pct &gt; 15 | black_moe_pct &gt; 15 | hispanic_moe_pct &gt; 15,\n      TRUE,\n      FALSE\n    )\n  )\n\n# Create summary statistics showing how many tracts have data quality issues\n\ntract_demo_selected_counties %&gt;%\n  group_by(county_name) %&gt;%\n  summarize(\n    total_tracts    = sum(!is.na(high_moe_flag), na.rm = TRUE),\n    high_moe_tracts = sum(high_moe_flag, na.rm = TRUE),\n  ) %&gt;%\n  kable(\n    caption = \"Tract Data Quality by County\",\n    digits = 0,\n    col.names = c(\"County Name\", \"Number of Tracts (excluding those without data)\", \"Tracts with Quality Issues (&gt;15% MOE)\")\n  )\n\n\nTract Data Quality by County\n\n\n\n\n\n\n\nCounty Name\nNumber of Tracts (excluding those without data)\nTracts with Quality Issues (&gt;15% MOE)\n\n\n\n\nCuyahoga\n421\n421\n\n\nMarion\n18\n18\n\n\nVinton\n2\n2\n\n\n\n\n\nI noticed that there were some tracts with no data, so the sum of tracts excludes those."
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#pattern-analysis",
    "href": "assignments/assignment_1/assignment1.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\n\n# Use group_by() and summarize() to create this comparison\n# Create a professional table showing the patterns\n\ntract_demo_selected_counties %&gt;%\n  group_by(high_moe_flag) %&gt;%\n  summarize(\n    avg_pct_white  = mean(pct_white, na.rm = TRUE),\n    avg_pct_black  = mean(pct_black, na.rm = TRUE),\n    avg_pct_hisp   = mean(pct_hispanic, na.rm = TRUE),\n    n_tracts       = n(),\n    .groups = \"drop\"\n  ) %&gt;%\n  kable(\n    caption = \"Average Demographics by High MOE Status\",\n    digits = 1,\n    col.names = c(\n      \"High MOE Flag\",\n      \"Avg % White\",\n      \"Avg % Black\",\n      \"Avg % Hispanic\",\n      \"Number of Tracts\"\n    )\n  )\n\n\nAverage Demographics by High MOE Status\n\n\n\n\n\n\n\n\n\nHigh MOE Flag\nAvg % White\nAvg % Black\nAvg % Hispanic\nNumber of Tracts\n\n\n\n\nTRUE\n52.6\n33.7\n6.6\n441\n\n\nNA\n98.9\n0.0\n0.0\n8\n\n\n\n\n\nThis summary was simple, but when I did a visual inspection of the data, I see there’s some patterns that this table analysis is obscuring. I see that the MOE for Hispanic residents is above 15% for all tracts! I’ll do a second summary table, this time showing\n\ntract_demo_selected_counties %&gt;%\n  summarize(\n    n_tracts          = n(),\n    white_high_moe    = sum(white_moe_pct    &gt; 15, na.rm = TRUE),\n    black_high_moe    = sum(black_moe_pct    &gt; 15, na.rm = TRUE),\n    hispanic_high_moe = sum(hispanic_moe_pct &gt; 15, na.rm = TRUE),\n    pct_white_high    = 100 * white_high_moe    / n_tracts,\n    pct_black_high    = 100 * black_high_moe    / n_tracts,\n    pct_hispanic_high = 100 * hispanic_high_moe / n_tracts\n  ) %&gt;%\n  kable(\n    caption = \"Tracts with MOE &gt; 15% by Demographic Group\",\n    digits = 1,\n    col.names = c(\n      \"Total Tracts\",\n      \"White High MOE Tracts\",\n      \"Black High MOE Tracts\",\n      \"Hispanic High MOE Tracts\",\n      \"% of Tracts with White High MOE\",\n      \"% of Tracts with Black High MOE\",\n      \"% of Tracts with Hispanic High MOE\"\n    )\n  )\n\n\nTracts with MOE &gt; 15% by Demographic Group\n\n\n\n\n\n\n\n\n\n\n\nTotal Tracts\nWhite High MOE Tracts\nBlack High MOE Tracts\nHispanic High MOE Tracts\n% of Tracts with White High MOE\n% of Tracts with Black High MOE\n% of Tracts with Hispanic High MOE\n\n\n\n\n449\n306\n416\n403\n68.2\n92.7\n89.8\n\n\n\n\n\n\ntract_demo_selected_counties %&gt;%\n  group_by(county_name) %&gt;%\n  summarize(\n    n_tracts          = n(),\n    white_high_moe    = sum(white_moe_pct    &gt; 15, na.rm = TRUE),\n    black_high_moe    = sum(black_moe_pct    &gt; 15, na.rm = TRUE),\n    hispanic_high_moe = sum(hispanic_moe_pct &gt; 15, na.rm = TRUE),\n    pct_white_high    = 100 * white_high_moe    / n_tracts,\n    pct_black_high    = 100 * black_high_moe    / n_tracts,\n    pct_hispanic_high = 100 * hispanic_high_moe / n_tracts,\n    .groups = \"drop\"\n  ) %&gt;%\n  kable(\n    caption = \"County-Level Breakdown of Tracts with MOE &gt; 15%, by Demographic Group\",\n    digits = 1,\n    col.names = c(\n      \"County\",\n      \"Total Tracts\",\n      \"White High MOE Tracts\",\n      \"Black High MOE Tracts\",\n      \"Hispanic High MOE Tracts\",\n      \"% of Tracts with White High MOE\",\n      \"% of Tracts with Black High MOE\",\n      \"% of Tracts with Hispanic High MOE\"\n    )\n  )\n\n\nCounty-Level Breakdown of Tracts with MOE &gt; 15%, by Demographic Group\n\n\n\n\n\n\n\n\n\n\n\n\nCounty\nTotal Tracts\nWhite High MOE Tracts\nBlack High MOE Tracts\nHispanic High MOE Tracts\n% of Tracts with White High MOE\n% of Tracts with Black High MOE\n% of Tracts with Hispanic High MOE\n\n\n\n\nCuyahoga\n428\n298\n402\n383\n69.6\n93.9\n89.5\n\n\nMarion\n18\n8\n12\n18\n44.4\n66.7\n100.0\n\n\nVinton\n3\n0\n2\n2\n0.0\n66.7\n66.7\n\n\n\n\n\nPattern Analysis: A larger share of census tracts flagged as High MOE (&gt;15%) are driven primarily by high median income margins of error among Black or Hispanic populations. The last chart highlights that, with both communities having a higher MOE as a percentage of tracts being marked.\nFor Marion and Vinton counties, this is potentially explained by Black and Hispanic populations being a smaller percentage of the total. This would make the sample sizes for both communities fairly small in these counties. In Cuyahoga county, a visual inspection reveals tracts being flagged when they have small percentages of Hispanic, Black, or White population."
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#analysis-integration-and-professional-summary",
    "href": "assignments/assignment_1/assignment1.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements:\n\nOverall Pattern Identification: What are the systematic patterns across all your analyses?\nEquity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings?\nRoot Cause Analysis: What underlying factors drive both data quality issues and bias risk?\nStrategic Recommendations: What should the Department implement to address these systematic issues?\n\nExecutive Summary:\nAcross the analyses, a pattern emerges: the MOE for demographic data is highly sensitive to both the population size of census tracts and the demographic composition within them. Small-population counties, such as Vinton County, show particularly large MOEs due to limited sample sizes in the ACS. Similarly, tracts with highly diverse populations tend to have inflated MOEs across multiple demographic groups, making nearly every group susceptible to high uncertainty in the estimates.\nIn examining equity implications, Black and Hispanic populations face the highest risk of unreliable data. While the largest MOE observed for White populations is around 200%, Black and Hispanic populations frequently have MOEs in the hundreds of percent and, in extreme cases, as high as 2000%. This suggests that algorithmic decisions relying on this demographic data can be highly unreliable for these communities.\nWe can classify the root causes of these data issues into two categories. First, small sample sizes in low-population tracts and counties amplify MOEs for all estimates. Second, demographic variability can take the form of both extremely small minority populations and highly heterogeneous tracts. This further inflates MOE. An example is Census Tract 1711.02, which is nearly 87% Black but flagged as “High MOE” because the MOE for the White population exceeds 40%.\nTo address these systematic issues, decision-makers should incorporate demographic composition into analyses, weighting MOEs appropriately when interpreting the demographic data. Underrepresented communities should be prioritized for follow-up investigations at the tract or county level to better understand them. By acknowledging and adjusting for these data limitations, we can improve the equity and reliability of policy decisions."
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#specific-recommendations",
    "href": "assignments/assignment_1/assignment1.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\n\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\n\n# Format as a professional table with kable()\n\ncounty_data %&gt;%\n  mutate(\n    algorithm_recommendation = case_when(\n      median_income_moe_cat == \"High Confidence: MOE &lt; 5%\"  ~ \"Safe for algorithmic decisions\",\n      median_income_moe_cat == \"Moderate Confidence: MOE 5-10%\" ~ \"Use with caution - monitor outcomes\",\n      median_income_moe_cat == \"Low Confidence: MOE &gt; 10%\" ~ \"Requires manual review or additional data\",\n      TRUE ~ \"Check data\"\n    )\n  ) %&gt;%\n  select(county_name, median_incomeE, median_income_moe_pct, median_income_moe_cat, algorithm_recommendation) %&gt;%\n  kable(\n    caption = \"County-Level Median Income Reliability and Algorithmic Recommendations\",\n    digits = 1,\n    col.names = c(\n      \"County Name\",\n      \"Median Income\",\n      \"MOE % Median Income\",\n      \"Reliability Category\",\n      \"Algorithm Recommendation\"\n    )\n  )\n\n\nCounty-Level Median Income Reliability and Algorithmic Recommendations\n\n\n\n\n\n\n\n\n\nCounty Name\nMedian Income\nMOE % Median Income\nReliability Category\nAlgorithm Recommendation\n\n\n\n\nAdams\n46234\n8.8\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nAllen\n58976\n3.7\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nAshland\n62254\n4.7\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nAshtabula\n53663\n4.1\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nAthens\n48750\n7.6\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nAuglaize\n75231\n3.7\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nBelmont\n56943\n4.2\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nBrown\n66677\n4.6\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nButler\n77062\n2.1\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nCarroll\n59872\n5.9\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nChampaign\n70486\n5.5\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nClark\n58954\n2.6\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nClermont\n79573\n2.9\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nClinton\n64210\n7.1\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nColumbiana\n55473\n4.0\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nCoshocton\n52048\n5.9\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nCrawford\n52486\n4.2\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nCuyahoga\n60074\n1.2\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nDarke\n60237\n6.5\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nDefiance\n69302\n4.8\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nDelaware\n123995\n1.9\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nErie\n65171\n4.4\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nFairfield\n82969\n3.9\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nFayette\n56773\n6.2\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nFranklin\n71070\n1.3\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nFulton\n71453\n4.5\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nGallia\n55533\n6.4\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nGeauga\n97162\n3.9\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nGreene\n81243\n3.0\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nGuernsey\n53901\n7.7\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nHamilton\n68249\n1.6\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nHancock\n67006\n4.6\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nHardin\n55876\n5.1\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nHarrison\n54056\n6.7\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nHenry\n71616\n5.9\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nHighland\n60522\n5.0\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nHocking\n59007\n4.9\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nHolmes\n72987\n4.1\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nHuron\n64144\n4.1\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nJackson\n56549\n7.8\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nJefferson\n53124\n4.4\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nKnox\n71246\n5.2\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nLake\n76835\n1.7\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nLawrence\n51846\n5.2\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nLicking\n78505\n2.8\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nLogan\n69125\n4.8\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nLorain\n67272\n1.7\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nLucas\n57265\n1.7\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nMadison\n77062\n4.7\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nMahoning\n54279\n2.0\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nMarion\n55106\n8.3\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nMedina\n89968\n2.7\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nMeigs\n46255\n6.9\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nMercer\n73278\n4.5\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nMiami\n71440\n3.2\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nMonroe\n55381\n9.8\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nMontgomery\n61942\n1.6\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nMorgan\n51056\n7.8\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nMorrow\n70412\n6.4\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nMuskingum\n56810\n4.9\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nNoble\n51547\n10.8\nLow Confidence: MOE &gt; 10%\nRequires manual review or additional data\n\n\nOttawa\n69515\n5.0\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nPaulding\n65331\n5.7\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nPerry\n62899\n7.3\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nPickaway\n67600\n4.7\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nPike\n50832\n8.4\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nPortage\n69796\n3.2\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nPreble\n66355\n5.9\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nPutnam\n79453\n4.0\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nRichland\n56557\n3.5\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nRoss\n58048\n4.1\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nSandusky\n60814\n5.0\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nScioto\n46360\n3.6\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nSeneca\n62476\n3.9\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nShelby\n73502\n4.6\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nStark\n63130\n1.8\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nSummit\n68360\n1.3\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nTrumbull\n53537\n3.6\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nTuscarawas\n61953\n3.7\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nUnion\n104496\n2.9\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nVanWert\n64841\n5.4\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nVinton\n50967\n12.3\nLow Confidence: MOE &gt; 10%\nRequires manual review or additional data\n\n\nWarren\n103128\n2.3\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nWashington\n59053\n4.2\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nWayne\n70320\n2.7\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nWilliams\n60632\n3.6\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nWood\n70537\n3.2\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nWyandot\n68552\n7.6\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\n\n\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nCounties suitable for immediate algorithmic implementation: Most Ohio counties are appropriate for immediate algorithmic use; however, some caveats apply. In counties with small or underrepresented populations, certain communities may face higher margins of error, and in highly diverse counties, careful attention or additional algorithmic analysis is needed.\nCounties requiring additional oversight: Adams, Athens, Carroll, Champaign, Clinton, Coshocton, Darke, Fayette, Gallia, Guernsey, Hardin, Harrison, Henry, Highland, Jackson, Knox, Lawrence, Marion, Meigs, Monroe, Morgan, Morrow, Paulding, Perry, Pike, Preble, Sandusky, Van Wert, Wyandot.\n\n\nThese counties can be further categorized based on population and demographic characteristics. In counties with very small populations, targeted local outreach may be sufficient to understand community needs. In larger or more diverse counties, more granular analysis at sub-county levels may be necessary to ensure accurate insights and equitable decision-making.\n\n\nCounties needing alternative approaches: Noble, Vinton.\n\n\nThese counties have some of the lowest populations in Ohio, making traditional ACS estimates highly uncertain. Given the small population size, targeted local outreach—allowing for larger effective sample sizes than the ACS—may be the most feasible approach to accurately assess community needs."
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#questions-for-further-investigation",
    "href": "assignments/assignment_1/assignment1.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\nThe relationship between demographics and population size in determining MOE warrants further investigation. The current analysis, which applies MOE cutoffs based on either demographic or county-level data, involves tradeoffs and may exclude or underrepresent certain populations. Developing a cutoff that incorporates both population size and demographic characteristics could provide a more accurate and equitable assessment, rather than relying single, uniform metrics."
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#submission-checklist",
    "href": "assignments/assignment_1/assignment1.html#submission-checklist",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\nAll code chunks run without errors\nAll “[Fill this in]” prompts have been completed\nTables are properly formatted and readable\nExecutive summary addresses all four required components\nPortfolio navigation includes this assignment\nCensus API key is properly set\nDocument renders correctly to HTML\n\nRemember: Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/your_file_name.html"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html",
    "href": "weekly-notes/week-04-notes.html",
    "title": "Week 4 Notes",
    "section": "",
    "text": "This week’s class is the last piece of the puzzle before we can start building models: doing GIS/spatial visualizations in R.\nRemember: Spatial-autocorrelation violates independence assumptions! Also, training data may under-represent certain areas.\nVector Data Model (one simplified geometric rep. of the real world)\n\nPoints, Lines, Polygons\nEach of the above will have a geometry (shape + location), and attributes (data about that feature)\n\nCoordinate Reference Systems (CRS)\n\nSince the Earth is round, and maps are flat, we need projections! They can’t preserve area, distance, and angles though.\nAlready, we abstract the Earth’s shape. The Earth is a geoid (imperfect surface), and we approximate it using an ellipsoid. There are multiple ellipsoids that can be used to approximate and some can be better depending on where you live.\nThen we must tie the ellipsoid to the real Earth. This is the datum. Ex: Clarke’s 1866 Ellipsoid, centered on Meade’s Ranch, KS.\nThen we can put down a Lat/Long grid. Ex: The North American Datum of 1927 (NAD27) was formed from Clarke’s work.\nNAD83 is another 3D model of the earth that was mapped and uses GRS80.\nWGS84 is what GPS uses.\nAll of the above standards are Geographic Coordinate Systems (GCS).\nThese must then be projected onto a computer screen/flat surface! Recall example of cylindrical projections and the corresponding distortions (Mercator is an example). Can remember how projections mess with things with SADD (Shape, Area, Distance, Direction)\nWe then use Project Coordinate System to provide a localized coordinate system built on a regular non-distorted grid (as opposed to lat/long whihc are in degrees with respect to center of the earth).\n\nOne is UTM, dividing the earth into 60 zones, each of 6 degrees of long. Each zone is projected individually, and is referenced using the Zone (because coordinates are defined from the bottom-left origin of each zone). No negatives, and in meters (with some false northing or easting value added).\nAnother is State Plane (in the US, used in PA for example). Each state has its own projection, reducing distortions at the state level. Similar system as UTM where there’s an origin corner, but the distance can be feet or meters.\n\nGCS (lat/long coordinates, decimal degrees, good for global datasets + web mapping, bad for area/distance)\nPCS ()\nSee below for programmatic considerations"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#key-concepts-learned-notes",
    "href": "weekly-notes/week-04-notes.html#key-concepts-learned-notes",
    "title": "Week 4 Notes",
    "section": "",
    "text": "This week’s class is the last piece of the puzzle before we can start building models: doing GIS/spatial visualizations in R.\nRemember: Spatial-autocorrelation violates independence assumptions! Also, training data may under-represent certain areas.\nVector Data Model (one simplified geometric rep. of the real world)\n\nPoints, Lines, Polygons\nEach of the above will have a geometry (shape + location), and attributes (data about that feature)\n\nCoordinate Reference Systems (CRS)\n\nSince the Earth is round, and maps are flat, we need projections! They can’t preserve area, distance, and angles though.\nAlready, we abstract the Earth’s shape. The Earth is a geoid (imperfect surface), and we approximate it using an ellipsoid. There are multiple ellipsoids that can be used to approximate and some can be better depending on where you live.\nThen we must tie the ellipsoid to the real Earth. This is the datum. Ex: Clarke’s 1866 Ellipsoid, centered on Meade’s Ranch, KS.\nThen we can put down a Lat/Long grid. Ex: The North American Datum of 1927 (NAD27) was formed from Clarke’s work.\nNAD83 is another 3D model of the earth that was mapped and uses GRS80.\nWGS84 is what GPS uses.\nAll of the above standards are Geographic Coordinate Systems (GCS).\nThese must then be projected onto a computer screen/flat surface! Recall example of cylindrical projections and the corresponding distortions (Mercator is an example). Can remember how projections mess with things with SADD (Shape, Area, Distance, Direction)\nWe then use Project Coordinate System to provide a localized coordinate system built on a regular non-distorted grid (as opposed to lat/long whihc are in degrees with respect to center of the earth).\n\nOne is UTM, dividing the earth into 60 zones, each of 6 degrees of long. Each zone is projected individually, and is referenced using the Zone (because coordinates are defined from the bottom-left origin of each zone). No negatives, and in meters (with some false northing or easting value added).\nAnother is State Plane (in the US, used in PA for example). Each state has its own projection, reducing distortions at the state level. Similar system as UTM where there’s an origin corner, but the distance can be feet or meters.\n\nGCS (lat/long coordinates, decimal degrees, good for global datasets + web mapping, bad for area/distance)\nPCS ()\nSee below for programmatic considerations"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#coding-techniquestechnical-notes",
    "href": "weekly-notes/week-04-notes.html#coding-techniquestechnical-notes",
    "title": "Week 4 Notes",
    "section": "Coding Techniques/Technical Notes",
    "text": "Coding Techniques/Technical Notes\n\nsf (simple features) Package\n\nAllows you to read in multiple types of spatial data (Shapefiles, GeoJSON, KML/KMZ, DB connections, etc.) as an sf (similar to how we read csv data as df or tibbles)\n\ntigris Package\n\nContains shape data for the census (as opposed to tidycensus which contains attribute data)\nst_drop_geometry: will drop the geometry column from your table, so you just have attribute data.\n\nYou can use ggplot() to map using + geom_sf()\nYou can filter your data using st_filter(), and add the argument of .predicate = relationshop, where relationship can be something like st_touches, st_intersects, etc.\nAlso important st_ functions, like st_dissolve!\nChecking and Setting CRS\n\nuse st_crs(data) to check the current CRS\niff missing, st_set_crs(data, crs_id).\n\nIFF!!!\n\ntransform CRS using data_transformed &lt;- data %&gt;% st_transform(crs = id)"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#questions-challenges",
    "href": "weekly-notes/week-04-notes.html#questions-challenges",
    "title": "Week 4 Notes",
    "section": "## Questions & Challenges",
    "text": "## Questions & Challenges"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#connections-to-policy",
    "href": "weekly-notes/week-04-notes.html#connections-to-policy",
    "title": "Week 4 Notes",
    "section": "## Connections to Policy",
    "text": "## Connections to Policy"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#reflection",
    "href": "weekly-notes/week-04-notes.html#reflection",
    "title": "Week 4 Notes",
    "section": "## Reflection",
    "text": "## Reflection"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html",
    "href": "weekly-notes/week-05-notes.html",
    "title": "Week 5 Notes",
    "section": "",
    "text": "Statistical Learning\n\nGenerally, we observe data, believe there’s some relationship, and then use some approaches for estimating that relationship. Recall linear regression and other techniques from stats class.\nDefining a fn (f) for Y as a function of some predictors X with some random error (epsilon).\nTwo approaches:\n\nParametric: Makes assumption about functional form (ex: linear), reduces problem to estimate a few parameters, easier to interpet &lt;- Our focus!\nNon-parametric: Doesn’t assume a specific form, more flexible, requires more data to interpret.\n\n\nParametric Approach: Linear Regression - estimating the beta coefficients using sample data.\n\nWe will look at Ordinary Least Squares (OLS)\nWorks really well and is a foundation of more complex models! But, it also makes assumptions that need to be checked and can be susceptible to outliers.\nUsually don’t care about β₀ (the intercept) because it won’t make sense (recall income when population is 0 example). β₁ and other slope coefficients are useful! We test their significance.\n\nWe will set the null hypothesis to β₁ = 0 (i.e there is no change in y w.r.t x). We have our estimated β₁. We will test if we can get the estimate by chaneg if the null hypothesis is true.\nt-stat: How many std errors away from 0? If absolute val of the t-stat is high, we have more confidence that our tested relationship is real…\np-value: Probability of seeing our estimate if null hypothesis is true. Small p -&gt; reject null hypothesis and conclude a rel. exists.\n\n\nIn inferential stats, we use \\(R^2\\) to determine how well a model is.\n\nEx: \\(R^2\\) = 0.208 in the income population example (~21% of variation in income is explained by population)\n\nIs this good? For inference, shows population matters but that other factors exist. For prediction, it’s moderate.\n\nA problem: Overfitting in Regression. Probably can’t generalize to other samples of data. High \\(R^2\\) doesn’t mean good predictions!\n\nTrain/test split on data to evaluate the performance of model the withheld data.\n\nRMSE (Root Mean Squared Estimate)\n\nUseful metric when evaluating how good the prediction is. Ex: an RMSE of 9536 could mean our predictions are off by ~9500 on average.\n\n\nCross-Validation: Multiple train/test splits.\n\nk-fold cross-validation: Recall from stats class. You treat the k fold as test set, iterate through all the folds and repeat.\n\nChecking Assumptions! Recall that regression relies on these, else coeffs may be biases, std errs can be wrong, and predictions can be unreliable.\n\nAssumption 1: Linearity. Check the residual plot! You shouldn’t see any pattern.\nAssumption 2: Constant Variance. You don’t want heteroscedasticity (variance changing across X). Can check the scatter plot or plot of residuals.\n\nCan also test more formally: Breusch-Pagan (bptest from library(lmtest))\n\nA very low (&lt; 0.05) p-value is evidence of heteroscedasticity.\n\n\nAssumption 3: Normality of Residuals. Less critical for point predictors but important for confidence intervals.\n\nTest using Q-Q plot of residuals. Should lay along the line.\n\nAssumption 4: Multicollinearity. Predictors shouldn’t be too correlated. Coeffs become unstable and hard to interpret.\n\nVIF (variance inflation factor) &gt; 10 is a big concern!\n\nAssumption 5: No influential outliers! Only those with high leverage AND large residuals are an issue.\n\nCheck using Cook’s Distance &gt; 4/n = potentially influential. High leverage + large residual will “pull” the regression line.\nDon’t just throw the point away.\n\n\nImproving the Model:\n\nAdd more predictors/features. Can help with heteroscedasticity issues (i.e you have some variables that could account for increasing or decreasing variance across indep. vars).\n\nEx: Categorical Variables (feature engineering/creating new dummy variables).\n\nLog Transformations (if rel is curved, try transforming it). Log models show percentage relationships!"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#key-concepts-learned-notes",
    "href": "weekly-notes/week-05-notes.html#key-concepts-learned-notes",
    "title": "Week 5 Notes",
    "section": "",
    "text": "Statistical Learning\n\nGenerally, we observe data, believe there’s some relationship, and then use some approaches for estimating that relationship. Recall linear regression and other techniques from stats class.\nDefining a fn (f) for Y as a function of some predictors X with some random error (epsilon).\nTwo approaches:\n\nParametric: Makes assumption about functional form (ex: linear), reduces problem to estimate a few parameters, easier to interpet &lt;- Our focus!\nNon-parametric: Doesn’t assume a specific form, more flexible, requires more data to interpret.\n\n\nParametric Approach: Linear Regression - estimating the beta coefficients using sample data.\n\nWe will look at Ordinary Least Squares (OLS)\nWorks really well and is a foundation of more complex models! But, it also makes assumptions that need to be checked and can be susceptible to outliers.\nUsually don’t care about β₀ (the intercept) because it won’t make sense (recall income when population is 0 example). β₁ and other slope coefficients are useful! We test their significance.\n\nWe will set the null hypothesis to β₁ = 0 (i.e there is no change in y w.r.t x). We have our estimated β₁. We will test if we can get the estimate by chaneg if the null hypothesis is true.\nt-stat: How many std errors away from 0? If absolute val of the t-stat is high, we have more confidence that our tested relationship is real…\np-value: Probability of seeing our estimate if null hypothesis is true. Small p -&gt; reject null hypothesis and conclude a rel. exists.\n\n\nIn inferential stats, we use \\(R^2\\) to determine how well a model is.\n\nEx: \\(R^2\\) = 0.208 in the income population example (~21% of variation in income is explained by population)\n\nIs this good? For inference, shows population matters but that other factors exist. For prediction, it’s moderate.\n\nA problem: Overfitting in Regression. Probably can’t generalize to other samples of data. High \\(R^2\\) doesn’t mean good predictions!\n\nTrain/test split on data to evaluate the performance of model the withheld data.\n\nRMSE (Root Mean Squared Estimate)\n\nUseful metric when evaluating how good the prediction is. Ex: an RMSE of 9536 could mean our predictions are off by ~9500 on average.\n\n\nCross-Validation: Multiple train/test splits.\n\nk-fold cross-validation: Recall from stats class. You treat the k fold as test set, iterate through all the folds and repeat.\n\nChecking Assumptions! Recall that regression relies on these, else coeffs may be biases, std errs can be wrong, and predictions can be unreliable.\n\nAssumption 1: Linearity. Check the residual plot! You shouldn’t see any pattern.\nAssumption 2: Constant Variance. You don’t want heteroscedasticity (variance changing across X). Can check the scatter plot or plot of residuals.\n\nCan also test more formally: Breusch-Pagan (bptest from library(lmtest))\n\nA very low (&lt; 0.05) p-value is evidence of heteroscedasticity.\n\n\nAssumption 3: Normality of Residuals. Less critical for point predictors but important for confidence intervals.\n\nTest using Q-Q plot of residuals. Should lay along the line.\n\nAssumption 4: Multicollinearity. Predictors shouldn’t be too correlated. Coeffs become unstable and hard to interpret.\n\nVIF (variance inflation factor) &gt; 10 is a big concern!\n\nAssumption 5: No influential outliers! Only those with high leverage AND large residuals are an issue.\n\nCheck using Cook’s Distance &gt; 4/n = potentially influential. High leverage + large residual will “pull” the regression line.\nDon’t just throw the point away.\n\n\nImproving the Model:\n\nAdd more predictors/features. Can help with heteroscedasticity issues (i.e you have some variables that could account for increasing or decreasing variance across indep. vars).\n\nEx: Categorical Variables (feature engineering/creating new dummy variables).\n\nLog Transformations (if rel is curved, try transforming it). Log models show percentage relationships!"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#coding-techniquestechnical-notes",
    "href": "weekly-notes/week-05-notes.html#coding-techniquestechnical-notes",
    "title": "Week 5 Notes",
    "section": "## Coding Techniques/Technical Notes",
    "text": "## Coding Techniques/Technical Notes"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#questions-challenges",
    "href": "weekly-notes/week-05-notes.html#questions-challenges",
    "title": "Week 5 Notes",
    "section": "## Questions & Challenges",
    "text": "## Questions & Challenges"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#connections-to-policy",
    "href": "weekly-notes/week-05-notes.html#connections-to-policy",
    "title": "Week 5 Notes",
    "section": "## Connections to Policy",
    "text": "## Connections to Policy"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#reflection",
    "href": "weekly-notes/week-05-notes.html#reflection",
    "title": "Week 5 Notes",
    "section": "## Reflection",
    "text": "## Reflection"
  },
  {
    "objectID": "labs/lab-05-model-competition/lab5.html",
    "href": "labs/lab-05-model-competition/lab5.html",
    "title": "Lab 5 - Model Building Competition",
    "section": "",
    "text": "library(caret)\n\n# 10-fold cross-validation\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\ncv_model &lt;- train(home_valueE ~ median_incomeE,\n                  data = challenge_data,\n                  method = \"lm\",\n                  trControl = train_control)\n\ncv_model$results\n\n  intercept     RMSE  Rsquared      MAE   RMSESD RsquaredSD    MAESD\n1      TRUE 23164.79 0.8450294 17813.94 9260.733  0.1289697 6239.799\n\ncv_model &lt;- train(home_valueE ~ median_rentE,\n                  data = challenge_data,\n                  method = \"lm\",\n                  trControl = train_control)\n\ncv_model$results\n\n  intercept     RMSE  Rsquared      MAE   RMSESD RsquaredSD    MAESD\n1      TRUE 27115.71 0.8127802 22232.92 9173.039  0.1277109 7321.037\n\ncv_model &lt;- train(home_valueE ~ log(percent_collegeE),\n                  data = challenge_data,\n                  method = \"lm\",\n                  trControl = train_control)\n\ncv_model$results\n\n  intercept     RMSE  Rsquared      MAE  RMSESD RsquaredSD    MAESD\n1      TRUE 53083.21 0.5353948 43827.74 17168.8   0.262354 14650.44\n\ncv_model &lt;- train(home_valueE ~ median_incomeE + median_rentE + log(percent_collegeE),\n                  data = challenge_data,\n                  method = \"lm\",\n                  trControl = train_control)\ncv_model$results\n\n  intercept     RMSE  Rsquared      MAE   RMSESD RsquaredSD    MAESD\n1      TRUE 20299.88 0.8934533 15887.33 6976.052 0.08245509 6099.722"
  },
  {
    "objectID": "assignments/assignment_2/assignment2.html",
    "href": "assignments/assignment_2/assignment2.html",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "assignments/assignment_2/assignment2.html#assignment-overview",
    "href": "assignments/assignment_2/assignment2.html#assignment-overview",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "assignments/assignment_2/assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "href": "assignments/assignment_2/assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 1: Healthcare Access for Vulnerable Populations",
    "text": "Part 1: Healthcare Access for Vulnerable Populations\n\nResearch Question\nWhich Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\nYour analysis should identify counties that should be priorities for healthcare investment and policy intervention.\n\n\nRequired Analysis Steps\nComplete the following analysis, documenting each step with code and brief explanations:\n\nStep 1: Data Collection (5 points)\nLoad the required spatial data: - Pennsylvania county boundaries - Pennsylvania hospitals (from lecture data) - Pennsylvania census tracts\nYour Task:\n\n# Load required packages ---\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(tigris)\nlibrary(scales)\nlibrary(sf)\n\nlibrary(patchwork)\nlibrary(here)\n\n# Load spatial data ---\npa_counties &lt;- st_read(\"./data/Pennsylvania_County_Boundaries.shp\", quiet = TRUE)\nhospitals &lt;- st_read(\"./data/hospitals.geojson\", quiet = TRUE)\ncensus_tracts &lt;- tracts(state = \"PA\", cb = TRUE, progress_bar = FALSE)\n\n# Check that all data loaded correctly ---\n\n# Checking that the CRS are the same:\ncat(\"pa_counties CRS:\", st_crs(pa_counties)$input, \"\\n\")\n\npa_counties CRS: WGS 84 / Pseudo-Mercator \n\ncat(\"hospitals CRS:\", st_crs(hospitals)$input, \"\\n\")\n\nhospitals CRS: WGS 84 \n\ncat(\"census_tracts CRS:\", st_crs(census_tracts)$input, \"\\n\")\n\ncensus_tracts CRS: NAD83 \n\n# Standardize CRS\nhospitals &lt;- st_transform(hospitals, st_crs(pa_counties))\ncensus_tracts &lt;- st_transform(census_tracts, st_crs(pa_counties))\n\nQuestions to answer:\n\nHow many hospitals are in your dataset?\n\n223 Hospitals\n\nHow many census tracts?\n\n3445 Census Tracts\n\nWhat coordinate reference system is each dataset in?\n\nAs calculated in the above code block:\n\npa_counties CRS: WGS 84 / Pseudo-Mercator\nhospitals CRS: WGS 84\ncensus_tracts CRS: NAD83\n\nTo keep the CRS consistent, I transformed hospitals and census tracts to match that of pa_counties (WGS 84 / Pseudo-Mercator).\n\n\n\n\n\nStep 2: Get Demographic Data\nUse tidycensus to download tract-level demographic data for Pennsylvania.\nRequired variables: - Total population - Median household income - Population 65 years and over (you may need to sum multiple age categories)\nYour Task:\n\n# Determining what variables are needed for &gt;= 65 years:\nacs_vars_2022 &lt;- load_variables(2022, \"acs5\", cache = TRUE) \n\nacs_vars_2022_65_above &lt;- acs_vars_2022 %&gt;% \n  filter(concept == \"Sex by Age\") %&gt;%\n  filter(str_detect(label, \"([6-9][5-9]|[7-9][0-9]|85).*years\"))\n\nacs_age_pop &lt;- acs_vars_2022_65_above$name\n\n# In the wide format, we need names assigned to the 12 population vars based on age and sex\nnames(acs_age_pop) &lt;- acs_vars_2022_65_above$label %&gt;%\n  str_to_lower() %&gt;%\n  str_replace_all(\"estimate!!(total!!)?\", \"\") %&gt;%  # remove 'estimate!!' and 'total!!' if present\n  str_replace_all(\"!!|and| \", \"_\") %&gt;%           # replace !!, 'and', and spaces with _\n  str_replace_all(\"years\", \"\") %&gt;%               # remove 'years'\n  str_replace_all(\"[^a-z0-9_]\", \"\")             # remove any remaining punctuation\n\ntract_demographics &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    median_income = \"B19013_001\",\n    acs_age_pop\n  ),\n  state = \"PA\",\n  year = 2022,\n  output = \"wide\"\n)\n\nfinal_tract_demographics &lt;- tract_demographics %&gt;%\n  mutate(\n    pop_65_plus = rowSums(\n      select(., starts_with(\"total_male_\") | starts_with(\"total_female_\")) %&gt;%\n        select(., ends_with(\"E\")),\n      na.rm = TRUE\n    )\n  ) %&gt;%\n  select(GEOID, NAME, total_popE, median_incomeE, pop_65_plus, \n         total_popM, median_incomeM)\n\n\n# Join to tract boundaries\ntracts_with_data &lt;- census_tracts %&gt;%\n  left_join(final_tract_demographics, by = \"GEOID\")\n\nsum(is.na(tracts_with_data$median_incomeE))\n\n[1] 62\n\nmedian(tracts_with_data$median_incomeE, na.rm = TRUE)\n\n[1] 70188\n\n\nQuestions to answer: - What year of ACS data are you using? - 2022 ACS Data - How many tracts have missing income data? - 62 tracts - What is the median income across all PA census tracts? - 70188 —\n\n\nStep 3: Define Vulnerable Populations\nIdentify census tracts with vulnerable populations based on TWO criteria: 1. Low median household income (choose an appropriate threshold) 2. Significant elderly population (choose an appropriate threshold)\nYour Task:\n\n# Filter for vulnerable tracts based on your criteria\n\nQuestions to answer: - What income threshold did you choose and why? - What elderly population threshold did you choose and why? - How many tracts meet your vulnerability criteria? - What percentage of PA census tracts are considered vulnerable by your definition?\n\n\n\nStep 4: Calculate Distance to Hospitals\nFor each vulnerable tract, calculate the distance to the nearest hospital.\nYour Task:\n\n# Transform to appropriate projected CRS\n\n\n# Calculate distance from each tract centroid to nearest hospital\n\nRequirements: - Use an appropriate projected coordinate system for Pennsylvania - Calculate distances in miles - Explain why you chose your projection\nQuestions to answer: - What is the average distance to the nearest hospital for vulnerable tracts? - What is the maximum distance? - How many vulnerable tracts are more than 15 miles from the nearest hospital?\n\n\n\nStep 5: Identify Underserved Areas\nDefine “underserved” as vulnerable tracts that are more than 15 miles from the nearest hospital.\nYour Task:\n\n# Create underserved variable\n\nQuestions to answer: - How many tracts are underserved? - What percentage of vulnerable tracts are underserved? - Does this surprise you? Why or why not?\n\n\n\nStep 6: Aggregate to County Level\nUse spatial joins and aggregation to calculate county-level statistics about vulnerable populations and hospital access.\nYour Task:\n\n# Spatial join tracts to counties\n\n\n# Aggregate statistics by county\n\nRequired county-level statistics: - Number of vulnerable tracts - Number of underserved tracts\n- Percentage of vulnerable tracts that are underserved - Average distance to nearest hospital for vulnerable tracts - Total vulnerable population\nQuestions to answer: - Which 5 counties have the highest percentage of underserved vulnerable tracts? - Which counties have the most vulnerable people living far from hospitals? - Are there any patterns in where underserved counties are located?\n\n\n\nStep 7: Create Summary Table\nCreate a professional table showing the top 10 priority counties for healthcare investment.\nYour Task:\n\n# Create and format priority counties table\n\nRequirements: - Use knitr::kable() or similar for formatting - Include descriptive column names - Format numbers appropriately (commas for population, percentages, etc.) - Add an informative caption - Sort by priority (you decide the metric)"
  },
  {
    "objectID": "assignments/assignment_2/assignment2.html#part-2-comprehensive-visualization",
    "href": "assignments/assignment_2/assignment2.html#part-2-comprehensive-visualization",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 2: Comprehensive Visualization",
    "text": "Part 2: Comprehensive Visualization\nUsing the skills from Week 3 (Data Visualization), create publication-quality maps and charts.\n\nMap 1: County-Level Choropleth\nCreate a choropleth map showing healthcare access challenges at the county level.\nYour Task:\n\n# Create county-level access map\n\nRequirements: - Fill counties by percentage of vulnerable tracts that are underserved - Include hospital locations as points - Use an appropriate color scheme - Include clear title, subtitle, and caption - Use theme_void() or similar clean theme - Add a legend with formatted labels\n\n\n\nMap 2: Detailed Vulnerability Map\nCreate a map highlighting underserved vulnerable tracts.\nYour Task:\n\n# Create detailed tract-level map\n\nRequirements: - Show underserved vulnerable tracts in a contrasting color - Include county boundaries for context - Show hospital locations - Use appropriate visual hierarchy (what should stand out?) - Include informative title and subtitle\n\n\n\nChart: Distribution Analysis\nCreate a visualization showing the distribution of distances to hospitals for vulnerable populations.\nYour Task:\n\n# Create distribution visualization\n\nSuggested chart types: - Histogram or density plot of distances - Box plot comparing distances across regions - Bar chart of underserved tracts by county - Scatter plot of distance vs. vulnerable population size\nRequirements: - Clear axes labels with units - Appropriate title - Professional formatting - Brief interpretation (1-2 sentences as a caption or in text)"
  },
  {
    "objectID": "assignments/assignment_2/assignment2.html#part-3-bring-your-own-data-analysis",
    "href": "assignments/assignment_2/assignment2.html#part-3-bring-your-own-data-analysis",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 3: Bring Your Own Data Analysis",
    "text": "Part 3: Bring Your Own Data Analysis\nChoose your own additional spatial dataset and conduct a supplementary analysis.\n\nChallenge Options\nChoose ONE of the following challenge exercises, or propose your own research question using OpenDataPhilly data (https://opendataphilly.org/datasets/).\nNote these are just loose suggestions to spark ideas - follow or make your own as the data permits and as your ideas evolve. This analysis should include bringing in your own dataset, ensuring the projection/CRS of your layers align and are appropriate for the analysis (not lat/long or geodetic coordinate systems). The analysis portion should include some combination of spatial and attribute operations to answer a relatively straightforward question\n\n\nEducation & Youth Services\nOption A: Educational Desert Analysis - Data: Schools, Libraries, Recreation Centers, Census tracts (child population) - Question: “Which neighborhoods lack adequate educational infrastructure for children?” - Operations: Buffer schools/libraries (0.5 mile walking distance), identify coverage gaps, overlay with child population density - Policy relevance: School district planning, library placement, after-school program siting\nOption B: School Safety Zones - Data: Schools, Crime Incidents, Bike Network - Question: “Are school zones safe for walking/biking, or are they crime hotspots?” - Operations: Buffer schools (1000ft safety zone), spatial join with crime incidents, assess bike infrastructure coverage - Policy relevance: Safe Routes to School programs, crossing guard placement\n\n\n\nEnvironmental Justice\nOption C: Green Space Equity - Data: Parks, Street Trees, Census tracts (race/income demographics) - Question: “Do low-income and minority neighborhoods have equitable access to green space?” - Operations: Buffer parks (10-minute walk = 0.5 mile), calculate tree canopy or park acreage per capita, compare by demographics - Policy relevance: Climate resilience, environmental justice, urban forestry investment —\n\n\nPublic Safety & Justice\nOption D: Crime & Community Resources - Data: Crime Incidents, Recreation Centers, Libraries, Street Lights - Question: “Are high-crime areas underserved by community resources?” - Operations: Aggregate crime counts to census tracts or neighborhoods, count community resources per area, spatial correlation analysis - Policy relevance: Community investment, violence prevention strategies —\n\n\nInfrastructure & Services\nOption E: Polling Place Accessibility - Data: Polling Places, SEPTA stops, Census tracts (elderly population, disability rates) - Question: “Are polling places accessible for elderly and disabled voters?” - Operations: Buffer polling places and transit stops, identify vulnerable populations, find areas lacking access - Policy relevance: Voting rights, election infrastructure, ADA compliance\n\n\n\nHealth & Wellness\nOption F: Recreation & Population Health - Data: Recreation Centers, Playgrounds, Parks, Census tracts (demographics) - Question: “Is lack of recreation access associated with vulnerable populations?” - Operations: Calculate recreation facilities per capita by neighborhood, buffer facilities for walking access, overlay with demographic indicators - Policy relevance: Public health investment, recreation programming, obesity prevention\n\n\n\nEmergency Services\nOption G: EMS Response Coverage - Data: Fire Stations, EMS stations, Population density, High-rise buildings - Question: “Are population-dense areas adequately covered by emergency services?” - Operations: Create service area buffers (5-minute drive = ~2 miles), assess population coverage, identify gaps in high-density areas - Policy relevance: Emergency preparedness, station siting decisions\n\n\n\nArts & Culture\nOption H: Cultural Asset Distribution - Data: Public Art, Museums, Historic sites/markers, Neighborhoods - Question: “Do all neighborhoods have equitable access to cultural amenities?” - Operations: Count cultural assets per neighborhood, normalize by population, compare distribution across demographic groups - Policy relevance: Cultural equity, tourism, quality of life, neighborhood identity\n\n\n\n\nData Sources\nOpenDataPhilly: https://opendataphilly.org/datasets/ - Most datasets available as GeoJSON, Shapefile, or CSV with coordinates - Always check the Metadata for a data dictionary of the fields.\nAdditional Sources: - Pennsylvania Open Data: https://data.pa.gov/ - Census Bureau (via tidycensus): Demographics, economic indicators, commute patterns - TIGER/Line (via tigris): Geographic boundaries\n\n\nRecommended Starting Points\nIf you’re feeling confident: Choose an advanced challenge with multiple data layers. If you are a beginner, choose something more manageable that helps you understand the basics\nIf you have a different idea: Propose your own question! Just make sure: - You can access the spatial data - You can perform at least 2 spatial operations\n\n\nYour Analysis\nYour Task:\n\nFind and load additional data\n\nDocument your data source\nCheck and standardize the CRS\nProvide basic summary statistics\n\n\n\n# Load your additional dataset\n\nQuestions to answer: - What dataset did you choose and why? - What is the data source and date? - How many features does it contain? - What CRS is it in? Did you need to transform it?\n\n\nPose a research question\n\nWrite a clear research statement that your analysis will answer.\nExamples: - “Do vulnerable tracts have adequate public transit access to hospitals?” - “Are EMS stations appropriately located near vulnerable populations?” - “Do areas with low vehicle access have worse hospital access?”\n\n\nConduct spatial analysis\n\nUse at least TWO spatial operations to answer your research question.\nRequired operations (choose 2+): - Buffers - Spatial joins - Spatial filtering with predicates - Distance calculations - Intersections or unions - Point-in-polygon aggregation\nYour Task:\n\n# Your spatial analysis\n\nAnalysis requirements: - Clear code comments explaining each step - Appropriate CRS transformations - Summary statistics or counts - At least one map showing your findings - Brief interpretation of results (3-5 sentences)\nYour interpretation:\n[Write your findings here]"
  },
  {
    "objectID": "assignments/assignment_2/assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "href": "assignments/assignment_2/assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Finally - A few comments about your incorporation of feedback!",
    "text": "Finally - A few comments about your incorporation of feedback!\nTake a few moments to clean up your markdown document and then write a line or two or three about how you may have incorporated feedback that you recieved after your first assignment."
  },
  {
    "objectID": "assignments/assignment_2/assignment2.html#submission-requirements",
    "href": "assignments/assignment_2/assignment2.html#submission-requirements",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Submission Requirements",
    "text": "Submission Requirements\nWhat to submit:\n\nRendered HTML document posted to your course portfolio with all code, outputs, maps, and text\n\nUse embed-resources: true in YAML so it’s a single file\nAll code should run without errors\nAll maps and charts should display correctly\n\n\nFile naming: LastName_FirstName_Assignment2.html and LastName_FirstName_Assignment2.qmd"
  },
  {
    "objectID": "assignments/assignment_2/Kakumanu_Sujan_Assignment2.html",
    "href": "assignments/assignment_2/Kakumanu_Sujan_Assignment2.html",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "assignments/assignment_2/Kakumanu_Sujan_Assignment2.html#assignment-overview",
    "href": "assignments/assignment_2/Kakumanu_Sujan_Assignment2.html#assignment-overview",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "assignments/assignment_2/Kakumanu_Sujan_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "href": "assignments/assignment_2/Kakumanu_Sujan_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 1: Healthcare Access for Vulnerable Populations",
    "text": "Part 1: Healthcare Access for Vulnerable Populations\n\nResearch Question\nWhich Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\nYour analysis should identify counties that should be priorities for healthcare investment and policy intervention.\n\n\nRequired Analysis Steps\nComplete the following analysis, documenting each step with code and brief explanations:\n\nStep 1: Data Collection (5 points)\nLoad the required spatial data: - Pennsylvania county boundaries - Pennsylvania hospitals (from lecture data) - Pennsylvania census tracts\nYour Task:\n\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(tigris)\nlibrary(scales)\nlibrary(sf)\nlibrary(knitr)\nlibrary(patchwork)\n\nlibrary(patchwork)\nlibrary(here)\n\npa_counties &lt;- st_read(\"./data/Pennsylvania_County_Boundaries.shp\", quiet = TRUE)\nhospitals &lt;- st_read(\"./data/hospitals.geojson\", quiet = TRUE)\ncensus_tracts &lt;- tracts(state = \"PA\", cb = TRUE, progress_bar = FALSE)\n\n# Checking the CRS:\ncat(\"pa_counties CRS:\", st_crs(pa_counties)$input, \" EPSG:\", st_crs(pa_counties)$epsg, \"\\n\")\n\npa_counties CRS: WGS 84 / Pseudo-Mercator  EPSG: 3857 \n\ncat(\"hospitals CRS:\", st_crs(hospitals)$input, \" EPSG:\", st_crs(hospitals)$epsg, \"\\n\")\n\nhospitals CRS: WGS 84  EPSG: 4326 \n\ncat(\"census_tracts CRS:\", st_crs(census_tracts)$input, \" EPSG:\", st_crs(hospitals)$epsg, \"\\n\")\n\ncensus_tracts CRS: NAD83  EPSG: 4326 \n\n\nQuestions to answer:\n\nHow many hospitals are in your dataset?\n\n223 Hospitals\n\nHow many census tracts?\n\n3445 Census Tracts\n\nWhat coordinate reference system is each dataset in?\n\nAs calculated in the above code block:\n\npa_counties CRS: WGS 84 / Pseudo-Mercator EPSG: 3857\nhospitals CRS: WGS 84 EPSG: 4326\ncensus_tracts CRS: NAD83 EPSG: 4326\n\n\n\n\n\n\nStep 2: Get Demographic Data\nUse tidycensus to download tract-level demographic data for Pennsylvania.\nRequired variables: - Total population - Median household income - Population 65 years and over (you may need to sum multiple age categories)\nYour Task:\n\n# Determining what variables are needed for &gt;= 65 years:\nacs_vars_2022 &lt;- load_variables(2022, \"acs5\", cache = TRUE) \n\nacs_vars_2022_65_above &lt;- acs_vars_2022 %&gt;% \n  filter(concept == \"Sex by Age\") %&gt;%\n  filter(str_detect(label, \"([6-9][5-9]|[7-9][0-9]|85).*years\"))\n\nacs_age_pop &lt;- acs_vars_2022_65_above$name\n\n# In the wide format, we need names assigned to the 12 population vars based on age and sex\nnames(acs_age_pop) &lt;- acs_vars_2022_65_above$label %&gt;%\n  str_to_lower() %&gt;%\n  str_replace_all(\"estimate!!(total!!)?\", \"\") %&gt;%  # remove 'estimate!!' and 'total!!' if present\n  str_replace_all(\"!!|and| \", \"_\") %&gt;%           # replace !!, 'and', and spaces with _\n  str_replace_all(\"years\", \"\") %&gt;%               # remove 'years'\n  str_replace_all(\"[^a-z0-9_]\", \"\")             # remove any remaining punctuation\n\ntract_demographics &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    median_income = \"B19013_001\",\n    acs_age_pop\n  ),\n  state = \"PA\",\n  year = 2022,\n  output = \"wide\"\n)\n\nfinal_tract_demographics &lt;- tract_demographics %&gt;%\n  mutate(\n    pop_65_plus = rowSums(\n      select(., starts_with(\"total_male_\") | starts_with(\"total_female_\")) %&gt;%\n        select(., ends_with(\"E\")),\n      na.rm = TRUE\n    )\n  ) %&gt;%\n  select(GEOID, NAME, total_popE, median_incomeE, pop_65_plus, \n         total_popM, median_incomeM)\n\n\n# Join to tract boundaries\ntracts_with_data &lt;- census_tracts %&gt;%\n  left_join(final_tract_demographics, by = \"GEOID\")\n\nsum(is.na(tracts_with_data$median_incomeE))\n\n[1] 62\n\nmedian(tracts_with_data$median_incomeE, na.rm = TRUE)\n\n[1] 70188\n\n\nQuestions to answer:\n\nWhat year of ACS data are you using?\n\n2022 ACS Data\n\nHow many tracts have missing income data?\n\n62 tracts\n\nWhat is the median income across all PA census tracts?\n\n$70188\n\n\n\n\n\nStep 3: Define Vulnerable Populations\nIdentify census tracts with vulnerable populations based on TWO criteria: 1. Low median household income (choose an appropriate threshold) 2. Significant elderly population (choose an appropriate threshold)\nYour Task:\n\ntracts_with_data &lt;- tracts_with_data %&gt;%\n  mutate(pct_65_plus = (pop_65_plus / total_popE) * 100)\n\nincome_threshold &lt;- quantile(\n  tracts_with_data$median_incomeE, \n  0.25, \n  na.rm = TRUE\n)\nelderly_threshold &lt;- quantile(\n  tracts_with_data$pct_65_plus,\n  0.75,\n  na.rm = TRUE\n)\n\ntracts_with_data &lt;- tracts_with_data %&gt;%\n  mutate(\n    low_income = median_incomeE &lt;= income_threshold,\n    high_elderly = pct_65_plus &gt;= elderly_threshold,\n    vulnerable = low_income & high_elderly\n  )\n\nn_vulnerable_tracts &lt;- sum(tracts_with_data$vulnerable, na.rm = TRUE)\npct_vulnerable_tracts &lt;- n_vulnerable_tracts / nrow(tracts_with_data) * 100\n\nQuestions to answer:\n\nWhat income threshold did you choose and why?\n\nI went with a quantile approach, because I wanted to see what median incomes are especially low in the context of Pennsylvania. The threshold is the bottom 25% of median incomes in the state. Based on 2022 data, that is $55924. This approach can also stand the test of time, because it will update as we get new data.\n\nWhat elderly population threshold did you choose and why?\n\nI went with a similar approach as above, but this time picking the counties that are in the top 25% of tracts with population above 65 This is a dynamic cutoff, but with 2022 data it is 23.1% of the population being &gt;=65.\n\nHow many tracts meet your vulnerability criteria?\n\n165 tracts meet this vulnerability criteria (having income in the lowest 25% of PA census tracts AND having an elderly population in the top 25%)\n\nWhat percentage of PA census tracts are considered vulnerable by your definition?\n\n~4.79% of PA census tracts meet this criteria. (165 vulnerable / 3445 total)\n\n\n\n\n\nStep 4: Calculate Distance to Hospitals\nFor each vulnerable tract, calculate the distance to the nearest hospital.\nYour Task:\n\n# Transform to appropriate projected CRS\npa_projection &lt;- 3365 # NAD83 / Pennsylvania South\n\npa_counties &lt;- st_transform(pa_counties, crs = pa_projection)\ntracts_with_data &lt;- st_transform(tracts_with_data, crs = pa_projection)\nhospitals &lt;- st_transform(hospitals, crs = pa_projection)\n\ncat(\"pa_counties CRS:\", st_crs(pa_counties)$input, \"\\n\")\n\npa_counties CRS: EPSG:3365 \n\ncat(\"hospitals CRS:\", st_crs(hospitals)$input, \"\\n\")\n\nhospitals CRS: EPSG:3365 \n\ncat(\"census_tracts CRS:\", st_crs(tracts_with_data)$input, \"\\n\\n\")\n\ncensus_tracts CRS: EPSG:3365 \n\n# Calculate distance from each tract centroid to nearest hospital\n\ntract_centroids &lt;- st_centroid(tracts_with_data)\ntracts_with_data &lt;- tracts_with_data %&gt;%\n  mutate(\n    dist_to_hospital = st_distance(tract_centroids, hospitals)  %&gt;% \n    apply(1, min) %&gt;% \n    as.numeric() / 1609.34\n  )\n\ntracts_with_data %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(vulnerable == TRUE) %&gt;%\n  summarise(\n    avg_distance = mean(dist_to_hospital, na.rm = TRUE),\n    max_distance = max(dist_to_hospital, na.rm = TRUE),\n    more_than_15_miles = sum(dist_to_hospital &gt; 15, na.rm = TRUE),\n  )\n\n  avg_distance max_distance more_than_15_miles\n1     15.57102     62.85519                 56\n\n\nRequirements:\n\nUse an appropriate projected coordinate system for Pennsylvania\nCalculate distances in miles\n\nDone with “as.numeric() / 1609.34”.\n\nExplain why you chose your projection\n\nI used NAD83 / Pennsylvania South (EPSG:3365) for this statewide analysis. Pennsylvania spans two State Plane zones designed to reduce distortion across the state’s large east-west extent. Rather than conducting separate analyses for each zone, I applied the South zone projection statewide to balance efficiency with measurement accuracy. Some of the northern counties will experience distortion as a result.\n\n\nQuestions to answer:\n\nWhat is the average distance to the nearest hospital for vulnerable tracts?\n\n~15.57 miles\n\nWhat is the maximum distance?\n\n~62.86 miles\n\nHow many vulnerable tracts are more than 15 miles from the nearest hospital?\n\n56 tracts\n\n\n\n\n\nStep 5: Identify Underserved Areas\nDefine “underserved” as vulnerable tracts that are more than 15 miles from the nearest hospital.\nYour Task:\n\ntracts_with_data &lt;- tracts_with_data %&gt;%\n  mutate(\n    underserved = vulnerable & dist_to_hospital &gt; 15\n  )\n\ntracts_with_data %&gt;%\n  st_drop_geometry() %&gt;%\n  summarise(\n    n_underserved = sum(underserved, na.rm = TRUE),\n    n_vulnerable = sum(vulnerable, na.rm = TRUE),\n    pct_vulnerable_underserved = sum(underserved, na.rm = TRUE) / sum(vulnerable, na.rm = TRUE) * 100\n  )\n\n  n_underserved n_vulnerable pct_vulnerable_underserved\n1            56          165                   33.93939\n\n# Doing some data explortation to see what distinct counties contain the underserved counties\n# tracts_with_data %&gt;%\n#   st_drop_geometry() %&gt;%\n#   filter(underserved == TRUE) %&gt;%\n#   distinct(NAMELSADCO) %&gt;%\n#   arrange(NAMELSADCO)\n\nQuestions to answer:\n\nHow many tracts are underserved?\n\n56 tracts\n\nWhat percentage of vulnerable tracts are underserved?\n\n33.94%\n\nDoes this surprise you? Why or why not?\n\nNot entirely. I explored the counties that contain these underserved tracts and most of them are in rural Pennsylvania, where it’s reasonable to expect an aging population and lower income levels. Rural counties are also where you can expect large distances to hospitals.\n\n\n\n\n\nStep 6: Aggregate to County Level\nUse spatial joins and aggregation to calculate county-level statistics about vulnerable populations and hospital access.\nYour Task:\n\n# Spatial join tracts to counties\ntracts_by_county &lt;- tracts_with_data %&gt;% \n  st_join(pa_counties %&gt;% select(COUNTY_NAM))\n\n# tracts_by_county is larger than tracts_with_data, which seems to be because tracts are appearing for multiple counties. For the sake of this analysis, I will do the join again but tie a census tract to the county it's mostly in (largest county).\n\ntracts_by_county &lt;- tracts_with_data %&gt;% \n  st_join(pa_counties %&gt;% select(COUNTY_NAM), largest = TRUE)\n\n# Aggregate statistics by county\ncounty_stats &lt;- tracts_by_county %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(COUNTY_NAM) %&gt;%\n  summarise(\n    total_tracts = n(), # including to understand the scale of the county\n    vulnerable_tracts = sum(vulnerable, na.rm = TRUE),\n    underserved_tracts = sum(underserved, na.rm = TRUE),\n    pct_vulnerable_underserved = round(\n      ifelse(vulnerable_tracts &gt; 0, \n             (underserved_tracts / vulnerable_tracts) * 100, 0), 1),\n    avg_dist_to_hospital = round(mean(dist_to_hospital, na.rm = TRUE), 2),\n    total_pop_in_vulnerable = sum(total_popE[vulnerable == TRUE], na.rm = TRUE),\n  )  %&gt;%\n  arrange(desc(pct_vulnerable_underserved))\n\ncounty_stats %&gt;%\n  kable(col.names = c(\"County\", \"Total Tracts\", \"Vulnerable Tracts\", \n                      \"Underserved Tracts\", \"% Vulnerable\", \"Avg Distance (mi)\", \"Total Pop in Vulnerable Tracts\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCounty\nTotal Tracts\nVulnerable Tracts\nUnderserved Tracts\n% Vulnerable\nAvg Distance (mi)\nTotal Pop in Vulnerable Tracts\n\n\n\n\nARMSTRONG\n19\n1\n1\n100.0\n26.91\n2106\n\n\nBRADFORD\n15\n1\n1\n100.0\n33.40\n5466\n\n\nCLARION\n13\n2\n2\n100.0\n28.45\n3586\n\n\nCLINTON\n10\n1\n1\n100.0\n31.59\n3176\n\n\nELK\n9\n1\n1\n100.0\n26.96\n1557\n\n\nFOREST\n2\n1\n1\n100.0\n60.38\n2701\n\n\nHUNTINGDON\n13\n1\n1\n100.0\n32.19\n2558\n\n\nINDIANA\n24\n2\n2\n100.0\n19.06\n5703\n\n\nJUNIATA\n6\n1\n1\n100.0\n49.24\n1782\n\n\nLANCASTER\n116\n1\n1\n100.0\n17.15\n4387\n\n\nLEBANON\n33\n1\n1\n100.0\n13.89\n3107\n\n\nLYCOMING\n32\n1\n1\n100.0\n14.63\n2052\n\n\nMONROE\n51\n1\n1\n100.0\n32.97\n1299\n\n\nNORTHUMBERLAND\n25\n4\n4\n100.0\n32.30\n9087\n\n\nSULLIVAN\n4\n1\n1\n100.0\n74.39\n918\n\n\nSUSQUEHANNA\n12\n1\n1\n100.0\n25.63\n1612\n\n\nVENANGO\n17\n1\n1\n100.0\n22.35\n4785\n\n\nWAYNE\n17\n1\n1\n100.0\n30.51\n2445\n\n\nCLEARFIELD\n20\n4\n3\n75.0\n43.07\n13056\n\n\nSOMERSET\n24\n4\n3\n75.0\n20.99\n11454\n\n\nMCKEAN\n12\n3\n2\n66.7\n24.22\n7832\n\n\nPOTTER\n7\n3\n2\n66.7\n36.16\n9062\n\n\nWARREN\n13\n3\n2\n66.7\n21.05\n8041\n\n\nBEDFORD\n12\n2\n1\n50.0\n31.23\n7578\n\n\nCRAWFORD\n23\n4\n2\n50.0\n24.01\n9072\n\n\nDAUPHIN\n67\n2\n1\n50.0\n15.09\n8410\n\n\nJEFFERSON\n13\n2\n1\n50.0\n15.63\n4566\n\n\nLAWRENCE\n28\n2\n1\n50.0\n16.65\n4200\n\n\nFAYETTE\n36\n7\n3\n42.9\n18.45\n25230\n\n\nBLAIR\n38\n3\n1\n33.3\n9.24\n7602\n\n\nCAMBRIA\n42\n6\n2\n33.3\n17.08\n13551\n\n\nNORTHAMPTON\n74\n3\n1\n33.3\n13.58\n10073\n\n\nLUZERNE\n101\n7\n2\n28.6\n14.26\n21062\n\n\nWESTMORELAND\n113\n11\n3\n27.3\n14.57\n25093\n\n\nERIE\n73\n4\n1\n25.0\n17.28\n12216\n\n\nBEAVER\n53\n5\n1\n20.0\n15.16\n9763\n\n\nALLEGHENY\n394\n22\n1\n4.5\n7.45\n51790\n\n\nADAMS\n27\n0\n0\n0.0\n20.58\n0\n\n\nBERKS\n107\n0\n0\n0.0\n13.78\n0\n\n\nBUCKS\n147\n0\n0\n0.0\n12.51\n0\n\n\nBUTLER\n47\n0\n0\n0.0\n19.28\n0\n\n\nCAMERON\n2\n0\n0\n0.0\n62.56\n0\n\n\nCARBON\n17\n0\n0\n0.0\n19.58\n0\n\n\nCENTRE\n41\n0\n0\n0.0\n18.69\n0\n\n\nCHESTER\n124\n0\n0\n0.0\n22.04\n0\n\n\nCOLUMBIA\n16\n0\n0\n0.0\n12.66\n0\n\n\nCUMBERLAND\n55\n1\n0\n0.0\n17.00\n2438\n\n\nDELAWARE\n152\n1\n0\n0.0\n7.61\n2972\n\n\nFRANKLIN\n33\n2\n0\n0.0\n16.13\n7949\n\n\nFULTON\n3\n0\n0\n0.0\n23.65\n0\n\n\nGREENE\n10\n0\n0\n0.0\n24.44\n0\n\n\nLACKAWANNA\n60\n5\n0\n0.0\n8.85\n13322\n\n\nLEHIGH\n81\n1\n0\n0.0\n9.77\n3974\n\n\nMERCER\n35\n5\n0\n0.0\n12.60\n16686\n\n\nMIFFLIN\n13\n0\n0\n0.0\n18.31\n0\n\n\nMONTGOMERY\n219\n1\n0\n0.0\n8.26\n2069\n\n\nMONTOUR\n4\n0\n0\n0.0\n10.31\n0\n\n\nPERRY\n10\n0\n0\n0.0\n43.52\n0\n\n\nPHILADELPHIA\n408\n16\n0\n0.0\n3.65\n61873\n\n\nPIKE\n25\n0\n0\n0.0\n63.07\n0\n\n\nSCHUYLKILL\n42\n3\n0\n0.0\n21.05\n11194\n\n\nSNYDER\n8\n0\n0\n0.0\n47.46\n0\n\n\nTIOGA\n10\n1\n0\n0.0\n35.01\n3459\n\n\nUNION\n11\n2\n0\n0.0\n22.49\n9762\n\n\nWASHINGTON\n62\n3\n0\n0.0\n14.51\n5008\n\n\nWYOMING\n7\n0\n0\n0.0\n42.57\n0\n\n\nYORK\n108\n4\n0\n0.0\n16.18\n11199\n\n\n\n\ncounty_stats %&gt;%\n  head(18) %&gt;%\n  select(COUNTY_NAM, vulnerable_tracts, underserved_tracts, pct_vulnerable_underserved)\n\n# A tibble: 18 × 4\n   COUNTY_NAM     vulnerable_tracts underserved_tracts pct_vulnerable_underser…¹\n   &lt;chr&gt;                      &lt;int&gt;              &lt;int&gt;                     &lt;dbl&gt;\n 1 ARMSTRONG                      1                  1                       100\n 2 BRADFORD                       1                  1                       100\n 3 CLARION                        2                  2                       100\n 4 CLINTON                        1                  1                       100\n 5 ELK                            1                  1                       100\n 6 FOREST                         1                  1                       100\n 7 HUNTINGDON                     1                  1                       100\n 8 INDIANA                        2                  2                       100\n 9 JUNIATA                        1                  1                       100\n10 LANCASTER                      1                  1                       100\n11 LEBANON                        1                  1                       100\n12 LYCOMING                       1                  1                       100\n13 MONROE                         1                  1                       100\n14 NORTHUMBERLAND                 4                  4                       100\n15 SULLIVAN                       1                  1                       100\n16 SUSQUEHANNA                    1                  1                       100\n17 VENANGO                        1                  1                       100\n18 WAYNE                          1                  1                       100\n# ℹ abbreviated name: ¹​pct_vulnerable_underserved\n\ntracts_by_county %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(underserved == TRUE) %&gt;%\n  group_by(COUNTY_NAM) %&gt;%\n  summarise(\n    underserved_tracts = n(),\n    vulnerable_pop_far_from_hospitals = sum(total_popE, na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(vulnerable_pop_far_from_hospitals)) %&gt;%\n  head(5)\n\n# A tibble: 5 × 3\n  COUNTY_NAM     underserved_tracts vulnerable_pop_far_from_hospitals\n  &lt;chr&gt;                       &lt;int&gt;                             &lt;dbl&gt;\n1 FAYETTE                         3                             13304\n2 NORTHUMBERLAND                  4                              9087\n3 WESTMORELAND                    3                              8574\n4 CLEARFIELD                      3                              7771\n5 SOMERSET                        3                              7209\n\n\nRequired county-level statistics:\n\nNumber of vulnerable tracts\nNumber of underserved tracts\n\nPercentage of vulnerable tracts that are underserved\nAverage distance to nearest hospital for vulnerable tracts\nTotal vulnerable population\n\nQuestions to answer:\n\nWhich 5 counties have the highest percentage of underserved vulnerable tracts?\n\nBased on my criteria, 18 counties have the same percentage of vulnerable tracts that are underserved. Those 18 counties are printed above.\n\nWhich counties have the most vulnerable people living far from hospitals?\n\nFayette, Northumberland, Westmoreland, and Clearfield, and Somerset.\n\nAre there any patterns in where underserved counties are located?\n\nMost of the counties (and all of the counties with the most vulnerable people) are large rural counties.\n\n\n\n\n\nStep 7: Create Summary Table\nCreate a professional table showing the top 10 priority counties for healthcare investment.\nYour Task:\n\ntracts_by_county %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(underserved == TRUE) %&gt;%\n  group_by(COUNTY_NAM) %&gt;%\n  summarise(\n    underserved_tracts = n(),\n    vulnerable_pop_far_from_hospitals = sum(total_popE, na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(vulnerable_pop_far_from_hospitals)) %&gt;%\n  head(10) %&gt;%\n  kable(\n    col.names = c(\"County\", \"Underserved Tracts\", \"Total Population\"),\n    align = c(\"l\", \"r\", \"r\"),\n    format.args = list(big.mark = \",\"),\n    caption = \"Top 10 Priority Counties for Healthcare Investment - Ranked by total population in underserved vulnerable tracts (&gt;15 miles from nearest hospital)\"\n  )\n\n\nTop 10 Priority Counties for Healthcare Investment - Ranked by total population in underserved vulnerable tracts (&gt;15 miles from nearest hospital)\n\n\nCounty\nUnderserved Tracts\nTotal Population\n\n\n\n\nFAYETTE\n3\n13,304\n\n\nNORTHUMBERLAND\n4\n9,087\n\n\nWESTMORELAND\n3\n8,574\n\n\nCLEARFIELD\n3\n7,771\n\n\nSOMERSET\n3\n7,209\n\n\nLUZERNE\n2\n5,728\n\n\nINDIANA\n2\n5,703\n\n\nBRADFORD\n1\n5,466\n\n\nMCKEAN\n2\n5,452\n\n\nWARREN\n2\n5,103\n\n\n\n\n\nNote: Vulnerable tracts are defined as those in the bottom 25% for median household income and top 25% for percentage of population 65+. Underserved tracts are vulnerable tracts located more than 15 miles from the nearest hospital.\nRequirements: - Use knitr::kable() or similar for formatting - Include descriptive column names - Format numbers appropriately (commas for population, percentages, etc.) - Add an informative caption - Sort by priority (you decide the metric)"
  },
  {
    "objectID": "assignments/assignment_2/Kakumanu_Sujan_Assignment2.html#part-2-comprehensive-visualization",
    "href": "assignments/assignment_2/Kakumanu_Sujan_Assignment2.html#part-2-comprehensive-visualization",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 2: Comprehensive Visualization",
    "text": "Part 2: Comprehensive Visualization\nUsing the skills from Week 3 (Data Visualization), create publication-quality maps and charts.\n\nMap 1: County-Level Choropleth\nCreate a choropleth map showing healthcare access challenges at the county level.\nYour Task:\n\ncounty_map_data &lt;- pa_counties %&gt;%\n  left_join(county_stats, by = \"COUNTY_NAM\")\n\nggplot() +\n  geom_sf(data = county_map_data, \n          aes(fill = pct_vulnerable_underserved),\n          color = \"white\", \n          size = 0.3) +\n  geom_sf(data = hospitals, \n          color = \"navy\", \n          size = 1.5, \n          alpha = 0.75,\n          shape = 3) +\n  scale_fill_gradient(\n    low = \"#fee6ce\",\n    high = \"#a63603\",\n    na.value = \"grey90\",\n    name = \"% of Vulnerable Census Tracts\\nthat are Underserved \",\n    labels = function(x) paste0(x, \"%\")\n  ) +\n  labs(\n    title = \"Healthcare Access Challenges in Pennsylvania\",\n    subtitle = \"Percentage of vulnerable tracts in a county that are underserved (&gt;15 miles from a hospital)\",\n    caption = \"Vulnerable tracts: bottom 25% in median income & top 25% in population 65+, relative to the rest of PA\\nBlue crosses indicate hospital locations\"\n  ) +\n  theme_void() +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(size = 10, hjust = 0.5),\n    plot.caption = element_text(size = 8, hjust = 0.5),\n    legend.position = \"left\"\n  )\n\n\n\n\n\n\n\n\nRequirements: - Fill counties by percentage of vulnerable tracts that are underserved - Include hospital locations as points - Use an appropriate color scheme - Include clear title, subtitle, and caption - Use theme_void() or similar clean theme - Add a legend with formatted labels\n\n\n\nMap 2: Detailed Vulnerability Map\nCreate a map highlighting underserved vulnerable tracts.\nYour Task:\n\nggplot() +\n  geom_sf(data = tracts_with_data, \n          fill = \"gray85\", \n          color = \"white\", \n          size = 0.1) +\n  geom_sf(data = tracts_with_data %&gt;% filter(underserved == TRUE), \n          fill = \"maroon\",\n          color = \"white\", \n          size = 0.1) +\n  geom_sf(data = pa_counties, \n          fill = NA, \n          color = \"black\", \n          size = 0.5) +\n  # Hospital locations\n  geom_sf(data = hospitals, \n          color = \"navy\", \n          size = 1.5, \n          alpha = 0.75,\n          shape = 3) +\n  labs(\n    title = \"Underserved Vulnerable Census Tracts in Pennsylvania\",\n    subtitle = \"Red areas show vulnerable tracts where the center is more than 15 miles from the nearest hospital\",\n    caption = \"Vulnerable tracts: bottom 25% in median income & top 25% in population 65+, relative to the rest of PA\\nBlue crosses indicate hospital locations\"\n  ) +\n  theme_void() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14, hjust = 0.5),\n    plot.subtitle = element_text(size = 10, hjust = 0.5, margin = margin(b = 10)),\n    plot.caption = element_text(size = 8, hjust = 0, margin = margin(t = 10)),\n    plot.margin = margin(20, 20, 20, 20)\n  )\n\n\n\n\n\n\n\n\nRequirements: - Show underserved vulnerable tracts in a contrasting color - Include county boundaries for context - Show hospital locations - Use appropriate visual hierarchy (what should stand out?) - Include informative title and subtitle\n\n\n\nChart: Distribution Analysis\nCreate a visualization showing the distribution of distances to hospitals for vulnerable populations.\nYour Task:\n\n# Create distribution visualization\ntracts_with_data %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(vulnerable == TRUE) %&gt;%\n  ggplot(aes(x = dist_to_hospital)) +\n  geom_histogram(bins = 30, fill = \"navy\", color = \"white\") +\n  geom_vline(xintercept = 15, linetype = \"dashed\", color = \"maroon\", size = 1) +\n  labs(\n    title = \"Distance to Nearest Hospital for Vulnerable Tracts\",\n    x = \"Distance (miles)\",\n    y = \"Number of Vulnerable Tracts\",\n    caption = \"Vulnerable tracts: bottom 25% in median income & top 25% in population 65+, relative to the rest of PA\\nDashed line shows 15-mile threshold used to define underserved.\\nNote how there is a clustering of tracts with relatively close hospital access (&lt;15 miles) and a very long tail beyond the 15 mile cutoff.\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nSuggested chart types: - Histogram or density plot of distances - Box plot comparing distances across regions - Bar chart of underserved tracts by county - Scatter plot of distance vs. vulnerable population size\nRequirements: - Clear axes labels with units - Appropriate title - Professional formatting - Brief interpretation (1-2 sentences as a caption or in text)"
  },
  {
    "objectID": "assignments/assignment_2/Kakumanu_Sujan_Assignment2.html#part-3-bring-your-own-data-analysis",
    "href": "assignments/assignment_2/Kakumanu_Sujan_Assignment2.html#part-3-bring-your-own-data-analysis",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 3: Bring Your Own Data Analysis",
    "text": "Part 3: Bring Your Own Data Analysis\nChoose your own additional spatial dataset and conduct a supplementary analysis.\n\nChallenge\n\nInfrastructure & Services\n\nData: Polling Places, SEPTA stops, Census tracts (elderly population, disability rates)\nQuestion: “Are polling places accessible for elderly and disabled voters?”\nOperations: Buffer polling places and transit stops, identify vulnerable populations, find areas lacking access\nPolicy relevance: Voting rights, election infrastructure, ADA compliance\n\n\n\n\n\nData Sources\nOpenDataPhilly: https://opendataphilly.org/datasets/ - Most datasets available as GeoJSON, Shapefile, or CSV with coordinates - Always check the Metadata for a data dictionary of the fields.\nAdditional Sources: - Pennsylvania Open Data: https://data.pa.gov/ - Census Bureau (via tidycensus): Demographics, economic indicators, commute patterns - TIGER/Line (via tigris): Geographic boundaries\n\n\nYour Analysis\nYour Task:\n\nFind and load additional data\n\nDocument your data source\nCheck and standardize the CRS\nProvide basic summary statistics\n\n\n\npolling_places &lt;- st_read(\"./data/polling_places.geojson\", quiet = TRUE)\nst_crs(polling_places)$input\n\n[1] \"WGS 84\"\n\npolling_places &lt;- st_transform(polling_places, crs = pa_projection)\nst_crs(polling_places)$input\n\n[1] \"EPSG:3365\"\n\nphilly_tracts &lt;- tracts_by_county %&gt;%\n  filter(COUNTY_NAM == \"PHILADELPHIA\")\n\npolling_places %&gt;%\n  st_drop_geometry() %&gt;%\n  count(accessibility_code) %&gt;%\n  mutate(\n    percentage = round(n / sum(n) * 100, 1),\n    accessibility_type = case_when(\n      accessibility_code == \"A\" ~ \"Alternate Entrance\",\n      accessibility_code == \"B\" ~ \"Substantially Accessible\",\n      accessibility_code == \"F\" ~ \"Fully Accessible\",\n      accessibility_code == \"M\" ~ \"Accessibility Modified\",\n      accessibility_code == \"N\" ~ \"Not Accessible\",\n      accessibility_code == \"R\" ~ \"Accessible With Ramp\",\n      TRUE ~ \"Unknown\"\n    )\n  ) %&gt;%\n  arrange(desc(n)) %&gt;%\n  select(accessibility_code, accessibility_type, n, percentage) %&gt;%\n  bind_rows(\n    summarise(., \n              accessibility_code = \"TOTAL\",\n              accessibility_type = \"\",\n              n = sum(n),\n              percentage = sum(percentage))\n  ) %&gt;%\n  kable(\n    col.names = c(\"Code\", \"Accessibility Type\", \"Count\", \"%\"),\n    align = c(\"l\", \"l\", \"r\", \"r\"),\n    caption = \"Polling Locations in Philadelphia by Accesibility Type\"\n  )\n\n\nPolling Locations in Philadelphia by Accesibility Type\n\n\nCode\nAccessibility Type\nCount\n%\n\n\n\n\nM\nAccessibility Modified\n740\n43.5\n\n\nF\nFully Accessible\n674\n39.6\n\n\nA\nAlternate Entrance\n166\n9.7\n\n\nR\nAccessible With Ramp\n72\n4.2\n\n\nB\nSubstantially Accessible\n51\n3.0\n\n\nTOTAL\n\n1703\n100.0\n\n\n\n\n\n\nsepta_bus_stops &lt;- st_read(\"./data/2022_SEPTA_Bus_Stops.geojson\", quiet = TRUE)\n\nst_crs(septa_bus_stops)$input\n\n[1] \"WGS 84\"\n\nsepta_bus_stops &lt;- st_transform(septa_bus_stops, crs = pa_projection)\nst_crs(septa_bus_stops)$input\n\n[1] \"EPSG:3365\"\n\nphilly_bus_stops &lt;- septa_bus_stops %&gt;%\n  st_filter(philly_tracts)\n\nQuestions to answer:\n\nWhat dataset did you choose and why?\n\nI chose polling data because I found it interesting that they’re classified by fairly detailed accessibility codes. Above is a table of the polling locations in Philadelphia by accessibility type. I also chose data points describing SEPT bus stops. I want to see if there are any polling locations not near bus stops.\n\nWhat is the data source and date?\n\nPolling Places: December 8, 2014. Found here\nSEPTA Bus Stops: 2022. Found here\n\nHow many features does it contain?\n\n1703 Polling Places (in Philadelphia)\n18066 Bus Stops (in SEPTA’s service area, not necessarily just in Philadelphia)\n\nAfter filtering to bus stops only in the Philly census tracts, there are 10654 bus stops.\n\n\nWhat CRS is it in? Did you need to transform it?\n\nBoth were in WGS 84 and had to be transformed to EPSG:3365.\n\n\n\n\nPose a research question\n\nWhat Philadelphia polling locations are further than a 5, 10, and 15 min walk from a bus stop?\nNote: Using an average walk speed of ~2.5mph, this would be distances of 0.2, 0.4, and 0.6 miles respectively.\n\n\nConduct spatial analysis\n\nUse at least TWO spatial operations to answer your research question.\nRequired operations (choose 2+): - Buffers - Spatial joins - Spatial filtering with predicates - Distance calculations - Intersections or unions - Point-in-polygon aggregation\nYour Task:\n\n# Your spatial analysis\n\n# State plane is in feet, so I'm converting miles to feet to create the buffers\nbus_buffer_5min &lt;- st_buffer(philly_bus_stops, dist = 0.20 * 5280)\nbus_buffer_10min &lt;- st_buffer(philly_bus_stops, dist = 0.40 * 5280)\nbus_buffer_15min &lt;- st_buffer(philly_bus_stops, dist = 0.60 * 5280)\n\n# Doing this to consolidate buffers for easier analysis\nbus_zone_5min &lt;- st_union(bus_buffer_5min)\nbus_zone_10min &lt;- st_union(bus_buffer_10min)\nbus_zone_15min &lt;- st_union(bus_buffer_15min)\n\npolling_no_transit_5min &lt;- polling_places %&gt;%\n  st_difference(bus_zone_5min)\n\npolling_no_transit_10min &lt;- polling_places %&gt;%\n  st_difference(bus_zone_10min)\n\npolling_no_transit_15min &lt;- polling_places %&gt;%\n  st_difference(bus_zone_15min)\n\n# Here, I'm finding the distance to the nearest bus stop, and setting some boolean variables for my time categories.\npolling_with_distance &lt;- polling_places %&gt;%\n  mutate(\n    dist_to_bus = st_distance(., philly_bus_stops) %&gt;% \n      apply(1, min) %&gt;% \n      as.numeric() / 5280,\n    beyond_5min = dist_to_bus &gt; 0.20,\n    beyond_10min = dist_to_bus &gt; 0.40,\n    beyond_15min = dist_to_bus &gt; 0.60\n  )\n\npolling_with_distance %&gt;%\n  st_drop_geometry() %&gt;%\n  summarise(\n    total_polling = n(),\n    beyond_5min = sum(beyond_5min, na.rm = TRUE),\n    beyond_10min = sum(beyond_10min, na.rm = TRUE),\n    beyond_15min = sum(beyond_15min, na.rm = TRUE),\n    pct_beyond_5min = round((beyond_5min / total_polling) * 100, 1),\n    pct_beyond_10min = round((beyond_10min / total_polling) * 100, 1),\n    pct_beyond_15min = round((beyond_15min / total_polling) * 100, 1)\n  )\n\n  total_polling beyond_5min beyond_10min beyond_15min pct_beyond_5min\n1          1703         106            3            0             6.2\n  pct_beyond_10min pct_beyond_15min\n1              0.2                0\n\nmap_5min &lt;- ggplot() +\n  geom_sf(data = philly_tracts, fill = \"gray95\", color = \"white\", size = 0.2) +\n  geom_sf(data = polling_with_distance %&gt;% filter(beyond_5min), \n          color = \"gold\", size = 1.5, alpha = 0.7) +\n  geom_sf(data = polling_with_distance %&gt;% filter(!beyond_5min), \n          color = \"gray70\", size = 0.5, alpha = 0.3) +\n  theme_void() +\n  labs(title = \"&gt; 5 min walk\") +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\"))\n\n# Map 2: Beyond 10 minutes\nmap_10min &lt;- ggplot() +\n  geom_sf(data = philly_tracts, fill = \"gray95\", color = \"white\", size = 0.2) +\n  geom_sf(data = polling_with_distance %&gt;% filter(beyond_10min), \n          color = \"orangered\", size = 1.5, alpha = 0.7) +\n  geom_sf(data = polling_with_distance %&gt;% filter(!beyond_10min), \n          color = \"gray70\", size = 0.5, alpha = 0.3) +\n  theme_void() +\n  labs(title = \"&gt; 10 min walk\") +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\"))\n\n# Map 3: Beyond 15 minutes\nmap_15min &lt;- ggplot() +\n  geom_sf(data = philly_tracts, fill = \"gray95\", color = \"white\", size = 0.2) +\n  geom_sf(data = polling_with_distance %&gt;% filter(beyond_15min), \n          color = \"maroon\", size = 1.5, alpha = 0.7) +\n  geom_sf(data = polling_with_distance %&gt;% filter(!beyond_15min), \n          color = \"gray70\", size = 0.5, alpha = 0.3) +\n  theme_void() +\n  labs(title = \"&gt; 15 min walk\") +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\"))\n\n# Combine maps\nmap_5min + map_10min + map_15min +\n  plot_annotation(\n    title = \"Philadelphia Polling Places Beyond Certain Walking Distances from Bus Stops\",\n    subtitle = \"Colored dots show polling locations beyond specified walking time to nearest bus stop\",\n    theme = theme(plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n                  plot.subtitle = element_text(hjust = 0.5, size = 10))\n  )\n\n\n\n\n\n\n\n\nAnalysis requirements: - Clear code comments explaining each step - Appropriate CRS transformations - Summary statistics or counts - At least one map showing your findings - Brief interpretation of results (3-5 sentences)\nYour interpretation:\nIt seems that there’s more interesting analysis that can occur for polling stations more than 5 minutes walk from a bus stop. They seem mostly concentrated along the periphery of the city. Future analysis can be done on the condition of streets around these polling stations (a 5 minute walk across a quaint neighborhood is much easier than a 5 minute walk across/adjacent a busy highway). This can be combined with the accessibility information of polling stations, whether they’re in vulnerable census tracts, or near other forms of transit (SEPTA Metro, Trolleys, IndeGo bikeshare, bike lanes, etc.)"
  },
  {
    "objectID": "assignments/assignment_2/Kakumanu_Sujan_Assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "href": "assignments/assignment_2/Kakumanu_Sujan_Assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Finally - A few comments about your incorporation of feedback!",
    "text": "Finally - A few comments about your incorporation of feedback!\nI made sure to hide my API key this time, using R Studio’s environment variables. I cleaned up more setup and instruction comments."
  },
  {
    "objectID": "assignments/assignment_2/Kakumanu_Sujan_Assignment2.html#submission-requirements",
    "href": "assignments/assignment_2/Kakumanu_Sujan_Assignment2.html#submission-requirements",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Submission Requirements",
    "text": "Submission Requirements\nWhat to submit:\n\nRendered HTML document posted to your course portfolio with all code, outputs, maps, and text\n\nUse embed-resources: true in YAML so it’s a single file\nAll code should run without errors\nAll maps and charts should display correctly\n\n\nFile naming: LastName_FirstName_Assignment2.html and LastName_FirstName_Assignment2.qmd"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#coding-techniquestechnical-notes",
    "href": "weekly-notes/week-06-notes.html#coding-techniquestechnical-notes",
    "title": "Week 6 Notes",
    "section": "Coding Techniques/Technical Notes",
    "text": "Coding Techniques/Technical Notes\n\nst_as_sf\n\nConverting CSV data to Spatial Data (sf). The CSV may have a lat and lon, but needs a geometry.\nKey: You provide what crs the CSV is in! Having a lat lon column probably means it’s WGS84 or NAD83.\n\nNote, WGS84 and NAD83 are similar. With our spatial analysis, being off a couple feet is probably fine so you can provide an educated guess as to what the crs is. This may not be OK in other fields.\n\n\nas.factor(name)\n\nIf you have some spatial data (neighborhoods for example), you can create variables using this\n\nname = as.factor(name). It creates binary indicators for all neighborhoods.\n\nlevels(data.sf$name) -&gt; can check what the reference var is.\n\nThen to create the model Ex: model_neighborhoods &lt;- lm(SalePrice ~ LivingArea + name, data = boston.sf)\nI(var^2) when defining a model to add a polynomial coeff (in this case, a squared term)"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#questions-challenges",
    "href": "weekly-notes/week-06-notes.html#questions-challenges",
    "title": "Week 6 Notes",
    "section": "## Questions & Challenges",
    "text": "## Questions & Challenges"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#connections-to-policy",
    "href": "weekly-notes/week-06-notes.html#connections-to-policy",
    "title": "Week 6 Notes",
    "section": "## Connections to Policy",
    "text": "## Connections to Policy"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#reflection",
    "href": "weekly-notes/week-06-notes.html#reflection",
    "title": "Week 6 Notes",
    "section": "## Reflection",
    "text": "## Reflection"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html",
    "href": "weekly-notes/week-06-notes.html",
    "title": "Week 6 Notes",
    "section": "",
    "text": "Motivation for Spatial ML and Advanced Regression\n\nRecall a simple regression model for house value. A lot of the variation in house price is unexplained by just a few variables. Imagine a small house in a desirable neighborhood, a house with an extra bathroom, backyard, etc.\n\nThis is where categorical variables can come in handy (as opposed to continuous variables like sqft, med income, etc.) Neighborhoods can be a good spatial categorical variable. See st_as_sf in the technical notes below.\n\nWhen creating categorical vars, one will become a reference variables. Model interpretation is done wrt the reference! Ex, if the neighborhood Allston is picked a the reference variables for neighborhood, model interpretation would be “This neighborhood is MORE/LESS/SAME expensive than Allston”. See levels() fn below.\n\nInteraction\n\nNo Interaction (var + var) suggests only the intercept of the line changing, not the slope (parallel lines). Imagine living area having the same impact on home value across neighborhoods.\nInteraction (var * var) suggests different slope and intercept.\nInterpret the interaction coeffs. Compare \\(R^2\\) between the models.\nDon’t use interaction if…\n\nsmall samples (need sufficient data for each group); overfitting (too many interactions make model unstable)\n\n\nPolynomial Coefficients\n\nImagine a theory proposing that house value wrt age is actually a u-shaped relationship.\nYou can add Age as a squared term instead of a linear term and compare the impact of this transformation.\n\n3 Approaches to Spatial Features: Recall 1st Law of Geography…\n\nBuffer Aggregation (count or sum events within defined distance e.g 500ft)\nk-Nearest Neighbors (Ex: avg dist to closest 3 events)\nDistance to Specific Points (Ex: distance to downtown, nearest transit station, etc.)\n\nFixed Effect\n\nRecall the neighborhood categorical variable. That neighborhood captures so much information (school quality, park access, transportation, etc). We call that a fixed effect (this ex, a “neighborhood fixed effect”)\n\nCross Validation with Categorical Vars\n\nToday, we focused on just k-fold CV with categorical\nYou have to make sure that there’s enough data in each category!!! Imagine a neighborhood with 1 house. n &lt; 10 will likely cause CV problems…\nShould add vars and compare the model performance between each. Note that RMSE makes errors really large, so try to understand what could cause this. Ex: adding variables may improve specificity, but errors may still seem large. Could be because of small categories, outliers, etc."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#key-concepts-learned-notes",
    "href": "weekly-notes/week-06-notes.html#key-concepts-learned-notes",
    "title": "Week 6 Notes",
    "section": "",
    "text": "Motivation for Spatial ML and Advanced Regression\n\nRecall a simple regression model for house value. A lot of the variation in house price is unexplained by just a few variables. Imagine a small house in a desirable neighborhood, a house with an extra bathroom, backyard, etc.\n\nThis is where categorical variables can come in handy (as opposed to continuous variables like sqft, med income, etc.) Neighborhoods can be a good spatial categorical variable. See st_as_sf in the technical notes below.\n\nWhen creating categorical vars, one will become a reference variables. Model interpretation is done wrt the reference! Ex, if the neighborhood Allston is picked a the reference variables for neighborhood, model interpretation would be “This neighborhood is MORE/LESS/SAME expensive than Allston”. See levels() fn below.\n\nInteraction\n\nNo Interaction (var + var) suggests only the intercept of the line changing, not the slope (parallel lines). Imagine living area having the same impact on home value across neighborhoods.\nInteraction (var * var) suggests different slope and intercept.\nInterpret the interaction coeffs. Compare \\(R^2\\) between the models.\nDon’t use interaction if…\n\nsmall samples (need sufficient data for each group); overfitting (too many interactions make model unstable)\n\n\nPolynomial Coefficients\n\nImagine a theory proposing that house value wrt age is actually a u-shaped relationship.\nYou can add Age as a squared term instead of a linear term and compare the impact of this transformation.\n\n3 Approaches to Spatial Features: Recall 1st Law of Geography…\n\nBuffer Aggregation (count or sum events within defined distance e.g 500ft)\nk-Nearest Neighbors (Ex: avg dist to closest 3 events)\nDistance to Specific Points (Ex: distance to downtown, nearest transit station, etc.)\n\nFixed Effect\n\nRecall the neighborhood categorical variable. That neighborhood captures so much information (school quality, park access, transportation, etc). We call that a fixed effect (this ex, a “neighborhood fixed effect”)\n\nCross Validation with Categorical Vars\n\nToday, we focused on just k-fold CV with categorical\nYou have to make sure that there’s enough data in each category!!! Imagine a neighborhood with 1 house. n &lt; 10 will likely cause CV problems…\nShould add vars and compare the model performance between each. Note that RMSE makes errors really large, so try to understand what could cause this. Ex: adding variables may improve specificity, but errors may still seem large. Could be because of small categories, outliers, etc."
  },
  {
    "objectID": "weekly-notes/week-07-notes.html",
    "href": "weekly-notes/week-07-notes.html",
    "title": "Week 7 Notes",
    "section": "",
    "text": "Clustered Errors (Spatial Autocorrelation of Residuals)\n\nRecall the Tobler’s 1st Law of Geography. This autocorrelation can occur for errors when evaluating models as well. A model generalized to a city may have + residuals in certain areas, and - residuals in others. If the model is good, we should see random scattering, not clustering of errors in space.\n\nMoran’s I\n\nMeasuring spatial autocorrelation. Ranging between -1 (Perfect negative correlation - dispersion), 0 (Random spatial pattern), and 1 (Perfect positive correlation - clustering).\nAt a high level, the formula calculates the difference of an observation from the mean, multiplied by the observation’s neighbor from the mean. Summation of all of these pairs.\n\nDefining Neighbor\n\nContiguity (polygons that share a border; queen, rook), Distance (all w/in X distance; fixed distance), k-Nearest (closest k-points; adaptive distance)\n\nSpatial Lag - average value of neighbors\nIn spatial stats, we learned about Spatial Lag/Spatial Error models. This is not going to be used in this class (focused on predictive models)\n\nWhy? Simultaneity Problem (we need new house prices, but don’t have them in a predictive model). OLS estimates are biased and inconsistent.\n\nBiased (expected value != true parameter). Inconsitent (doesn’t converge to true value as n -&gt; inf)"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#key-concepts-learned-notes",
    "href": "weekly-notes/week-07-notes.html#key-concepts-learned-notes",
    "title": "Week 7 Notes",
    "section": "",
    "text": "Clustered Errors (Spatial Autocorrelation of Residuals)\n\nRecall the Tobler’s 1st Law of Geography. This autocorrelation can occur for errors when evaluating models as well. A model generalized to a city may have + residuals in certain areas, and - residuals in others. If the model is good, we should see random scattering, not clustering of errors in space.\n\nMoran’s I\n\nMeasuring spatial autocorrelation. Ranging between -1 (Perfect negative correlation - dispersion), 0 (Random spatial pattern), and 1 (Perfect positive correlation - clustering).\nAt a high level, the formula calculates the difference of an observation from the mean, multiplied by the observation’s neighbor from the mean. Summation of all of these pairs.\n\nDefining Neighbor\n\nContiguity (polygons that share a border; queen, rook), Distance (all w/in X distance; fixed distance), k-Nearest (closest k-points; adaptive distance)\n\nSpatial Lag - average value of neighbors\nIn spatial stats, we learned about Spatial Lag/Spatial Error models. This is not going to be used in this class (focused on predictive models)\n\nWhy? Simultaneity Problem (we need new house prices, but don’t have them in a predictive model). OLS estimates are biased and inconsistent.\n\nBiased (expected value != true parameter). Inconsitent (doesn’t converge to true value as n -&gt; inf)"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#coding-techniquestechnical-notes",
    "href": "weekly-notes/week-07-notes.html#coding-techniquestechnical-notes",
    "title": "Week 7 Notes",
    "section": "Coding Techniques/Technical Notes",
    "text": "Coding Techniques/Technical Notes\n\nlibrary(spdep)\n\nCreating the spatial lag.\n\nknn2b (to create neearest neighbors)\nnb2listw (provide the nearest neighbors to create a weights matrix - observation x observation with binary values)\nmoran.mc"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#questions-challenges",
    "href": "weekly-notes/week-07-notes.html#questions-challenges",
    "title": "Week 7 Notes",
    "section": "## Questions & Challenges",
    "text": "## Questions & Challenges"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#connections-to-policy",
    "href": "weekly-notes/week-07-notes.html#connections-to-policy",
    "title": "Week 7 Notes",
    "section": "## Connections to Policy",
    "text": "## Connections to Policy"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#reflection",
    "href": "weekly-notes/week-07-notes.html#reflection",
    "title": "Week 7 Notes",
    "section": "## Reflection",
    "text": "## Reflection"
  },
  {
    "objectID": "labs/lab-06-model-competition/lab6.html",
    "href": "labs/lab-06-model-competition/lab6.html",
    "title": "Lab 6 - Model Building Competition",
    "section": "",
    "text": "library(caret)"
  },
  {
    "objectID": "assignments/assignment_3/MUSA-5080-Midterm/model_script.html",
    "href": "assignments/assignment_3/MUSA-5080-Midterm/model_script.html",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "",
    "text": "This technical appendix documents the full workflow used to engineer and visualize spatial features for predicting residential housing prices in Philadelphia.\nCode\noptions(scipen = 999)\n\n# Packages\nif(!require(pacman)){install.packages(\"pacman\"); library(pacman, quietly = T)}\np_load(knitr, sf, tidyverse, tidycensus, tigris, here, dplyr, FNN, ggplot2, scales, patchwork, caret, Hmisc, stargazer)\n\n# Files\nsf_data &lt;- st_read(\"./data/OPA_data.geojson\", quiet = TRUE)\nnhoods &lt;- st_read(\"./data/philadelphia-neighborhoods.geojson\", quiet = TRUE)"
  },
  {
    "objectID": "assignments/assignment_3/MUSA-5080-Midterm/model_script.html#data-preparation",
    "href": "assignments/assignment_3/MUSA-5080-Midterm/model_script.html#data-preparation",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "1.1 Data Preparation",
    "text": "1.1 Data Preparation\nWe apply several filters to the property data to for quality and relevance. First, we restrict our analysis to residential properties sold between 2023 and 2024, excluding any other property categories. Second, we remove properties with sale prices below $10, as these are abnormal prices for residential properties.\nTo work with Github file size limits, the data is further trimmed of irrelevant columns.\n\n\nCode\n# Restrict to residential only\nresidential_categories &lt;- c(\n  \"APARTMENTS &gt; 4 UNITS\",\n  \"MULTI FAMILY\",\n  \"SINGLE FAMILY\",\n  \"MIXED USE\"\n)\nresidential_data &lt;- sf_data %&gt;%\n  filter(category_code_description %in% residential_categories,\n         year(sale_date) %in% c(2023, 2024),\n         mailing_city_state == \"PHILADELPHIA PA\",\n         sale_price &gt; 10\n         )\n\ntable(residential_data$category_code_description)\n\n# Making sure the file saved to the repo is the trimmed data (to stay below GitHub data limits)\nst_write(residential_data, \"./data/OPA_data.geojson\", driver = \"GeoJSON\", delete_dsn = TRUE, quiet = TRUE)\nfile.exists(\"./data/OPA_data.geojson\")\nOPA_raw &lt;- st_read(\"./data/OPA_data.geojson\", quiet = TRUE) %&gt;% \n  st_transform(2272)\n\n# OPA_data -&gt; cutting mostly NA columns or irrelevant columns for this model.\nOPA_raw &lt;- OPA_raw %&gt;%\n  select(-c(\n  cross_reference, date_exterior_condition, exempt_land, fireplaces, fuel, garage_type, house_extension, mailing_address_2, mailing_care_of, market_value_date, number_of_rooms, other_building, owner_2, separate_utilities, sewer, site_type, street_direction, suffix, unfinished, unit, utility\n  ))\n\nnames(OPA_raw)\n\n\nThe property sales data was gathered from the OPA properties public data set from the City of Philadelphia. This data set was 32 columns and 583,825 observations. This file was too large for our shared GitHub work space so it was reduced by filtering for residential properties, years 2023 and 2024, location within Philadelphia, and sale price over 10 since some were NA, 1, or 10. This was just enough to get the most basic and general data to work with that ran with GitHub size limits. This reduced the size to 22121 observations. The original geojson file was overwritten and named OPA_data.\nProperties selected for residential included apartments &gt;4 units, single family, multi-family, and mixed use. Mixed use was left in as there are still residential unit to account yet add more complex property types to our total data set when comparing sale price and other aspects such as total area to other observations. These properties should also be cross referenced with zoning codes for future research.\nWe left mixed use in during this process to give us the most general data set representation. There was also limited data cleaning other than omitting columns that were mostly NA. This gave our model the most general data set to work with despite lower future RMSE values. Future research would be needed to most accurately assess the choices of losing data and a more generalized Philadelphia housing market verses very clean data and more specific Philadelphia housing market that may omit certain aspects of the housing market like data in lower income areas or multi use residential aspects. This could have also been conducted in grouping NA values and sparse categories. More complexity could be accounted for in future work.\nThis was our start simple and add complexity approach. Our original to final OPA data set went from 583,825 to 22,121 observations and from 32 to 68 variables."
  },
  {
    "objectID": "assignments/assignment_3/MUSA-5080-Midterm/model_script.html#exploratory-data-analysis",
    "href": "assignments/assignment_3/MUSA-5080-Midterm/model_script.html#exploratory-data-analysis",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "1.2 Exploratory Data Analysis",
    "text": "1.2 Exploratory Data Analysis\nBelow are selected property variables—Total Livable Area, Bedrooms, Bathrooms, and Age—in relation to Sale Price. Properties with excessive square footage (&gt;10,000 sqft), missing bedroom or bathroom data, over 12 bathrooms with low sale prices, or implausible construction years were removed to reduce skew and data errors. This additional filtering was kept for the rest of the analysis in this report.\n\n\nCode\n# filter out outliers from the dataset\nOPA_data &lt;- OPA_raw %&gt;%\n  filter(\n    total_livable_area &lt;= 10000,\n    year_built &gt; 1800,\n    !is.na(number_of_bathrooms),\n    !is.na(number_of_bedrooms),\n    number_of_bathrooms &lt; 12,\n  ) %&gt;%\n  mutate(\n    year_built = as.numeric(year_built),\n    building_age = 2025 - year_built\n  )\n\np1 &lt;- ggplot(OPA_data, aes(x = total_livable_area, y = sale_price)) +\n  geom_point(alpha = 0.3, size = 0.8) +\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE) +\n  scale_y_continuous(labels = dollar_format()) +\n  scale_x_continuous(labels = comma_format()) +\n  labs(\n    title = \"Sale Price vs. Total Livable Area\",\n    x = \"Total Livable Area (sq ft)\",\n    y = \"Sale Price\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 10, face = \"bold\"))\n\np2 &lt;- ggplot(OPA_data, aes(x = factor(number_of_bedrooms), y = sale_price)) +\n  geom_boxplot(fill = \"gray\", alpha = 0.6, outlier.alpha = 0.3, outlier.size = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", color = \"red\", size = 2, shape = 18) +\n  scale_y_continuous(labels = dollar_format()) +\n  labs(\n    title = \"Sale Price vs. Number of Bedrooms\",\n    x = \"Number of Bedrooms\",\n    y = \"Sale Price\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 10, face = \"bold\"))\n\np3 &lt;- ggplot(OPA_data, aes(x = factor(number_of_bathrooms), y = sale_price)) +\n  geom_boxplot(fill = \"gray\", alpha = 0.6, outlier.alpha = 0.3, outlier.size = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", color = \"red\", size = 2, shape = 18) +\n  scale_y_continuous(labels = dollar_format()) +\n  labs(\n    title = \"Sale Price vs. Number of Bathrooms\",\n    x = \"Number of Bathrooms\",\n    y = \"Sale Price\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 10, face = \"bold\"))\n\np4 &lt;- ggplot(OPA_data, aes(x = building_age, y = sale_price)) +\n  geom_point(alpha = 0.3, size = 0.8) +\n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE) +\n  scale_y_continuous(labels = dollar_format()) +\n  labs(\n    title = \"Sale Price vs. Age\",\n    x = \"Age\",\n    y = \"Sale Price\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 10, face = \"bold\"))\n\n# Combine plots\n(p1 | p2) / (p3 | p4)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "assignments/assignment_3/MUSA-5080-Midterm/model_script.html#feature-engineering",
    "href": "assignments/assignment_3/MUSA-5080-Midterm/model_script.html#feature-engineering",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "1.3 Feature Engineering",
    "text": "1.3 Feature Engineering\n\n\nCode\nOPA_data &lt;- OPA_data %&gt;%\n  mutate(\n    # convert to numeric before interactions\n    total_livable_area = as.numeric(total_livable_area),\n    census_tract = as.numeric(as.character(census_tract)),\n    year_built = as.numeric(year_built),\n    total_area = as.numeric(total_area),\n    market_value = as.numeric(market_value),\n    number_of_bedrooms = as.numeric(number_of_bedrooms),\n\n    # building code and total area\n    int_type_tarea = as.numeric(as.factor(building_code_description)) * total_area,\n\n    # market and livable area\n    int_value_larea = market_value * total_livable_area,\n\n    # market and total area\n    int_value_tarea = market_value * total_area,\n\n    # livable area and exterior condition\n    int_larea_econd = total_livable_area * as.numeric(as.factor(exterior_condition)),\n\n    # livable area and interior condition\n    int_larea_icond = total_livable_area * as.numeric(as.factor(interior_condition)),\n\n    # livable area and bedrooms\n    int_larea_beds = total_livable_area * number_of_bedrooms\n  )\n\n\n\n\nCode\npa_crs &lt;- 2272  \nneighbor_points &lt;- st_transform(OPA_data, pa_crs)\n\nnrow(nhoods)\n\nst_crs(neighbor_points)\nnhoods &lt;- st_transform(nhoods, 2272)\n\n#joining houses to neighborhoods\nneighbor_points &lt;- neighbor_points %&gt;%\n  st_join(., nhoods, join = st_intersects)\n\n# one property doesn't lie in any neighborhood\nneighbor_points &lt;- neighbor_points[!is.na(neighbor_points$NAME),]\n\n#results \nneighbor_points %&gt;%\n  st_drop_geometry() %&gt;%\n  count(NAME) %&gt;%\n  arrange(desc(n))\n\n\n\n\nCode\n#spatial joins\nprice_by_nhood &lt;- neighbor_points %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(NAME) %&gt;%\n  dplyr::summarize(\n    median_price = median(sale_price, na.rm = TRUE),\n    n_sales = n()\n  )\n\nnhoods_prices &lt;- nhoods %&gt;%\n  left_join(., price_by_nhood, by = \"NAME\")\n\n#setting median house price classes\nnhoods_prices &lt;- nhoods_prices %&gt;%\n  mutate(\n    price_class = cut(median_price,\n                      breaks = c(0, 400000, 600000, 800000, 1000000, Inf),\n                      labels = c(\"Under $400k\", \"$400k-$600k\", \"$600k-$800k\", \n                                 \"$800k-$1M\", \"Over $1M\"),\n                      include.lowest = TRUE)\n  )\n\n#mapping\nggplot() +\n  geom_sf(data = nhoods_prices, aes(fill = price_class), \n          color = \"white\", size = 0.5) +\n  scale_fill_brewer(\n    name = \"Median Price\",\n    palette = \"YlOrRd\",\n    na.value = \"grey90\",\n    direction = 1\n  ) +\n  labs(\n    title = \"Median Home Prices by Philadelphia Neighborhood\",\n  ) +\n  theme_void() +\n  theme(\n    legend.position = \"right\",\n    plot.title = element_text(face = \"bold\", size = 14),\n    legend.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nprice_by_nhood %&gt;%\n  arrange(desc(median_price)) %&gt;%\n  head(10)\n\nprice_by_nhood %&gt;%\n  arrange(desc(median_price)) %&gt;%\n  print(n = 50)\n\nprice_by_nhood %&gt;%\n  arrange(desc(median_price)) %&gt;%\n  print(n = 50)\n\nprice_by_nhood %&gt;%\n  arrange(desc(n_sales)) %&gt;%\n  head(5)\n\n\n\n\nCode\n# Define wealthy as &gt;=$420,000 which is 1.5x city median of 279,900\nnhoods_prices &lt;- nhoods_prices %&gt;%\n  mutate(\n    wealthy_neighborhood = ifelse(median_price &gt;= 420000, \"Wealthy\", \"Not Wealthy\"),\n    wealthy_neighborhood = as.factor(wealthy_neighborhood)\n  )\n\nnhoods_prices %&gt;%\n  st_drop_geometry() %&gt;%\n  count(wealthy_neighborhood)\n\n\n  wealthy_neighborhood   n\n1          Not Wealthy 123\n2              Wealthy  26\n3                 &lt;NA&gt;  10\n\n\nCode\nneighbor_points &lt;- neighbor_points %&gt;%\n  left_join(.,\n            nhoods_prices %&gt;%\n              st_drop_geometry %&gt;%\n              select(NAME, wealthy_neighborhood),\n            by = \"NAME\")\n\n# Still add neighbor points to OPA data\n\n\nHouseholds were denoted as wealthy if their median household price was over $420,000, which is 1.5x city median of 279,900. This term will be used in an interaction in Model 4 to account for theoretical differences in wealthy neighborhoods, such as inflated costs for additional home amenities such as bedrooms, bathrooms, or livable floor area."
  },
  {
    "objectID": "assignments/assignment_3/MUSA-5080-Midterm/model_script.html#data-preparation-1",
    "href": "assignments/assignment_3/MUSA-5080-Midterm/model_script.html#data-preparation-1",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "2.1 Data Preparation",
    "text": "2.1 Data Preparation\n\n\nCode\n# link variables and aliases\nvars &lt;- c(\"pop_tot\" = \"B01001_001\",\n          \"med_hh_inc\" = \"B19013_001\",\n          \"med_age\" = \"B01002_001\")\n\n# the FIPS code for the state of PA is 42\nfips_pa &lt;- 42\n\n\nVariables pulled from the census include total population, median household income, and median age.\n\n\nCode\n# retrieve data from the ACS for 2023\ndemo_vars_pa &lt;- get_acs(geography = \"tract\",\n                        variable = vars,\n                        year = 2023,\n                        state = fips_pa,\n                        output = \"wide\",\n                        geometry = T,\n                        progress_bar = F) %&gt;% \n  st_transform(2272)\n\n# separate NAME column into its constituent parts\ndemo_vars_pa &lt;- demo_vars_pa %&gt;%\n  separate(NAME,\n           into = c(\"TRACT\", \"COUNTY\", \"STATE\"),\n           sep = \"; \",\n           remove = T) %&gt;% \n  mutate(TRACT = parse_number(TRACT),\n         COUNTY = sub(x = COUNTY, \" County\", \"\"))\n\n# filter out Philadelphia tracts\ndemo_vars_phl &lt;- demo_vars_pa %&gt;%\n  filter(COUNTY == \"Philadelphia\")\n\n\n\n\nCode\n# plot cenusus variables to compare\nplot(demo_vars_phl[,\"pop_totE\"],\n     main = \"Total Population\",\n     breaks = seq(0, 10500, 500),\n     nbreaks = 21)\n\n\n\n\n\n\n\n\n\nCode\nplot(demo_vars_phl[,\"med_hh_incE\"],\n     main = \"Median Household Income\",\n     breaks = seq(0, 200000, 10000),\n     nbreaks = 20)\n\n\n\n\n\n\n\n\n\nCode\nplot(demo_vars_phl[,\"med_ageE\"],\n     main = \"Median Age\",\n     breaks = seq(0, 75, 5),\n     nbreaks = 15)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# get NA counts per column\nna_counts &lt;- sapply(demo_vars_phl, function(x) {sum(is.na(x))})\nkable(t(as.data.frame(na_counts)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGEOID\nTRACT\nCOUNTY\nSTATE\npop_totE\npop_totM\nmed_hh_incE\nmed_hh_incM\nmed_ageE\nmed_ageM\ngeometry\n\n\n\n\nna_counts\n0\n0\n0\n0\n0\n0\n27\n27\n17\n17\n0\n\n\n\n\n\nCode\n# filter out all rows that have at least one column with an na value\nna_index &lt;- !complete.cases(demo_vars_phl %&gt;% st_drop_geometry())\ndemo_vars_phl_na &lt;- demo_vars_phl[na_index,]\nkable(head(demo_vars_phl_na, 5) %&gt;% select(-ends_with(\"M\")) %&gt;% st_drop_geometry(),\n      col.names = c(\"GeoID\", \"Tract\", \"County\", \"State\", \"Population\", \"Median HH Inc ($)\", \"Median Age (yrs)\"),\n      row.names = F)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeoID\nTract\nCounty\nState\nPopulation\nMedian HH Inc ($)\nMedian Age (yrs)\n\n\n\n\n42101016500\n165.00\nPhiladelphia\nPennsylvania\n2165\nNA\n44.1\n\n\n42101980902\n9809.02\nPhiladelphia\nPennsylvania\n0\nNA\nNA\n\n\n42101980800\n9808.00\nPhiladelphia\nPennsylvania\n0\nNA\nNA\n\n\n42101028500\n285.00\nPhiladelphia\nPennsylvania\n2625\nNA\n36.1\n\n\n42101036901\n369.01\nPhiladelphia\nPennsylvania\n49\nNA\n42.1\n\n\n\n\n\n27 and 17 census tracts have a value of NA for median household income and median age, respectively. For the 17 census tracts where there is no reported population, the median household income and median age will be set to 0. The remaining 10 census tracts that have population but no reported median household income will be omitted from the dataset.\n\n\nCode\n# create a dataset with NAs replaced with zero where applicable\ndemo_vars_phl_rep &lt;- demo_vars_phl %&gt;% \n  mutate(med_hh_incE = case_when(pop_totE == 0 & is.na(med_hh_incE) ~ 0,\n                                 .default = med_hh_incE),\n         med_ageE = case_when(pop_totE == 0 & is.na(med_ageE) ~ 0,\n                                 .default = med_ageE))\n\n# final cleaned dataset without the 10 census tracts that have population values but have NA values for Median Household Income\ndemo_vars_phl_clean &lt;- demo_vars_phl_rep[complete.cases(demo_vars_phl_rep %&gt;%\n                                                          select(-ends_with(\"M\")) %&gt;%\n                                                          st_drop_geometry()),]\n\n# table with the omitted census tracts\ndemo_vars_phl_omit &lt;- demo_vars_phl_rep[!complete.cases(demo_vars_phl_rep %&gt;% select(-ends_with(\"M\")) %&gt;% st_drop_geometry()),]\nkable(demo_vars_phl_omit %&gt;% select(-ends_with(\"M\")) %&gt;% st_drop_geometry(),\n      col.names = c(\"GeoID\", \"Tract\", \"County\", \"State\", \"Population\", \"Median HH Inc ($)\", \"Median Age (yrs)\"),\n      row.names = F, caption = \"Census Tracts Omitted from Analysis due to Data Unavailability\")\n\n\n\nCensus Tracts Omitted from Analysis due to Data Unavailability\n\n\n\n\n\n\n\n\n\n\n\nGeoID\nTract\nCounty\nState\nPopulation\nMedian HH Inc ($)\nMedian Age (yrs)\n\n\n\n\n42101016500\n165.00\nPhiladelphia\nPennsylvania\n2165\nNA\n44.1\n\n\n42101028500\n285.00\nPhiladelphia\nPennsylvania\n2625\nNA\n36.1\n\n\n42101036901\n369.01\nPhiladelphia\nPennsylvania\n49\nNA\n42.1\n\n\n42101014800\n148.00\nPhiladelphia\nPennsylvania\n892\nNA\n40.9\n\n\n42101027700\n277.00\nPhiladelphia\nPennsylvania\n5489\nNA\n36.9\n\n\n42101030100\n301.00\nPhiladelphia\nPennsylvania\n6446\nNA\n37.2\n\n\n42101989100\n9891.00\nPhiladelphia\nPennsylvania\n1240\nNA\n29.7\n\n\n42101989300\n9893.00\nPhiladelphia\nPennsylvania\n160\nNA\n30.5\n\n\n42101020500\n205.00\nPhiladelphia\nPennsylvania\n3383\nNA\n33.3\n\n\n42101980200\n9802.00\nPhiladelphia\nPennsylvania\n396\nNA\n74.9\n\n\n\n\n\n\n\nCode\n# join census variables to the OPA data\nOPA_data &lt;- st_join(OPA_data, demo_vars_phl_clean %&gt;% select(pop_totE, med_hh_incE, med_ageE))\n\n# isolate NA rows and plot where they are\ncensusNAs &lt;- OPA_data[is.na(OPA_data$med_hh_incE),]\n\ncensus_plt1 &lt;- ggplot() +\n  geom_sf(data = demo_vars_phl_clean$geometry) +\n  geom_sf(data = censusNAs, size = 0.15) +\n  theme_void() +\n  labs(title = \"Properties without Census Data\")\ncensus_plt2 &lt;- ggplot() +\n  geom_sf(data = demo_vars_phl_clean$geometry, fill = \"black\", color = \"transparent\") +\n  theme_void() +\n  labs(title = \"Census Tracts with Data (Black)\")\n\n(census_plt1 | census_plt2)\n\n\n\n\n\n\n\n\n\nOf the 22121 properties in the dataset after cleaning and omitting outliers, 248 - approximately 1.1% of the dataset - have no associated census data due to the lack of a Median Household Income value for those census tracts. Comparing plots of property locations without census data and that of census tracts which have data confirms this spatial relationship."
  },
  {
    "objectID": "assignments/assignment_3/MUSA-5080-Midterm/model_script.html#data-preparation-2",
    "href": "assignments/assignment_3/MUSA-5080-Midterm/model_script.html#data-preparation-2",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "3.1 Data Preparation",
    "text": "3.1 Data Preparation\nThis stage prepares and validates the OpenDataPhilly spatial datasets used to engineer neighborhood-level variables for the housing model.\nSteps Performed\n\nTransformed all spatial datasets to EPSG 2272 (NAD83 / PA South ftUS) for consistent distance measurements.\nRemoved invalid geometries, dropped Z/M values, and converted all housing geometries to points.\nImported and projected OpenDataPhilly amenities:\n\nTransit Stops\nHospitals\nParks & Recreation Sites\nSchools Parcels (centroids created from polygon features)\nCrime Incidents (2023 and 2024 combined)\n\n\n\n\nCode\n#CRS & radii\npa_crs &lt;- 2272    # NAD83 / PA South (ftUS)\nmi_to_ft   &lt;- 5280\nr_park_ft   &lt;- 0.25 * mi_to_ft\nr_crime_ft  &lt;- 0.50 * mi_to_ft\nr_school_ft &lt;- 0.50 * mi_to_ft\n\n# turn off spherical geometry (makes buffer/join ops faster)\nsf::sf_use_s2(FALSE)\n\n## CONVERT SALES DATA TO POINTS ##\nOPA_points &lt;- st_transform(OPA_data, pa_crs)\n\n#Drop Z/M if present\nst_geometry(OPA_points) &lt;- st_zm(st_geometry(OPA_points), drop = TRUE, what = \"ZM\")\n\n#Make geometries valid\nst_geometry(OPA_points) &lt;- st_make_valid(st_geometry(OPA_points))\n\n#Ensure POINT geometry (works for points/lines/polygons/collections)\nst_geometry(OPA_points) &lt;- st_point_on_surface(st_geometry(OPA_points))\n\n#Add sale ID\nOPA_points &lt;- OPA_points %&gt;%\n  mutate(sale_id = dplyr::row_number())\n\n#read & project layers\ntransit   &lt;- st_read(\"./data/Transit_Stops_(Spring_2025)/Transit_Stops_(Spring_2025).shp\", quiet = TRUE) |&gt; st_transform(pa_crs)\nhospitals &lt;- st_read(\"./data/Hospitals.geojson\", quiet = TRUE) |&gt; st_transform(pa_crs)\nparksrec  &lt;- st_read(\"./data/PPR_Program_Sites.geojson\", quiet = TRUE)|&gt; st_transform(pa_crs)\nschools_polygons   &lt;- st_read(\"./data/Schools_Parcels.geojson\", quiet = TRUE) |&gt; st_transform(pa_crs)\ncrime_2023     &lt;- st_read(\"./data/crime_incidents_2023/incidents_part1_part2.shp\", quiet = TRUE)        |&gt; st_transform(pa_crs)\ncrime_2024     &lt;- st_read(\"./data/crime_incidents_2024/incidents_part1_part2.shp\", quiet = TRUE)        |&gt; st_transform(pa_crs)\n\n#combine 2023 & 2024 crime datasets\ncrime &lt;- rbind(crime_2023, crime_2024)\n\n#create centroids for schools dataset\nschools &lt;- if (any(st_geometry_type(schools_polygons) %in% c(\"POLYGON\",\"MULTIPOLYGON\"))) {\n  st_centroid(schools_polygons, )\n} else {\n  schools_polygons\n}\n\n#crop transit data to philadelphia\nphilly_boundary &lt;- st_union(nhoods)\n\ntransit &lt;- st_filter(transit, philly_boundary, .predicate = st_within)"
  },
  {
    "objectID": "assignments/assignment_3/MUSA-5080-Midterm/model_script.html#exploratory-data-analysis-1",
    "href": "assignments/assignment_3/MUSA-5080-Midterm/model_script.html#exploratory-data-analysis-1",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "3.2 Exploratory Data Analysis",
    "text": "3.2 Exploratory Data Analysis\nExploratory plots and maps examine the raw accessibility patterns across Philadelphia before feature engineering.\n\n\nCode\n# Transit stops (raw)\nggplot() +\n  geom_sf(data = nhoods, \n          fill = NA, color = \"grey70\", linewidth = 0.3) +\n  geom_sf(data = transit, size = 0.3, alpha = 0.6) +\n  labs(title = \"Raw Layer Check: Transit Stops (SEPTA Spring 2025)\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nCode\n# Hospitals (raw)\nggplot() +\n  geom_sf(data = nhoods, \n          fill = NA, color = \"grey70\", linewidth = 0.3) +\n  geom_sf(data = hospitals, size = 0.6, alpha = 0.7) +\n  labs(title = \"Raw Layer Check: Hospitals\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nCode\n# Parks & Recreation Program Sites (raw)\nggplot() +\n  geom_sf(data = nhoods, \n          fill = NA, color = \"grey70\", linewidth = 0.3) +\n  geom_sf(data = parksrec, size = 0.4, alpha = 0.6) +\n  labs(title = \"Raw Layer Check: Parks & Recreation Sites\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nCode\n# Schools (centroids of polygons) — raw\nggplot() +\n  geom_sf(data = nhoods, \n          fill = NA, color = \"grey70\", linewidth = 0.3) +\n  geom_sf(data = schools, size = 0.4, alpha = 0.7) +\n  labs(title = \"Raw Layer Check: Schools (Centroids)\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nCode\n# Crime points are huge; sampling for speed\nset.seed(5080)\ncrime_quick &lt;- if (nrow(crime) &gt; 30000) dplyr::slice_sample(crime, n = 30000) else crime\n\nggplot() +\n  geom_sf(data = nhoods, \n          fill = NA, color = \"grey70\", linewidth = 0.3) +\n  geom_sf(data = crime_quick, size = 0.1, alpha = 0.25) +\n  labs(title = \"Raw Layer Check: Crime Incidents (sampled if large)\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n3.2.1 Interpretation\n\nTransit Stops: Dense corridors radiate from Center City, showing strong transit coverage.\nHospitals: Sparse but geographically balanced.\nParks & Recreation: uneven distribution,\nSchools: evenly distributed across most neighborhoods\nCrime: Visibly concentrated, confirming the need for log-transformed counts"
  },
  {
    "objectID": "assignments/assignment_3/MUSA-5080-Midterm/model_script.html#feature-engineering-1",
    "href": "assignments/assignment_3/MUSA-5080-Midterm/model_script.html#feature-engineering-1",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "3.3 Feature Engineering",
    "text": "3.3 Feature Engineering\nSpatial features were derived using two complementary approaches: k-Nearest Neighbor (kNN) and buffer-based counts, depending on whether accessibility was best captured as proximity or exposure.\n\n\nCode\n#| message: false\n#| warning: false\n\n#clean sales data\nsales_xy &lt;- st_coordinates(OPA_points)\nok_sales  &lt;- complete.cases(sales_xy)\nOPA_points &lt;- OPA_points[ok_sales, ]    # keep only rows with valid XY\nsales_xy  &lt;- st_coordinates(OPA_points) # refresh coordinates\n\ntransit_xy &lt;- st_coordinates(transit)\nhosp_xy    &lt;- st_coordinates(hospitals)\n\n# feature 1 - distance to nearest transit stop (ft)\nknn_tr &lt;- FNN::get.knnx(\n  data  = st_coordinates(transit),\n  query = sales_xy,\n  k = 1)\n\nOPA_points &lt;- OPA_points %&gt;%\n  mutate(dist_nearest_transit_ft = as.numeric(knn_tr$nn.dist[,1]))\n\n# feature 2 - distance to nearest hospital (ft)\nknn_hp &lt;- FNN::get.knnx(\n  data  = st_coordinates(hospitals),\n  query = sales_xy,\n  k = 1)\n\nOPA_points &lt;- OPA_points %&gt;%\n  mutate(dist_nearest_hospital_ft = as.numeric(knn_hp$nn.dist[,1]))\n\n\n\n\nCode\n# feature 3 - parks/rec sites within 0.25 mi (count)\nrel_parks &lt;- sf::st_is_within_distance(OPA_points, parksrec, dist = r_park_ft)\n\nOPA_points &lt;- OPA_points %&gt;%\n  mutate(parks_cnt_0p25mi = lengths(rel_parks))\n\n# feature 4 - crime count within 0.5 mi (per square mile)\ncrime_buffer &lt;- st_buffer(OPA_points, dist = r_crime_ft)\n\nrel_crime &lt;- st_intersects(crime_buffer, crime, sparse = TRUE)\n\n# count number of crimes\ncrime_cnt &lt;- lengths(rel_crime)\n\nrm(rel_crime)\n\nOPA_points &lt;- OPA_points |&gt;\n  mutate(\n    crime_cnt_0p5mi     = crime_cnt,\n    log1p_crime_cnt_0p5 = log1p(crime_cnt_0p5mi)\n  )\n\n# feature 5 - schools within 0.5 mi (using centroids)\nrel_sch &lt;- sf::st_is_within_distance(OPA_points, schools, dist = r_school_ft)\n\nOPA_points &lt;- OPA_points %&gt;%\n  mutate(schools_cnt_0p5mi = lengths(rel_sch))\n\n\n\n3.3.1 Summary Table and Justification\n\n\n\n\n\n\n\n\n\nFeature\nMethod\nParameter\nTheoretical Rationale\n\n\n\n\nDistance to Nearest Transit Stop\nkNN (k = 1)\n–\nCaptures ease of access to public transport; nearest stop approximates walkability and job access.\n\n\nDistance to Nearest Hospital\nkNN (k = 1)\n–\nReflects accessibility to health care and emergency services; proximity adds perceived security for households.\n\n\nParks & Rec Sites within 0.25 mi\nBuffer Count\nr = 0.25 mi\nMeasures exposure to green space and recreational facilities within a 5-minute walk; positive amenity effect on property value.\n\n\nCrime Incidents within 0.5 mi\nBuffer Count\nr = 0.5 mi\nRepresents local safety environment; higher crime counts reduce housing desirability.\n\n\nSchools within 0.5 mi\nBuffer Count\nr = 0.5 mi\nReflects educational access and family appeal; clustering of schools often raises residential demand.\n\n\nPopulation\nCensus\n–\nRepresents the present residential demand within a census tract\n\n\nMedian Household Income\nCensus\n–\nIndicative of the ability of present residents of a census tract to afford housing\n\n\nMedian Age\nCensus\n–\nMeasure of the dominant age group in a census tract (i.e. high student or elderly population)\n\n\n\n\n\n3.3.2 Feature Validation and Visualization\n\n\nCode\n## Transit Accessibility\nggplot(OPA_points, aes(x = dist_nearest_transit_ft)) +\n  geom_histogram(fill = \"steelblue\", color = \"white\", bins = 30) +\n  labs(title = \"Distribution: Distance to Nearest Transit Stop\",\n       x = \"Feet to Nearest Stop\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nggplot(OPA_points) +\n  geom_sf(data = nhoods, \n          fill = NA, color = \"grey70\", linewidth = 0.3) +\n  geom_sf(aes(color = dist_nearest_transit_ft), size = 0.5) +\n  scale_color_viridis_c(option = \"plasma\", labels = comma) +\n  labs(title = \"Transit Accessibility Across Sales Parcels\",\n       color = \"Distance (ft)\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nCode\n## Hospital Proximity\nggplot(OPA_points, aes(x = dist_nearest_hospital_ft)) +\n  geom_histogram(fill = \"darkorange\", color = \"white\", bins = 30) +\n  labs(title = \"Distribution: Distance to Nearest Hospital\",\n       x = \"Feet to Nearest Hospital\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nggplot(OPA_points) +\n  geom_sf(data = nhoods, \n          fill = NA, color = \"grey70\", linewidth = 0.3) +\n  geom_sf(aes(color = dist_nearest_hospital_ft), size = 0.5) +\n  scale_color_viridis_c(option = \"magma\", labels = comma) +\n  labs(title = \"Hospital Accessibility Across Sales Parcels\",\n       color = \"Distance (ft)\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nCode\n## Parks & Recreation\nggplot(OPA_points, aes(x = parks_cnt_0p25mi)) +\n  geom_histogram(fill = \"seagreen\", color = \"white\", bins = 20) +\n  labs(title = \"Distribution: Parks & Rec Sites Within 0.25 mi\",\n       x = \"Count within 0.25 mi\", y = \"Number of Parcels\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nggplot(OPA_points) +\n  geom_sf(data = nhoods, \n          fill = NA, color = \"grey70\", linewidth = 0.3) +\n  geom_sf(aes(color = parks_cnt_0p25mi), size = 0.6) +\n  scale_color_viridis_c(option = \"viridis\") +\n  labs(title = \"Proximity to Parks & Recreation (0.25 mi Buffer)\",\n       color = \"Parks Count\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nCode\n## Crime Counts\nggplot(OPA_points, aes(x = crime_cnt_0p5mi)) +\n  geom_histogram(fill = \"firebrick\", color = \"white\", bins = 30) +\n  labs(title = \"Distribution: Crime Incidents Within 0.5 mi\",\n       x = \"Crime Count (2023–2024)\", y = \"Number of Parcels\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nggplot(OPA_points) +\n  geom_sf(data = nhoods, \n          fill = NA, color = \"grey70\", linewidth = 0.3) +\n  geom_sf(aes(color = log1p_crime_cnt_0p5), size = 0.6) +\n  scale_color_viridis_c(option = \"inferno\") +\n  labs(title = \"Crime Exposure (log-transformed within 0.5 mi)\",\n       color = \"log(1+Crime Count)\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nCode\n## Schools Accessibility\nggplot(OPA_points, aes(x = schools_cnt_0p5mi)) +\n  geom_histogram(fill = \"purple\", color = \"white\", bins = 20) +\n  labs(title = \"Distribution: Schools Within 0.5 mi\",\n       x = \"School Count (0.5 mi Buffer)\", y = \"Number of Parcels\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nggplot(OPA_points) +\n  geom_sf(data = nhoods, \n          fill = NA, color = \"grey70\", linewidth = 0.3) +\n  geom_sf(aes(color = schools_cnt_0p5mi), size = 0.6) +\n  scale_color_viridis_c(option = \"cividis\") +\n  labs(title = \"School Accessibility (0.5 mi Buffer)\",\n       color = \"Schools Count\") +\n  theme_void()\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nTransit proximity: Most parcels are within 500 ft of a stop, confirming strong transit coverage across Philadelphia.\nHospital proximity: Right-skewed distribution, consistent with limited facility count.\nParks access: Sparse exposure (mostly 0–1 within 0.25 mi), highlighting recreational inequities.\nCrime exposure: Wide variation, clustered along high-density corridors; log-transformed to stabilize scale.\nSchool proximity: Uniform urban coverage with typical parcels having 4-7 schools within 0.5 mi.\n\n\n\nCode\nsp_data &lt;- st_read(\"./data/OPA_data.geojson\", quiet = T)\n\nstr(sp_data$sale_price)\n\n\n int [1:23611] 389000 20000 309000 185000 399000 50000 70000 50000 30000 230000 ...\n\n\nCode\nsp_data_filtered &lt;- sp_data %&gt;%\n  mutate(sale_price = as.numeric(sale_price)) %&gt;%\n  filter(sale_price &gt; 1000)\n\nggplot(sp_data_filtered, aes(x = sale_price)) + \n  geom_histogram(\n    binwidth = 20000, \n    fill = \"grey\",\n    color = \"black\"\n  ) +\n  labs(\n    title = \"Histogram of Sale Prices in Philadelphia\", \n    x = \"Sale Price\",\n    y = \"Count of Homes\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = label_dollar()) +\n  coord_cartesian(xlim = c(0, 2000000), ylim = c(0, 3000))\n\n\n\n\n\n\n\n\n\nCode\nsummary(sp_data_filtered$sale_price)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n    1500   170000   269000   348626   395000 30000000 \n\n\nCode\nsp_data_filtered &lt;- sp_data_filtered %&gt;%\n  mutate(\n    sale_price = as.numeric(sale_price),\n    sale_price_capped = pmin(sale_price, quantile(sale_price, 0.99, na.rm = TRUE))\n  )\n\nggplot(sp_data_filtered) +\n  geom_sf(aes(color = sale_price_capped), size = 0.6, alpha = 0.7) +\n  scale_color_viridis_c(labels = label_dollar(), name = \"Sale Price (USD)\") +\n  labs(title = \"Philadelphia Sale Prices\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# function to check for statistically significant correlations between independent variables\nsig_corr &lt;- function(dat, dep_var) {\n  # remove the independent variable from the dataset\n  dat_corr &lt;- dat %&gt;% select(-all_of(dep_var))\n  \n  # run a correlation matrix for the independent vars\n  correlation_matrix &lt;- rcorr(as.matrix(dat_corr))\n  values &lt;- correlation_matrix$r\n  vifs &lt;- apply(values, 1, function(x){\n    return(round(1/(1-abs(x)), 2))\n  })\n  \n  values_df &lt;- values %&gt;% as.data.frame()\n  vifs_df &lt;- vifs %&gt;% as.data.frame()\n  \n  # convert correlation coefficients and p-values to long format\n  corCoeff_df &lt;- correlation_matrix$r %&gt;% \n    as.data.frame() %&gt;% \n    mutate(var1 = rownames(.))\n  \n  corVIF_df &lt;- vifs %&gt;% \n    as.data.frame() %&gt;% \n    mutate(var1 = rownames(.))\n  \n  corPval_df &lt;- correlation_matrix$P %&gt;% \n    as.data.frame() %&gt;% \n    mutate(var1 = rownames(.))\n  \n  # merge long format data\n  corMerge &lt;- list(\n    corCoeff_df %&gt;% pivot_longer(-var1, names_to = \"var2\", values_to = \"correlation\") %&gt;% as.data.frame(),\n    corVIF_df %&gt;% pivot_longer(-var1, names_to = \"var2\", values_to = \"vif_factor\") %&gt;% as.data.frame(),\n    corPval_df %&gt;% pivot_longer(-var1, names_to = \"var2\", values_to = \"p_value\") %&gt;% as.data.frame()) %&gt;%\n    reduce(left_join, by = c(\"var1\", \"var2\"))\n  \n  # filter to isolate unique pairs, then rows with correlation &gt; 0.5 and p &lt; 0.05\n  corUnfiltered &lt;- corMerge %&gt;% \n    filter(var1 != var2) %&gt;% \n    rowwise() %&gt;% \n    filter(var1 &lt; var2) %&gt;% \n    ungroup() %&gt;% \n    as.data.frame()\n  \n  corFiltered &lt;- corUnfiltered %&gt;% \n    filter(abs(vif_factor) &gt; 3 & p_value &lt; 0.05) %&gt;% \n    arrange(desc(abs(correlation)))\n  \n  # save the raw correlation values and the filtered variable pairs\n  final &lt;- set_names(list(values_df, vifs_df, corUnfiltered, corFiltered),\n                     c(\"R2\", \"VIF\", \"AllCor\", \"SelCor\"))\n  \n  return(final)\n}\n\n# create a dataset with just modeling variables\nOPA_modelvars &lt;- OPA_points %&gt;% select(sale_price, total_livable_area, building_age, number_of_bedrooms, number_of_bathrooms,\n                                       pop_totE, med_hh_incE, med_ageE,\n                                       dist_nearest_transit_ft, dist_nearest_hospital_ft, parks_cnt_0p25mi, log1p_crime_cnt_0p5, schools_cnt_0p5mi,\n                                       )\n\n# calculate VIFs and determine potentially troublesome correlations between variables\nvif_check &lt;- sig_corr(OPA_modelvars %&gt;% st_drop_geometry(), dep_var = \"sale_price\")\n\nkable(vif_check[[\"VIF\"]])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntotal_livable_area\nbuilding_age\nnumber_of_bedrooms\nnumber_of_bathrooms\npop_totE\nmed_hh_incE\nmed_ageE\ndist_nearest_transit_ft\ndist_nearest_hospital_ft\nparks_cnt_0p25mi\nlog1p_crime_cnt_0p5\nschools_cnt_0p5mi\n\n\n\n\ntotal_livable_area\nInf\n1.22\n1.70\n1.99\n1.14\n1.24\n1.07\n1.07\n1.01\n1.03\n1.22\n1.03\n\n\nbuilding_age\n1.22\nInf\n1.01\n1.32\n1.04\n1.23\n1.18\n1.21\n1.12\n1.05\n1.43\n1.24\n\n\nnumber_of_bedrooms\n1.70\n1.01\nInf\n2.26\n1.02\n1.09\n1.05\n1.05\n1.03\n1.01\n1.07\n1.02\n\n\nnumber_of_bathrooms\n1.99\n1.32\n2.26\nInf\n1.12\n1.25\n1.03\n1.02\n1.04\n1.01\n1.09\n1.01\n\n\npop_totE\n1.14\n1.04\n1.02\n1.12\nInf\n1.33\n1.16\n1.09\n1.18\n1.00\n1.01\n1.08\n\n\nmed_hh_incE\n1.24\n1.23\n1.09\n1.25\n1.33\nInf\n1.13\n1.01\n1.13\n1.04\n1.31\n1.03\n\n\nmed_ageE\n1.07\n1.18\n1.05\n1.03\n1.16\n1.13\nInf\n1.15\n1.27\n1.15\n1.70\n1.25\n\n\ndist_nearest_transit_ft\n1.07\n1.21\n1.05\n1.02\n1.09\n1.01\n1.15\nInf\n1.25\n1.11\n1.72\n1.45\n\n\ndist_nearest_hospital_ft\n1.01\n1.12\n1.03\n1.04\n1.18\n1.13\n1.27\n1.25\nInf\n1.10\n1.85\n1.66\n\n\nparks_cnt_0p25mi\n1.03\n1.05\n1.01\n1.01\n1.00\n1.04\n1.15\n1.11\n1.10\nInf\n1.25\n1.23\n\n\nlog1p_crime_cnt_0p5\n1.22\n1.43\n1.07\n1.09\n1.01\n1.31\n1.70\n1.72\n1.85\n1.25\nInf\n2.46\n\n\nschools_cnt_0p5mi\n1.03\n1.24\n1.02\n1.01\n1.08\n1.03\n1.25\n1.45\n1.66\n1.23\n2.46\nInf\n\n\n\n\n\nNone of the variables tested have a significant VIF score that is above 3, indicating that there is little concern of multicollinearity in the models moving forward."
  },
  {
    "objectID": "assignments/assignment_3/MUSA-5080-Midterm/model_script.html#model-1-structural-terms",
    "href": "assignments/assignment_3/MUSA-5080-Midterm/model_script.html#model-1-structural-terms",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "4.1 Model 1: Structural Terms",
    "text": "4.1 Model 1: Structural Terms\nOur first model uses structural property characteristics to build a multiple linear regression, regressing sale price on total livable area, number of bedrooms, number of bathrooms, and building age.\n\n\nCode\nmodel1_data &lt;- na.omit(OPA_points)\n\nmodel1 &lt;- lm(\n  sale_price ~ \n    total_livable_area +\n    number_of_bedrooms +\n    number_of_bathrooms +\n    building_age,\n\n  data = model1_data\n)\n\nsummary(model1)\n\n\n\nCall:\nlm(formula = sale_price ~ total_livable_area + number_of_bedrooms + \n    number_of_bathrooms + building_age, data = model1_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1063434   -92640   -20598    58880  7759292 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         -11217.250  12625.853  -0.888    0.374    \ntotal_livable_area     221.516      5.126  43.217   &lt;2e-16 ***\nnumber_of_bedrooms  -34668.316   3239.669 -10.701   &lt;2e-16 ***\nnumber_of_bathrooms  70054.014   3967.307  17.658   &lt;2e-16 ***\nbuilding_age           144.447    104.814   1.378    0.168    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 233700 on 10324 degrees of freedom\nMultiple R-squared:  0.2762,    Adjusted R-squared:  0.2759 \nF-statistic: 984.7 on 4 and 10324 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "assignments/assignment_3/MUSA-5080-Midterm/model_script.html#model-2-incorporation-of-census-data",
    "href": "assignments/assignment_3/MUSA-5080-Midterm/model_script.html#model-2-incorporation-of-census-data",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "4.2 Model 2: Incorporation of Census Data",
    "text": "4.2 Model 2: Incorporation of Census Data\nOur second model builds on the structural property characteristics regression by incorporating census tract–level variables, including population, median household income, and median age.\n\n\nCode\nmodel2_data &lt;- na.omit(OPA_points)\n\nmodel2 &lt;- lm(\n  sale_price ~ \n    total_livable_area +\n    number_of_bedrooms +\n    number_of_bathrooms +\n    building_age +\n\n    pop_totE +\n    med_hh_incE +\n    med_ageE,            \n    \n  data = model2_data\n)\n\nsummary(model2)\n\n\n\nCall:\nlm(formula = sale_price ~ total_livable_area + number_of_bedrooms + \n    number_of_bathrooms + building_age + pop_totE + med_hh_incE + \n    med_ageE, data = model2_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-811594  -68780  -14242   37407 7882319 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         -1.468e+05  2.209e+04  -6.645 3.19e-11 ***\ntotal_livable_area   1.827e+02  4.950e+00  36.914  &lt; 2e-16 ***\nnumber_of_bedrooms  -1.752e+04  3.086e+03  -5.677 1.41e-08 ***\nnumber_of_bathrooms  5.754e+04  3.746e+03  15.359  &lt; 2e-16 ***\nbuilding_age         1.015e+02  1.018e+02   0.997    0.319    \npop_totE            -7.511e+00  1.370e+00  -5.483 4.27e-08 ***\nmed_hh_incE          2.584e+00  7.470e-02  34.587  &lt; 2e-16 ***\nmed_ageE             4.961e+02  3.580e+02   1.386    0.166    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 219700 on 10321 degrees of freedom\nMultiple R-squared:  0.3601,    Adjusted R-squared:  0.3596 \nF-statistic: 829.6 on 7 and 10321 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "assignments/assignment_3/MUSA-5080-Midterm/model_script.html#model-3-incorporation-of-spatial-features",
    "href": "assignments/assignment_3/MUSA-5080-Midterm/model_script.html#model-3-incorporation-of-spatial-features",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "4.3 Model 3: Incorporation of Spatial Features",
    "text": "4.3 Model 3: Incorporation of Spatial Features\n\n\nCode\nmodel3_data &lt;- na.omit(OPA_points)\n\nmodel3 &lt;- lm(\n  sale_price ~ \n    total_livable_area +\n    number_of_bedrooms +\n    number_of_bathrooms +\n    building_age +\n    total_area +\n    \n    pop_totE +\n    med_hh_incE +\n    med_ageE +  \n\n    dist_nearest_transit_ft +\n    dist_nearest_hospital_ft +\n    parks_cnt_0p25mi +\n    log1p_crime_cnt_0p5,\n    \n  data = model3_data\n)\n\nsummary(model3)\n\n\n\nCall:\nlm(formula = sale_price ~ total_livable_area + number_of_bedrooms + \n    number_of_bathrooms + building_age + total_area + pop_totE + \n    med_hh_incE + med_ageE + dist_nearest_transit_ft + dist_nearest_hospital_ft + \n    parks_cnt_0p25mi + log1p_crime_cnt_0p5, data = model3_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-918622  -69409  -13313   39352 7885223 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              -2.894e+05  4.384e+04  -6.602 4.26e-11 ***\ntotal_livable_area        1.638e+02  5.666e+00  28.913  &lt; 2e-16 ***\nnumber_of_bedrooms       -1.433e+04  3.086e+03  -4.645 3.44e-06 ***\nnumber_of_bathrooms       5.671e+04  3.736e+03  15.179  &lt; 2e-16 ***\nbuilding_age             -2.221e+02  1.135e+02  -1.957  0.05042 .  \ntotal_area                5.785e+00  7.711e-01   7.502 6.79e-14 ***\npop_totE                 -6.855e+00  1.382e+00  -4.960 7.17e-07 ***\nmed_hh_incE               2.686e+00  8.168e-02  32.885  &lt; 2e-16 ***\nmed_ageE                  1.083e+03  3.729e+02   2.905  0.00368 ** \ndist_nearest_transit_ft   1.249e+00  7.454e+00   0.168  0.86691    \ndist_nearest_hospital_ft -4.899e+00  7.799e-01  -6.282 3.47e-10 ***\nparks_cnt_0p25mi          3.092e+03  3.617e+03   0.855  0.39268    \nlog1p_crime_cnt_0p5       2.199e+04  4.391e+03   5.009 5.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 218400 on 10316 degrees of freedom\nMultiple R-squared:  0.3679,    Adjusted R-squared:  0.3672 \nF-statistic: 500.4 on 12 and 10316 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "assignments/assignment_3/MUSA-5080-Midterm/model_script.html#model-4-incorporation-of-interactions-and-fixed-effects",
    "href": "assignments/assignment_3/MUSA-5080-Midterm/model_script.html#model-4-incorporation-of-interactions-and-fixed-effects",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "4.4 Model 4: Incorporation of Interactions and Fixed Effects",
    "text": "4.4 Model 4: Incorporation of Interactions and Fixed Effects\n\n\nCode\n# join data separately here to avoid conflicts with earlier code blocks\nOPA_points_copy &lt;- left_join(OPA_points,\n                             neighbor_points %&gt;%\n                               select(parcel_number, wealthy_neighborhood) %&gt;%\n                               st_drop_geometry(),\n                             by = \"parcel_number\")\n\n\n\n\nCode\nmodel4_data &lt;- na.omit(OPA_points_copy)\n\nmodel4 &lt;- lm(\n  sale_price ~ \n    total_livable_area +\n    number_of_bedrooms +\n    number_of_bathrooms +\n    building_age +\n    total_area +\n    \n    pop_totE +\n    med_hh_incE +\n    med_ageE +  \n  \n    dist_nearest_transit_ft +\n    dist_nearest_hospital_ft +\n    parks_cnt_0p25mi +\n    log1p_crime_cnt_0p5 +\n    \n    number_of_bathrooms * wealthy_neighborhood +\n    \n    int_type_tarea +\n    int_value_larea +\n    int_value_tarea +\n    int_larea_econd +\n    int_larea_icond +\n    int_larea_beds,\n    \n                     \n  data = model4_data\n)\n\nsummary(model4)\n\n\n\nCall:\nlm(formula = sale_price ~ total_livable_area + number_of_bedrooms + \n    number_of_bathrooms + building_age + total_area + pop_totE + \n    med_hh_incE + med_ageE + dist_nearest_transit_ft + dist_nearest_hospital_ft + \n    parks_cnt_0p25mi + log1p_crime_cnt_0p5 + number_of_bathrooms * \n    wealthy_neighborhood + int_type_tarea + int_value_larea + \n    int_value_tarea + int_larea_econd + int_larea_icond + int_larea_beds, \n    data = model4_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1346106   -55375   -11955    28236  7763696 \n\nCoefficients:\n                                                  Estimate Std. Error t value\n(Intercept)                                      1.244e+05  4.545e+04   2.737\ntotal_livable_area                               1.576e+02  1.480e+01  10.654\nnumber_of_bedrooms                               3.121e+04  4.850e+03   6.434\nnumber_of_bathrooms                              2.500e+04  3.766e+03   6.637\nbuilding_age                                    -3.693e+02  1.063e+02  -3.475\ntotal_area                                       9.561e+00  1.411e+00   6.774\npop_totE                                        -3.870e+00  1.282e+00  -3.018\nmed_hh_incE                                      1.039e+00  9.112e-02  11.407\nmed_ageE                                         4.362e+02  3.457e+02   1.262\ndist_nearest_transit_ft                          1.250e+00  6.891e+00   0.181\ndist_nearest_hospital_ft                        -3.494e+00  7.206e-01  -4.848\nparks_cnt_0p25mi                                -4.718e+02  3.344e+03  -0.141\nlog1p_crime_cnt_0p5                             -9.748e+03  4.322e+03  -2.255\nwealthy_neighborhoodWealthy                      3.983e+04  1.496e+04   2.663\nint_type_tarea                                  -1.635e-02  1.014e-02  -1.613\nint_value_larea                                  1.564e-04  5.782e-06  27.055\nint_value_tarea                                 -8.169e-06  7.365e-07 -11.091\nint_larea_econd                                 -1.033e+01  2.605e+00  -3.966\nint_larea_icond                                 -1.224e+01  2.033e+00  -6.021\nint_larea_beds                                  -1.491e+01  2.010e+00  -7.418\nnumber_of_bathrooms:wealthy_neighborhoodWealthy  5.800e+04  7.671e+03   7.560\n                                                Pr(&gt;|t|)    \n(Intercept)                                     0.006218 ** \ntotal_livable_area                               &lt; 2e-16 ***\nnumber_of_bedrooms                              1.30e-10 ***\nnumber_of_bathrooms                             3.35e-11 ***\nbuilding_age                                    0.000512 ***\ntotal_area                                      1.32e-11 ***\npop_totE                                        0.002551 ** \nmed_hh_incE                                      &lt; 2e-16 ***\nmed_ageE                                        0.207050    \ndist_nearest_transit_ft                         0.856088    \ndist_nearest_hospital_ft                        1.26e-06 ***\nparks_cnt_0p25mi                                0.887786    \nlog1p_crime_cnt_0p5                             0.024129 *  \nwealthy_neighborhoodWealthy                     0.007762 ** \nint_type_tarea                                  0.106875    \nint_value_larea                                  &lt; 2e-16 ***\nint_value_tarea                                  &lt; 2e-16 ***\nint_larea_econd                                 7.37e-05 ***\nint_larea_icond                                 1.79e-09 ***\nint_larea_beds                                  1.28e-13 ***\nnumber_of_bathrooms:wealthy_neighborhoodWealthy 4.36e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 201400 on 10308 degrees of freedom\nMultiple R-squared:  0.4631,    Adjusted R-squared:  0.462 \nF-statistic: 444.5 on 20 and 10308 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n#there is only a premium on wealth neighborhood for total area, total livable area, and number of bathrooms that are significant. There is also a significant value for int_value_larea just from interacting the OPA data itsself which just assesses market value and size scalability."
  },
  {
    "objectID": "assignments/assignment_3/MUSA-5080-Midterm/model_script.html#comparison-of-model-performance",
    "href": "assignments/assignment_3/MUSA-5080-Midterm/model_script.html#comparison-of-model-performance",
    "title": "Philadelphia Housing Price Prediction - Technical Appendix",
    "section": "4.5 Comparison of Model Performance",
    "text": "4.5 Comparison of Model Performance\nWe can evaluate performance by conducting a 10-fold cross-validation of the 4 models, and comparing their RMSE, MAE, and \\(R^2\\).\n\n\nCode\n# Define 10-fold CV\ntrain_control &lt;- trainControl(\n  method = \"cv\",\n  number = 10,\n  savePredictions = \"final\"\n)\n\n\n\n\nCode\n# Model 1: Structural Features Only\nmodel1_cv &lt;- train(\n  sale_price ~ \n    total_livable_area +\n    number_of_bedrooms +\n    number_of_bathrooms +\n    building_age,\n  data = na.omit(OPA_points),\n  method = \"lm\",\n  trControl = train_control\n)\n\nmodel1_cv\n\n\nLinear Regression \n\n10329 samples\n    5 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 9295, 9297, 9296, 9297, 9296, 9296, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  230140.4  0.2865262  115174.1\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\n\nCode\n# Model 2: Structural + Census\nmodel2_cv &lt;- train(\n  sale_price ~ \n    total_livable_area +\n    number_of_bedrooms +\n    number_of_bathrooms +\n    building_age +\n    pop_totE +\n    med_hh_incE +\n    med_ageE,\n  data = na.omit(OPA_points),\n  method = \"lm\",\n  trControl = train_control\n)\n\nmodel2_cv\n\n\nLinear Regression \n\n10329 samples\n    8 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 9296, 9296, 9297, 9297, 9297, 9296, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  215430.3  0.3799429  90217.97\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\n\nCode\n# Model 3: Structural + Census + Spatial\nmodel3_cv &lt;- train(\n  sale_price ~ \n    total_livable_area +\n    number_of_bedrooms +\n    number_of_bathrooms +\n    building_age +\n    total_area +\n    \n    pop_totE +\n    med_hh_incE +\n    med_ageE +  \n\n    dist_nearest_transit_ft +\n    dist_nearest_hospital_ft +\n    parks_cnt_0p25mi +\n    log1p_crime_cnt_0p5,\n  data = na.omit(OPA_points),\n  method = \"lm\",\n  trControl = train_control\n)\n\nmodel3_cv\n\n\nLinear Regression \n\n10329 samples\n   13 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 9295, 9297, 9295, 9295, 9296, 9298, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  211021.3  0.4053303  90144.74\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\n\nCode\n# Model 4: Structural + Census + Spatial + Interaction\nmodel4_cv &lt;- train(\n  sale_price ~ \n    total_livable_area +\n    number_of_bedrooms +\n    number_of_bathrooms +\n    building_age +\n    total_area +\n    \n    pop_totE +\n    med_hh_incE +\n    med_ageE +  \n  \n    dist_nearest_transit_ft +\n    dist_nearest_hospital_ft +\n    parks_cnt_0p25mi +\n    log1p_crime_cnt_0p5 +\n    \n    number_of_bathrooms * wealthy_neighborhood +\n    \n    int_type_tarea +\n    int_value_larea +\n    int_value_tarea +\n    int_larea_econd +\n    int_larea_icond +\n    int_larea_beds,\n  \n  data = na.omit(OPA_points_copy),\n  method = \"lm\",\n  trControl = train_control\n)\n\nmodel4_cv\n\n\nLinear Regression \n\n10329 samples\n   20 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 9297, 9296, 9296, 9296, 9296, 9297, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  196081.1  0.4895234  71582.45\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\n\nCode\ncv_results &lt;- data.frame(\n  Model = c(\"Model 1\", \n            \"Model 2\", \n            \"Model 3\", \n            \"Model 4\"),\n  RMSE = c(\n        model1_cv$results$RMSE,\n        model2_cv$results$RMSE,\n        model3_cv$results$RMSE,\n        model4_cv$results$RMSE\n      ),\n  \n  log_RMSE = c(\n        log(model1_cv$results$RMSE),\n        log(model2_cv$results$RMSE),\n        log(model3_cv$results$RMSE),\n        log(model4_cv$results$RMSE)\n      ),\n  \n    MAE = c(\n        model1_cv$results$MAE,\n        model2_cv$results$MAE,\n        model3_cv$results$MAE,\n        model4_cv$results$MAE\n      ),\n  R_squared = c(\n        model1_cv$results$Rsquared,\n        model2_cv$results$Rsquared,\n        model3_cv$results$Rsquared,\n        model4_cv$results$Rsquared\n      )\n)\n\nprint(cv_results)\n\n\n    Model     RMSE log_RMSE       MAE R_squared\n1 Model 1 230140.4 12.34644 115174.13 0.2865262\n2 Model 2 215430.3 12.28039  90217.97 0.3799429\n3 Model 3 211021.3 12.25971  90144.74 0.4053303\n4 Model 4 196081.1 12.18628  71582.45 0.4895234\n\n\n\n\nCode\n# create model coefficient table in stargazer\nmodels_list &lt;- list(model1, model2, model3, model4)\nmodels_summary_table &lt;- stargazer(models_list, type = \"text\", style = \"default\")\n\n\n\n=============================================================================================================================================================\n                                                                                             Dependent variable:                                             \n                                                -------------------------------------------------------------------------------------------------------------\n                                                                                                 sale_price                                                  \n                                                           (1)                        (2)                         (3)                         (4)            \n-------------------------------------------------------------------------------------------------------------------------------------------------------------\ntotal_livable_area                                      221.516***                 182.734***                 163.814***                  161.483***         \n                                                         (5.126)                    (4.950)                     (5.666)                    (14.914)          \n                                                                                                                                                             \nnumber_of_bedrooms                                    -34,668.320***             -17,516.780***             -14,332.170***               33,544.300***       \n                                                       (3,239.669)                (3,085.682)                 (3,085.568)                 (4,886.944)        \n                                                                                                                                                             \nnumber_of_bathrooms                                   70,054.010***              57,541.230***               56,709.550***               29,510.940***       \n                                                       (3,967.307)                (3,746.347)                 (3,736.110)                 (3,784.735)        \n                                                                                                                                                             \nbuilding_age                                             144.447                    101.483                    -222.107*                  -361.569***        \n                                                        (104.814)                  (101.838)                   (113.514)                   (107.142)         \n                                                                                                                                                             \ntotal_area                                                                                                     5.785***                    9.178***          \n                                                                                                                (0.771)                     (1.423)          \n                                                                                                                                                             \npop_totE                                                                           -7.511***                   -6.855***                   -4.352***         \n                                                                                    (1.370)                     (1.382)                     (1.292)          \n                                                                                                                                                             \nmed_hh_incE                                                                         2.584***                   2.686***                    1.269***          \n                                                                                    (0.075)                     (0.082)                     (0.089)          \n                                                                                                                                                             \nmed_ageE                                                                            496.058                  1,083.065***                   385.094          \n                                                                                   (357.954)                   (372.864)                   (348.430)         \n                                                                                                                                                             \ndist_nearest_transit_ft                                                                                          1.249                       4.649           \n                                                                                                                (7.454)                     (6.953)          \n                                                                                                                                                             \ndist_nearest_hospital_ft                                                                                       -4.899***                   -3.733***         \n                                                                                                                (0.780)                     (0.726)          \n                                                                                                                                                             \nparks_cnt_0p25mi                                                                                               3,091.571                    888.652          \n                                                                                                              (3,616.676)                 (3,368.264)        \n                                                                                                                                                             \nlog1p_crime_cnt_0p5                                                                                          21,993.960***                -3,655.516         \n                                                                                                              (4,390.917)                 (4,325.072)        \n                                                                                                                                                             \nwealthy_neighborhoodWealthy                                                                                                              50,306.450***       \n                                                                                                                                         (14,800.200)        \n                                                                                                                                                             \nint_type_tarea                                                                                                                              -0.014           \n                                                                                                                                            (0.010)          \n                                                                                                                                                             \nint_value_larea                                                                                                                            0.0002***         \n                                                                                                                                           (0.00001)         \n                                                                                                                                                             \nint_value_tarea                                                                                                                           -0.00001***        \n                                                                                                                                           (0.00000)         \n                                                                                                                                                             \nint_larea_econd                                                                                                                           -10.661***         \n                                                                                                                                            (2.626)          \n                                                                                                                                                             \nint_larea_icond                                                                                                                           -11.958***         \n                                                                                                                                            (2.049)          \n                                                                                                                                                             \nint_larea_beds                                                                                                                            -15.738***         \n                                                                                                                                            (2.024)          \n                                                                                                                                                             \nnumber_of_bathrooms:wealthy_neighborhoodWealthy                                                                                          30,549.650***       \n                                                                                                                                          (7,811.896)        \n                                                                                                                                                             \nConstant                                               -11,217.250              -146,801.100***             -289,389.700***               50,804.290         \n                                                       (12,625.850)               (22,092.030)               (43,835.430)                (45,386.800)        \n                                                                                                                                                             \n-------------------------------------------------------------------------------------------------------------------------------------------------------------\nObservations                                              10,329                     10,329                     10,329                      10,329           \nR2                                                        0.276                      0.360                       0.368                       0.454           \nAdjusted R2                                               0.276                      0.360                       0.367                       0.453           \nResidual Std. Error                              233,656.600 (df = 10324)   219,730.600 (df = 10321)   218,431.100 (df = 10316)    203,010.600 (df = 10308)  \nF Statistic                                     984.706*** (df = 4; 10324) 829.572*** (df = 7; 10321) 500.373*** (df = 12; 10316) 429.301*** (df = 20; 10308)\n=============================================================================================================================================================\nNote:                                                                                                                             *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\n\n\nCode\n# plot predicted vs actual value plots\nggplot(model1_cv$pred, aes(x = obs, y = pred)) +\n  geom_point(alpha = 0.3) +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(labels = dollar_format()) +\n  scale_y_continuous(labels = dollar_format()) +\n  labs(\n    title = \"Model 1 Cross-Validation: Predicted vs. Actual Sale Price\",\n    x = \"Actual Sale Price\",\n    y = \"Predicted Sale Price\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nggplot(model2_cv$pred, aes(x = obs, y = pred)) +\n  geom_point(alpha = 0.3) +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(labels = dollar_format()) +\n  scale_y_continuous(labels = dollar_format()) +\n  labs(\n    title = \"Model 2 Cross-Validation: Predicted vs. Actual Sale Price\",\n    x = \"Actual Sale Price\",\n    y = \"Predicted Sale Price\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nggplot(model3_cv$pred, aes(x = obs, y = pred)) +\n  geom_point(alpha = 0.3) +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(labels = dollar_format()) +\n  scale_y_continuous(labels = dollar_format()) +\n  labs(\n    title = \"Model 3 Cross-Validation: Predicted vs. Actual Sale Price\",\n    x = \"Actual Sale Price\",\n    y = \"Predicted Sale Price\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nggplot(model4_cv$pred, aes(x = obs, y = pred)) +\n  geom_point(alpha = 0.3) +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  scale_x_continuous(labels = dollar_format()) +\n  scale_y_continuous(labels = dollar_format()) +\n  labs(\n    title = \"Model 4 Cross-Validation: Predicted vs. Actual Sale Price\",\n    x = \"Actual Sale Price\",\n    y = \"Predicted Sale Price\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "assignments/assignment_3/MUSA-5080-Midterm/midterm_presentation.html#why-improve-the-model",
    "href": "assignments/assignment_3/MUSA-5080-Midterm/midterm_presentation.html#why-improve-the-model",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Why Improve the Model?",
    "text": "Why Improve the Model?\nResearch Question\nIdentify which structural, spatial, and socio-economic predictors contribute to a more accurate Automated Valuation Model for the City of Philadelphia.\nMotivation\nImproving the accuracy of residential property tax assessment can mitigate inequity assessment methods, increase transparency in governmental processes, and make analysis more reliable and efficient"
  },
  {
    "objectID": "assignments/assignment_3/MUSA-5080-Midterm/midterm_presentation.html#data-sources",
    "href": "assignments/assignment_3/MUSA-5080-Midterm/midterm_presentation.html#data-sources",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Data Sources",
    "text": "Data Sources\n\nProperty Sales: (Philadelphia, 2023-2024) City of Philadelphia - OpenDataPhilly\nSocio-Economics: United States Census - American Community Survey\nSpatial Features: City of Philadelphia - OpenDataPhilly"
  },
  {
    "objectID": "assignments/assignment_3/MUSA-5080-Midterm/midterm_presentation.html#sale-prices-in-philadelphia",
    "href": "assignments/assignment_3/MUSA-5080-Midterm/midterm_presentation.html#sale-prices-in-philadelphia",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Sale Prices in Philadelphia",
    "text": "Sale Prices in Philadelphia\nHigher sale prices are concentrated in Central and Northwest Philadelphia"
  },
  {
    "objectID": "assignments/assignment_3/MUSA-5080-Midterm/midterm_presentation.html#factors-impacting-sale-price",
    "href": "assignments/assignment_3/MUSA-5080-Midterm/midterm_presentation.html#factors-impacting-sale-price",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Factors Impacting Sale Price",
    "text": "Factors Impacting Sale Price\nThere is a notable relationship between sale price and total livable area"
  },
  {
    "objectID": "assignments/assignment_3/MUSA-5080-Midterm/midterm_presentation.html#model-comparison",
    "href": "assignments/assignment_3/MUSA-5080-Midterm/midterm_presentation.html#model-comparison",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Model Comparison",
    "text": "Model Comparison\n\nModel Performance Metrics\n\n\nModel\nRMSE\nMAE\nR²\n\n\n\n\nModel 1\n230,140.41\n15,174.13\n0.287\n\n\nModel 2\n215,430.39\n90,217.97\n0.380\n\n\nModel 3\n211,021.39\n90,144.74\n0.405\n\n\nModel 4\n196,081.17\n71,582.45\n0.490\n\n\n\n\nModel 1 (Structural) and Model 2 (Structural + Census) had the worst performance.\nSpatial features (Model 3) and interaction terms (Model 4) boosted model performance."
  },
  {
    "objectID": "assignments/assignment_3/MUSA-5080-Midterm/midterm_presentation.html#top-predictors",
    "href": "assignments/assignment_3/MUSA-5080-Midterm/midterm_presentation.html#top-predictors",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Top Predictors",
    "text": "Top Predictors\n\nIn a wealthy neighborhood (\\(\\beta\\) = 50,306.450, p &lt; 0.01)\nNumber of bedrooms (\\(\\beta\\) = 33,544.290, p &lt; 0.01)\nNumber of bathrooms (\\(\\beta\\) = 29,510.940, p &lt; 0.01)\n\nInterpretation: Higher home values in wealthier neighborhoods make sense, especially historically affluent areas with reputational appeal and historic housing stock. Higher numbers of bedrooms and bathrooms tend to indicate more total livable area, which aligns with our observation that total livable area impacts sale price."
  },
  {
    "objectID": "assignments/assignment_3/MUSA-5080-Midterm/midterm_presentation.html#model-performance",
    "href": "assignments/assignment_3/MUSA-5080-Midterm/midterm_presentation.html#model-performance",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Model Performance",
    "text": "Model Performance"
  },
  {
    "objectID": "assignments/assignment_3/MUSA-5080-Midterm/midterm_presentation.html#model-performance-by-neighborhood-part-1",
    "href": "assignments/assignment_3/MUSA-5080-Midterm/midterm_presentation.html#model-performance-by-neighborhood-part-1",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Model Performance by Neighborhood (Part 1)",
    "text": "Model Performance by Neighborhood (Part 1)\n\nNeighborhoods in North Philadelphia and West Philadelphia (with the exception of University City), in addition to Chinatown are underpredicted.\nCenter City and parts of Northeast and Northwest Philadelphia are overpredicted."
  },
  {
    "objectID": "assignments/assignment_3/MUSA-5080-Midterm/midterm_presentation.html#model-performance-by-neighborhood-part-2",
    "href": "assignments/assignment_3/MUSA-5080-Midterm/midterm_presentation.html#model-performance-by-neighborhood-part-2",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Model Performance by Neighborhood (Part 2)",
    "text": "Model Performance by Neighborhood (Part 2)\n\nThis suggests local factors not included in the model are having an impact on sale prices.\nA particular area of concern is the underpredicting of sale prices in lower-income neighborhoods in North Philadelphia."
  },
  {
    "objectID": "assignments/assignment_3/MUSA-5080-Midterm/midterm_presentation.html#recommendations",
    "href": "assignments/assignment_3/MUSA-5080-Midterm/midterm_presentation.html#recommendations",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Recommendations",
    "text": "Recommendations\n\nPolicymakers should be aware that houses in lower-income neighborhoods will not be accurately predicted by this model.\nPhysical features of a structure such as the number of bedrooms/bathrooms serve as strong predictors of home sale price. Nearby attractive amenities such as proximity to transit stations as well as distance from crime also contribute."
  },
  {
    "objectID": "assignments/assignment_3/MUSA-5080-Midterm/midterm_presentation.html#limitations-next-steps",
    "href": "assignments/assignment_3/MUSA-5080-Midterm/midterm_presentation.html#limitations-next-steps",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Limitations & Next Steps",
    "text": "Limitations & Next Steps\n\n\nLimitations\n\nPhiladelphia sale prices don’t have a linear relationship, particularly among lower-priced homes\nVariables used were aggregated and not weighted\n\n\nNext Steps\n\nUpdate the model to account for lower neighborhoods and tailor spatial features to add texture and depth to the model\nConsider more layered data cleaning, given that we minimally cleaned the data in order to preserve complex property types (mixed-use etc.)"
  },
  {
    "objectID": "assignments/assignment_3/MUSA-5080-Midterm/midterm_presentation.html#questions",
    "href": "assignments/assignment_3/MUSA-5080-Midterm/midterm_presentation.html#questions",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Questions?",
    "text": "Questions?\nThank you! from Sujan, Henry, Ryan, Kavana, Chloe, and Nina :)   Contact us at: inquiries@tnc.com"
  },
  {
    "objectID": "assignments/assignment_3/MUSA-5080-Midterm/midterm_presentation.html#who-we-are",
    "href": "assignments/assignment_3/MUSA-5080-Midterm/midterm_presentation.html#who-we-are",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Who We Are",
    "text": "Who We Are"
  }
]