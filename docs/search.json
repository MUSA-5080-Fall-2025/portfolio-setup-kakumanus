[
  {
    "objectID": "instructions_week1.html",
    "href": "instructions_week1.html",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Welcome to MUSA 5080! This guide will help you set up your personal portfolio repository for the semester.\n\n\nBy the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey\n\n\n\n\nThis is what you are building: Dr. Delmelle’s sample portfolio\n\n\n\n\nBefore starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed\n\n\n\n\n\nYou should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL\n\n\n\n\n\nIf you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!\n\n\n\n\nEach week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes\n\n\n\n\n\n\nWait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version\n\n\n\n\n\n\nCommit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis\n\n\n\n\n\nQuarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial\n\n\n\n\nDuring Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too\n\n\n\nBefore submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "instructions_week1.html#what-youre-building",
    "href": "instructions_week1.html#what-youre-building",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "By the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey"
  },
  {
    "objectID": "instructions_week1.html#example",
    "href": "instructions_week1.html#example",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "This is what you are building: Dr. Delmelle’s sample portfolio"
  },
  {
    "objectID": "instructions_week1.html#prerequisites",
    "href": "instructions_week1.html#prerequisites",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed"
  },
  {
    "objectID": "instructions_week1.html#step-by-step-setup",
    "href": "instructions_week1.html#step-by-step-setup",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "You should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL"
  },
  {
    "objectID": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "href": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "If you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!"
  },
  {
    "objectID": "instructions_week1.html#weekly-workflow",
    "href": "instructions_week1.html#weekly-workflow",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Each week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes"
  },
  {
    "objectID": "instructions_week1.html#troubleshooting",
    "href": "instructions_week1.html#troubleshooting",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Wait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version"
  },
  {
    "objectID": "instructions_week1.html#pro-tips",
    "href": "instructions_week1.html#pro-tips",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Commit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis"
  },
  {
    "objectID": "instructions_week1.html#additional-resources",
    "href": "instructions_week1.html#additional-resources",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Quarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial"
  },
  {
    "objectID": "instructions_week1.html#getting-help",
    "href": "instructions_week1.html#getting-help",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "During Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too"
  },
  {
    "objectID": "instructions_week1.html#checklist",
    "href": "instructions_week1.html#checklist",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html",
    "href": "weekly-notes/week-01-notes.html",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "An overview of Git + GitHub\nQuarto basics\nAn overview of R\ntidyverse and dplyr"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "An overview of Git + GitHub\nQuarto basics\nAn overview of R\ntidyverse and dplyr"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#coding-techniques",
    "href": "weekly-notes/week-01-notes.html#coding-techniques",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nWhy we use tidyverse (tibbles vs dataframes)\nA review of basic data manipulation using dplyr in R (select, filter, mutate, summarize, groupby)\nQuarto and how to generate good looking reports using markdown"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#questions-challenges",
    "href": "weekly-notes/week-01-notes.html#questions-challenges",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nNo major challenges this week."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#connections-to-policy",
    "href": "weekly-notes/week-01-notes.html#connections-to-policy",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nProf. Delmelle mentioned how the course, as opposed to the statistical analysis class, is focused on prediction.\nIn addition, public sector data science has very different goals than the private sector. Tradeoffs exist, and we’re not trying to simply optimize profit."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#reflection",
    "href": "weekly-notes/week-01-notes.html#reflection",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\nI hadn’t used Quarto so it was cool to see how the code could be deployed using GitHub Pages."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "This portfolio documents my learning journey in Public Policy Analytics (MUSA 5080).\n\n\nAdvanced spatial analysis and data science for urban planning and public policy.\n\n\n\n\nWeekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge\n\n\n\n\nI recently moved to Philadelphia to pursue my Master of Urban Spatial Analytics. Prior to this, I spent most of my life in Columbus, OH where I graduated with a Computer Science & Engineering degree from The Ohio State University, and spent time as a software engineer for JPMorgan Chase. My goal by pursuing this masters program is to shift my career into being able to do spatial data science for public benefit, particularly around housing and transportation access issues.\nOutside of my academic and professional pursuits, I love cooking and trying new restaurants, exploring both urban and natural spaces, and spending time with friends/family.\n\n\n\n\nEmail: kakumanu@upenn.edu\nGitHub: @kakumanus"
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Advanced spatial analysis and data science for urban planning and public policy."
  },
  {
    "objectID": "index.html#portfolio-structure",
    "href": "index.html#portfolio-structure",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Weekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "I recently moved to Philadelphia to pursue my Master of Urban Spatial Analytics. Prior to this, I spent most of my life in Columbus, OH where I graduated with a Computer Science & Engineering degree from The Ohio State University, and spent time as a software engineer for JPMorgan Chase. My goal by pursuing this masters program is to shift my career into being able to do spatial data science for public benefit, particularly around housing and transportation access issues.\nOutside of my academic and professional pursuits, I love cooking and trying new restaurants, exploring both urban and natural spaces, and spending time with friends/family."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Email: kakumanu@upenn.edu\nGitHub: @kakumanus"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#coding-techniques",
    "href": "weekly-notes/week-02-notes.html#coding-techniques",
    "title": "Week 2 Notes",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\ntidycensus\n\nRather than downloading csv files from the Census website, we will use the tidycensus R package to programatically access this data! Allows us to get the latest data and with automatic geographic boundaries.\nTables contain estimates (recall, ACS is based on a sample) and MOEs\nAlways report MOEs\n\nTIGER/Line Files\n\nShapefiles (geographic boundaries) for census tracts, counties, states.\n\nHistorical Data Sources\n\nNHGIS, Longitudinal Tract DB\nWe need this because boundaries change!"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#questions-challenges",
    "href": "weekly-notes/week-02-notes.html#questions-challenges",
    "title": "Week 2 Notes",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nN/A"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#connections-to-policy",
    "href": "weekly-notes/week-02-notes.html#connections-to-policy",
    "title": "Week 2 Notes",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nNoted above."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#reflection",
    "href": "weekly-notes/week-02-notes.html#reflection",
    "title": "Week 2 Notes",
    "section": "Reflection",
    "text": "Reflection\n\nN/A"
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html",
    "href": "labs/lab0/scripts/lab0_template.html",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "",
    "text": "Welcome to your first lab! In this (not graded) assignment, you’ll practice the fundamental dplyr operations I overviewed in class using car sales data. This lab will help you get comfortable with:\n\nBasic data exploration\nColumn selection and manipulation\n\nCreating new variables\nFiltering data\nGrouping and summarizing\n\nInstructions: Copy this template into your portfolio repository under a lab_0/ folder, then complete each section with your code and answers. You will write the code under the comment section in each chunk. Be sure to also copy the data folder into your lab_0 folder."
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#data-structure-exploration",
    "href": "labs/lab0/scripts/lab0_template.html#data-structure-exploration",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "1.1 Data Structure Exploration",
    "text": "1.1 Data Structure Exploration\nExplore the structure of your data and answer these questions:\n\n# Use glimpse() to see the data structure\n\n\n# Check the column names\n\n\n# Look at the first few rows\nhead(car_data)\n\n# A tibble: 6 × 7\n  Manufacturer Model     `Engine size` `Fuel type` `Year of manufacture` Mileage\n  &lt;chr&gt;        &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n1 Ford         Fiesta              1   Petrol                       2002  127300\n2 Porsche      718 Caym…           4   Petrol                       2016   57850\n3 Ford         Mondeo              1.6 Diesel                       2014   39190\n4 Toyota       RAV4                1.8 Hybrid                       1988  210814\n5 VW           Polo                1   Petrol                       2006  127869\n6 Ford         Focus               1.4 Petrol                       2018   33603\n# ℹ 1 more variable: Price &lt;dbl&gt;\n\n\nQuestions to answer: - How many rows and columns does the dataset have? - What types of variables do you see (numeric, character, etc.)? - Are there any column names that might cause problems? Why?\nYour answers: - Rows: 50000 - Columns: 7\n- Variable types: Strings and Numbers - Problematic names: Year of manufacture is stored as a number rather than in a date format (date would be easier to do time based calculations on). Also, the column name is way too long."
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#tibble-vs-data-frame",
    "href": "labs/lab0/scripts/lab0_template.html#tibble-vs-data-frame",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "1.2 Tibble vs Data Frame",
    "text": "1.2 Tibble vs Data Frame\nCompare how tibbles and data frames display:\n\n# Look at the tibble version (what we have)\ncar_data\n\n# A tibble: 50,000 × 7\n   Manufacturer Model    `Engine size` `Fuel type` `Year of manufacture` Mileage\n   &lt;chr&gt;        &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n 1 Ford         Fiesta             1   Petrol                       2002  127300\n 2 Porsche      718 Cay…           4   Petrol                       2016   57850\n 3 Ford         Mondeo             1.6 Diesel                       2014   39190\n 4 Toyota       RAV4               1.8 Hybrid                       1988  210814\n 5 VW           Polo               1   Petrol                       2006  127869\n 6 Ford         Focus              1.4 Petrol                       2018   33603\n 7 Ford         Mondeo             1.8 Diesel                       2010   86686\n 8 Toyota       Prius              1.4 Hybrid                       2015   30663\n 9 VW           Polo               1.2 Petrol                       2012   73470\n10 Ford         Focus              2   Diesel                       1992  262514\n# ℹ 49,990 more rows\n# ℹ 1 more variable: Price &lt;dbl&gt;\n\n# Convert to regular data frame and display\ncar_df &lt;- as.data.frame(car_data)\nhead(car_df)\n\n  Manufacturer      Model Engine size Fuel type Year of manufacture Mileage\n1         Ford     Fiesta         1.0    Petrol                2002  127300\n2      Porsche 718 Cayman         4.0    Petrol                2016   57850\n3         Ford     Mondeo         1.6    Diesel                2014   39190\n4       Toyota       RAV4         1.8    Hybrid                1988  210814\n5           VW       Polo         1.0    Petrol                2006  127869\n6         Ford      Focus         1.4    Petrol                2018   33603\n  Price\n1  3074\n2 49704\n3 24072\n4  1705\n5  4101\n6 29204\n\n\nQuestion: What differences do you notice in how they print?\nYour answer: In the rendered q markdown view, the tibble is much cleaner. It prints the size of the tibble, but only shows 10 rows and however many columns fit properly in the view. The dataframe prints the all 5000 rows! I’ve edited the code to only print the head() to avoid this."
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#selecting-columns",
    "href": "labs/lab0/scripts/lab0_template.html#selecting-columns",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "2.1 Selecting Columns",
    "text": "2.1 Selecting Columns\nPractice selecting different combinations of columns:\n\n# Select just Model and Mileage columns\nmodel_mileage &lt;- car_data %&gt;%\n  select(Model, Mileage)\n\n\n# Select Manufacturer, Price, and Fuel type\nmanufacturer_price_fuel &lt;- car_data %&gt;%\n  select(Manufacturer, Price, `Fuel type`)\n\n\n# Challenge: Select all columns EXCEPT Engine Size\ncar_data_sans_engine &lt;- car_data %&gt;%\n  select(!`Engine size`)"
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#renaming-columns",
    "href": "labs/lab0/scripts/lab0_template.html#renaming-columns",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "2.2 Renaming Columns",
    "text": "2.2 Renaming Columns\nLet’s fix a problematic column name:\n\n# Rename 'Year of manufacture' to year\ncar_data &lt;- car_data %&gt;%\n  rename(year = `Year of manufacture`)\n\n\n# Check that it worked\nnames(car_data)\n\n[1] \"Manufacturer\" \"Model\"        \"Engine size\"  \"Fuel type\"    \"year\"        \n[6] \"Mileage\"      \"Price\"       \n\n\nQuestion: Why did we need back-ticks around Year of manufacture but not around year?\nYour answer: There are spaces in the column name, so the backticks help R understand what the full name."
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#calculate-car-age",
    "href": "labs/lab0/scripts/lab0_template.html#calculate-car-age",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "3.1 Calculate Car Age",
    "text": "3.1 Calculate Car Age\n\n# Create an 'age' column (2025 minus year of manufacture)\ncar_data &lt;- car_data %&gt;%\n  mutate(age = 2025-year)\n\n# Create a mileage_per_year column  \ncar_data &lt;- car_data %&gt;%\n  mutate(mileage_per_year = Mileage/age)\n\n# Look at your new columns\n#select(car_data, Model, year, age, Mileage, mileage_per_year)\ncar_data %&gt;%\n  select(Model, year, age, Mileage, mileage_per_year)\n\n# A tibble: 50,000 × 5\n   Model       year   age Mileage mileage_per_year\n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;\n 1 Fiesta      2002    23  127300            5535.\n 2 718 Cayman  2016     9   57850            6428.\n 3 Mondeo      2014    11   39190            3563.\n 4 RAV4        1988    37  210814            5698.\n 5 Polo        2006    19  127869            6730.\n 6 Focus       2018     7   33603            4800.\n 7 Mondeo      2010    15   86686            5779.\n 8 Prius       2015    10   30663            3066.\n 9 Polo        2012    13   73470            5652.\n10 Focus       1992    33  262514            7955.\n# ℹ 49,990 more rows"
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#categorize-cars",
    "href": "labs/lab0/scripts/lab0_template.html#categorize-cars",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "3.2 Categorize Cars",
    "text": "3.2 Categorize Cars\n\n# Create a price_category column where if price is &lt; 15000, it is coded as budget, between 15000 and 30000 is midrange and greater than 30000 is luxury (use case_when)\ncar_data &lt;- car_data %&gt;%\n  mutate(price_category = case_when(\n    Price &gt; 30000 ~ \"luxury\",\n    Price &gt;= 15000 & Price &lt;= 30000 ~ \"midrange\",\n    .default = \"budget\"\n  ))\n\n\n# Check your categories select the new column and show it\ncar_data %&gt;%\n  select(Manufacturer, Model, price_category)\n\n# A tibble: 50,000 × 3\n   Manufacturer Model      price_category\n   &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;         \n 1 Ford         Fiesta     budget        \n 2 Porsche      718 Cayman luxury        \n 3 Ford         Mondeo     midrange      \n 4 Toyota       RAV4       budget        \n 5 VW           Polo       budget        \n 6 Ford         Focus      midrange      \n 7 Ford         Mondeo     budget        \n 8 Toyota       Prius      luxury        \n 9 VW           Polo       budget        \n10 Ford         Focus      budget        \n# ℹ 49,990 more rows"
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#basic-filtering",
    "href": "labs/lab0/scripts/lab0_template.html#basic-filtering",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "4.1 Basic Filtering",
    "text": "4.1 Basic Filtering\n\n# Find all Toyota cars\ncar_data %&gt;%\n  filter(Manufacturer == \"Toyota\")\n\n# A tibble: 12,554 × 10\n   Manufacturer Model `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Toyota       RAV4            1.8 Hybrid       1988  210814  1705    37\n 2 Toyota       Prius           1.4 Hybrid       2015   30663 30297    10\n 3 Toyota       RAV4            2.2 Petrol       2007   79393 16026    18\n 4 Toyota       Yaris           1.4 Petrol       1998   97286  4046    27\n 5 Toyota       RAV4            2.4 Hybrid       2003  117425 11667    22\n 6 Toyota       Yaris           1.2 Petrol       1992  245990   720    33\n 7 Toyota       RAV4            2   Hybrid       2018   28381 52671     7\n 8 Toyota       Prius           1   Hybrid       2003  115291  6512    22\n 9 Toyota       Prius           1   Hybrid       1990  238571   961    35\n10 Toyota       Prius           1.8 Hybrid       2017   31958 38961     8\n# ℹ 12,544 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find cars with mileage less than 30,000\ncar_data %&gt;%\n  filter(Mileage &lt; 30000)\n\n# A tibble: 5,402 × 10\n   Manufacturer Model      `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Toyota       RAV4                 2   Hybrid       2018   28381 52671     7\n 2 VW           Golf                 2   Petrol       2020   18985 36387     5\n 3 BMW          M5                   4   Petrol       2017   22759 97758     8\n 4 Toyota       RAV4                 2.4 Petrol       2018   24588 49125     7\n 5 VW           Golf                 2   Hybrid       2018   25017 36957     7\n 6 Porsche      718 Cayman           2.4 Petrol       2021   14070 69526     4\n 7 Ford         Focus                1.8 Petrol       2020   22371 40336     5\n 8 Ford         Mondeo               1.6 Diesel       2015   21834 28435    10\n 9 VW           Passat               1.6 Diesel       2018   22122 36634     7\n10 VW           Passat               1.4 Diesel       2020   21413 39310     5\n# ℹ 5,392 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find luxury cars (from price category) with low mileage\ncar_data %&gt;%\n  filter(Mileage &lt; 30000 & price_category == \"luxury\")\n\n# A tibble: 3,256 × 10\n   Manufacturer Model      `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Toyota       RAV4                 2   Hybrid       2018   28381 52671     7\n 2 VW           Golf                 2   Petrol       2020   18985 36387     5\n 3 BMW          M5                   4   Petrol       2017   22759 97758     8\n 4 Toyota       RAV4                 2.4 Petrol       2018   24588 49125     7\n 5 VW           Golf                 2   Hybrid       2018   25017 36957     7\n 6 Porsche      718 Cayman           2.4 Petrol       2021   14070 69526     4\n 7 Ford         Focus                1.8 Petrol       2020   22371 40336     5\n 8 VW           Passat               1.6 Diesel       2018   22122 36634     7\n 9 VW           Passat               1.4 Diesel       2020   21413 39310     5\n10 Toyota       RAV4                 2.4 Petrol       2021    6829 66031     4\n# ℹ 3,246 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;"
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#multiple-conditions",
    "href": "labs/lab0/scripts/lab0_template.html#multiple-conditions",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "4.2 Multiple Conditions",
    "text": "4.2 Multiple Conditions\n\n# Find cars that are EITHER Honda OR Nissan\n# Note: I used %in% to avoid writing Manufacturer twice.\ncar_data %&gt;%\n  filter(Manufacturer %in% c(\"Honda\", \"Nissan\"))\n\n# A tibble: 0 × 10\n# ℹ 10 variables: Manufacturer &lt;chr&gt;, Model &lt;chr&gt;, Engine size &lt;dbl&gt;,\n#   Fuel type &lt;chr&gt;, year &lt;dbl&gt;, Mileage &lt;dbl&gt;, Price &lt;dbl&gt;, age &lt;dbl&gt;,\n#   mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find cars with price between $20,000 and $35,000\n# Note: I used between() to have a more concise query.\ncar_data %&gt;%\n  filter(between(Price, 20000, 35000 ))\n\n# A tibble: 7,301 × 10\n   Manufacturer Model  `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Ford         Mondeo           1.6 Diesel       2014   39190 24072    11\n 2 Ford         Focus            1.4 Petrol       2018   33603 29204     7\n 3 Toyota       Prius            1.4 Hybrid       2015   30663 30297    10\n 4 Toyota       Prius            1.4 Hybrid       2016   43893 29946     9\n 5 Toyota       Prius            1.4 Hybrid       2016   43130 30085     9\n 6 VW           Passat           1.6 Petrol       2016   64344 23641     9\n 7 Ford         Mondeo           1.6 Diesel       2015   21834 28435    10\n 8 BMW          M5               4.4 Petrol       2008  109941 31711    17\n 9 BMW          Z4               2.2 Petrol       2014   61332 26084    11\n10 Porsche      911              3.5 Petrol       2003  107705 24378    22\n# ℹ 7,291 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find diesel cars less than 10 years old\ncar_data %&gt;%\n  filter (age &lt; 10 & `Fuel type` == \"Diesel\")\n\n# A tibble: 2,040 × 10\n   Manufacturer Model   `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Ford         Fiesta            1   Diesel       2017   38370 16257     8\n 2 VW           Passat            1.6 Diesel       2018   22122 36634     7\n 3 VW           Passat            1.4 Diesel       2020   21413 39310     5\n 4 BMW          X3                2   Diesel       2018   27389 44018     7\n 5 Ford         Mondeo            2   Diesel       2016   51724 28482     9\n 6 Porsche      Cayenne           2.6 Diesel       2019   20147 76182     6\n 7 VW           Polo              1.2 Diesel       2018   37411 19649     7\n 8 Ford         Mondeo            1.8 Diesel       2016   29439 30886     9\n 9 Ford         Mondeo            1.4 Diesel       2020   18929 37720     5\n10 Ford         Mondeo            1.4 Diesel       2018   42017 28904     7\n# ℹ 2,030 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n\nQuestion: How many diesel cars are less than 10 years old?\nYour answer: 2040"
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#basic-summaries",
    "href": "labs/lab0/scripts/lab0_template.html#basic-summaries",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "5.1 Basic Summaries",
    "text": "5.1 Basic Summaries\n\n# Calculate average price by manufacturer\navg_price_by_brand &lt;- car_data %&gt;%\n  group_by(Manufacturer) %&gt;%\n  summarize(avg_price = mean(Price, na.rm = TRUE))\n\navg_price_by_brand\n\n# A tibble: 5 × 2\n  Manufacturer avg_price\n  &lt;chr&gt;            &lt;dbl&gt;\n1 BMW             24429.\n2 Ford            10672.\n3 Porsche         29104.\n4 Toyota          14340.\n5 VW              10363.\n\n# Calculate average mileage by fuel type\navg_price_by_fuel_type &lt;- car_data %&gt;%\n  group_by(`Fuel type`) %&gt;%\n  summarize(avg_price = mean(Price, na.rm = TRUE))\n\navg_price_by_fuel_type\n\n# A tibble: 3 × 2\n  `Fuel type` avg_price\n  &lt;chr&gt;           &lt;dbl&gt;\n1 Diesel         13145.\n2 Hybrid         14949.\n3 Petrol         13691.\n\n# Count cars by manufacturer\ncount_by_manufacturer &lt;- car_data %&gt;%\n  group_by(Manufacturer) %&gt;%\n  summarize(\"Count\" = n())\n\ncount_by_manufacturer\n\n# A tibble: 5 × 2\n  Manufacturer Count\n  &lt;chr&gt;        &lt;int&gt;\n1 BMW           4965\n2 Ford         14959\n3 Porsche       2609\n4 Toyota       12554\n5 VW           14913"
  },
  {
    "objectID": "labs/lab0/scripts/lab0_template.html#categorical-summaries",
    "href": "labs/lab0/scripts/lab0_template.html#categorical-summaries",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "5.2 Categorical Summaries",
    "text": "5.2 Categorical Summaries\n\n# Frequency table for price categories"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html",
    "href": "weekly-notes/week-02-notes.html",
    "title": "Week 2 Notes",
    "section": "",
    "text": "Algorithms\n\nHigh level, a set of rules or instructions for solving a problem or completing a task.\n\nAlgorithmic Decision Making in the Government\n\nCan be used to assist or replace human decision makers (mitigating biases that may exist in a human only process).\nCan use historical data providing Inputs(“features”, “predictors”, “independent variables”) to determine Outputs(“labels”, “outcome”, “dependent variables”).\nData has trade-offs! Data can be inaccurate, subject to biases itself, etc.\n\nData collection is a long running practice in gov’t (civic registration, census, admin records, operations research)\n\nNow, there is an increase in official and accidental (things that can be collected from sources like social media ) data.\nThere is a shift from historical analysis to prediction.\n\nIMPORTANT: Data Analytics is Subjective!\n\nData cleaning, coding/classification, collection, interpretation, model variables. All require human choices that embody human values and biases.\nEspecially data proxies/variables that can rely on historically biased data.\n\nCensus and ACS Data:\n\nFoundational in understanding community demographics, allocating govt resources, etc.\nCensus is 10 years and sent to everyone, 9 basic questions, constitutional requirement, determines political representation.\nAmerican Community Survey (ACS) sent to ~3% of households annually, with more detailed questions (income, education, employment, housing costs). Done at Block Group level.\n\nACS has 1-year estimates (areas &gt; 65k people) and 5-year estimates (all areas with census tracts). 5-year estimates are the most reliable and based on the largest sample.\n\nHierarchy of Census Data:\nNation\n├── Regions  \n├── States\n│   ├── Counties\n│   │   ├── Census Tracts (1,500-8,000 people)\n│   │   │   ├── Block Groups (600-3,000 people)  \n│   │   │   │   └── Blocks (≈85 people, Decennial only)\nMost policy analysis happens at County, Census, Block (although Blocks have big margins of error (MOE))\n\nCensus Data in R (see below)"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "Algorithms: High level, a set of rules or instructions for solving a problem or completing a task.\nAlgorithmic Decision Making in the Government\n\nTest"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#key-concepts-learned-notes",
    "href": "weekly-notes/week-02-notes.html#key-concepts-learned-notes",
    "title": "Week 2 Notes",
    "section": "",
    "text": "Algorithms\n\nHigh level, a set of rules or instructions for solving a problem or completing a task.\n\nAlgorithmic Decision Making in the Government\n\nCan be used to assist or replace human decision makers (mitigating biases that may exist in a human only process).\nCan use historical data providing Inputs(“features”, “predictors”, “independent variables”) to determine Outputs(“labels”, “outcome”, “dependent variables”).\nData has trade-offs! Data can be inaccurate, subject to biases itself, etc.\n\nData collection is a long running practice in gov’t (civic registration, census, admin records, operations research)\n\nNow, there is an increase in official and accidental (things that can be collected from sources like social media ) data.\nThere is a shift from historical analysis to prediction.\n\nIMPORTANT: Data Analytics is Subjective!\n\nData cleaning, coding/classification, collection, interpretation, model variables. All require human choices that embody human values and biases.\nEspecially data proxies/variables that can rely on historically biased data.\n\nCensus and ACS Data:\n\nFoundational in understanding community demographics, allocating govt resources, etc.\nCensus is 10 years and sent to everyone, 9 basic questions, constitutional requirement, determines political representation.\nAmerican Community Survey (ACS) sent to ~3% of households annually, with more detailed questions (income, education, employment, housing costs). Done at Block Group level.\n\nACS has 1-year estimates (areas &gt; 65k people) and 5-year estimates (all areas with census tracts). 5-year estimates are the most reliable and based on the largest sample.\n\nHierarchy of Census Data:\nNation\n├── Regions  \n├── States\n│   ├── Counties\n│   │   ├── Census Tracts (1,500-8,000 people)\n│   │   │   ├── Block Groups (600-3,000 people)  \n│   │   │   │   └── Blocks (≈85 people, Decennial only)\nMost policy analysis happens at County, Census, Block (although Blocks have big margins of error (MOE))\n\nCensus Data in R (see below)"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#coding-techniquestechnical-notes",
    "href": "weekly-notes/week-02-notes.html#coding-techniquestechnical-notes",
    "title": "Week 2 Notes",
    "section": "Coding Techniques/Technical Notes",
    "text": "Coding Techniques/Technical Notes\n\ntidycensus\n\nRather than downloading csv files from the Census website, we will use the tidycensus R package to programatically access this data! Allows us to get the latest data and with automatic geographic boundaries.\nTables contain estimates (recall, ACS is based on a sample) and MOEs\nAlways report MOEs\n\nTIGER/Line Files\n\nShapefiles (geographic boundaries) for census tracts, counties, states.\n\nHistorical Data Sources\n\nNHGIS, Longitudinal Tract DB\nWe need this because boundaries change!"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html",
    "href": "weekly-notes/week-03-notes.html",
    "title": "Week 3 Notes",
    "section": "",
    "text": "Importance of Data Visualization and Communication\n\nExample of Anscombe’s Quartet: Charts with the same summary statistics, but the way the data is represented would suggest otherwise.\nPoor policy decisions can result from misunderstood data. Recall the Jurjevich et al. study that found 72% of Portland Census tracts having unreliable child poverty estimates. This uncertainty was never communicated by planners.\nA note that block groups, since they’re smaller, can have high margins of error! Imagine (an example) just two people getting surveyed in a block group.\n\nGrammar of Graphics Principles (ggplot2 philosophy)\n\nData\nAesthetics: aes(); connection between data variables and visual properties of the plot\nGeometries: geom_points(); how to display the data - points, lines, etc.\nVisual: Adding scales, themes, facets, annotations, etc.\n\nExploratory Data Analysis (EDA)\n\nWhat does the data looking like? (distributions, missing values)\nWhat patterns exists? (relationships, clusters, trends)\nWhat’s unusual? (outliers, anomalies, data quality issues)\nWhat questions does this raise? (hypotheses)\nReliability and data quality?"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#key-concepts-learned-notes",
    "href": "weekly-notes/week-03-notes.html#key-concepts-learned-notes",
    "title": "Week 3 Notes",
    "section": "",
    "text": "Importance of Data Visualization and Communication\n\nExample of Anscombe’s Quartet: Charts with the same summary statistics, but the way the data is represented would suggest otherwise.\nPoor policy decisions can result from misunderstood data. Recall the Jurjevich et al. study that found 72% of Portland Census tracts having unreliable child poverty estimates. This uncertainty was never communicated by planners.\nA note that block groups, since they’re smaller, can have high margins of error! Imagine (an example) just two people getting surveyed in a block group.\n\nGrammar of Graphics Principles (ggplot2 philosophy)\n\nData\nAesthetics: aes(); connection between data variables and visual properties of the plot\nGeometries: geom_points(); how to display the data - points, lines, etc.\nVisual: Adding scales, themes, facets, annotations, etc.\n\nExploratory Data Analysis (EDA)\n\nWhat does the data looking like? (distributions, missing values)\nWhat patterns exists? (relationships, clusters, trends)\nWhat’s unusual? (outliers, anomalies, data quality issues)\nWhat questions does this raise? (hypotheses)\nReliability and data quality?"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#coding-techniquestechnical-notes",
    "href": "weekly-notes/week-03-notes.html#coding-techniquestechnical-notes",
    "title": "Week 3 Notes",
    "section": "Coding Techniques/Technical Notes",
    "text": "Coding Techniques/Technical Notes\n\nggplot2\n\nggplot(data = yourdata) + aes(x = variable1, y = variable2) + geom_something() + additional_layers()\nAgain, aesthetics are used to map data to visual properties. You shouldn’t be setting themes here (for example), but if you want the color to change based on a variable, you can do it here!"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#questions-challenges",
    "href": "weekly-notes/week-03-notes.html#questions-challenges",
    "title": "Week 3 Notes",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nN/A"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#connections-to-policy",
    "href": "weekly-notes/week-03-notes.html#connections-to-policy",
    "title": "Week 3 Notes",
    "section": "## Connections to Policy",
    "text": "## Connections to Policy"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#reflection",
    "href": "weekly-notes/week-03-notes.html#reflection",
    "title": "Week 3 Notes",
    "section": "## Reflection",
    "text": "## Reflection"
  },
  {
    "objectID": "labs/lab1/assignment1.html",
    "href": "labs/lab1/assignment1.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Ohio Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders\n\n\n\n\nSubmit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "labs/lab1/assignment1.html#scenario",
    "href": "labs/lab1/assignment1.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Ohio Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "labs/lab1/assignment1.html#learning-objectives",
    "href": "labs/lab1/assignment1.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "labs/lab1/assignment1.html#submission-instructions",
    "href": "labs/lab1/assignment1.html#submission-instructions",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Submit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "labs/lab1/assignment1.html#data-retrieval",
    "href": "labs/lab1/assignment1.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\n\n# Clean the county names to remove state name and \"County\" \n# Hint: use mutate() with str_remove()\n\n# Display the first few rows"
  },
  {
    "objectID": "labs/lab1/assignment1.html#data-quality-assessment",
    "href": "labs/lab1/assignment1.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Calculate MOE percentage and reliability categories using mutate()\n\n# Create a summary showing count of counties in each reliability category\n# Hint: use count() and mutate() to add percentages"
  },
  {
    "objectID": "labs/lab1/assignment1.html#high-uncertainty-counties",
    "href": "labs/lab1/assignment1.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\n\n# Format as table with kable() - include appropriate column names and caption\n\nData Quality Commentary:\n[Write 2-3 sentences explaining what these results mean for algorithmic decision-making. Consider: Which counties might be poorly served by algorithms that rely on this income data? What factors might contribute to higher uncertainty?]"
  },
  {
    "objectID": "labs/lab1/assignment1.html#focus-area-selection",
    "href": "labs/lab1/assignment1.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\n\nComment on the output: [write something :)]"
  },
  {
    "objectID": "labs/lab1/assignment1.html#tract-level-demographics",
    "href": "labs/lab1/assignment1.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You’ll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\n\n# Use get_acs() to retrieve tract-level data\n# Hint: You may need to specify county codes in the county parameter\n\n# Calculate percentage of each group using mutate()\n# Create percentages for white, Black, and Hispanic populations\n\n# Add readable tract and county name columns using str_extract() or similar"
  },
  {
    "objectID": "labs/lab1/assignment1.html#demographic-analysis",
    "href": "labs/lab1/assignment1.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\n# Find the tract with the highest percentage of Hispanic/Latino residents\n# Hint: use arrange() and slice() to get the top tract\n\n# Calculate average demographics by county using group_by() and summarize()\n# Show: number of tracts, average percentage for each racial/ethnic group\n\n# Create a nicely formatted table of your results using kable()"
  },
  {
    "objectID": "labs/lab1/assignment1.html#moe-analysis-for-demographic-variables",
    "href": "labs/lab1/assignment1.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\n# Calculate MOE percentages for white, Black, and Hispanic variables\n# Hint: use the same formula as before (margin/estimate * 100)\n\n# Create a flag for tracts with high MOE on any demographic variable\n# Use logical operators (| for OR) in an ifelse() statement\n\n# Create summary statistics showing how many tracts have data quality issues"
  },
  {
    "objectID": "labs/lab1/assignment1.html#pattern-analysis",
    "href": "labs/lab1/assignment1.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\n\n# Use group_by() and summarize() to create this comparison\n# Create a professional table showing the patterns\n\nPattern Analysis: [Describe any patterns you observe. Do certain types of communities have less reliable data? What might explain this?]"
  },
  {
    "objectID": "labs/lab1/assignment1.html#analysis-integration-and-professional-summary",
    "href": "labs/lab1/assignment1.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements: 1. Overall Pattern Identification: What are the systematic patterns across all your analyses? 2. Equity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings? 3. Root Cause Analysis: What underlying factors drive both data quality issues and bias risk? 4. Strategic Recommendations: What should the Department implement to address these systematic issues?\nExecutive Summary:\n[Your integrated 4-paragraph summary here]"
  },
  {
    "objectID": "labs/lab1/assignment1.html#specific-recommendations",
    "href": "labs/lab1/assignment1.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\n\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\n\n# Format as a professional table with kable()\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nCounties suitable for immediate algorithmic implementation: [List counties with high confidence data and explain why they’re appropriate]\nCounties requiring additional oversight: [List counties with moderate confidence data and describe what kind of monitoring would be needed]\nCounties needing alternative approaches: [List counties with low confidence data and suggest specific alternatives - manual review, additional surveys, etc.]"
  },
  {
    "objectID": "labs/lab1/assignment1.html#questions-for-further-investigation",
    "href": "labs/lab1/assignment1.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n[List 2-3 questions that your analysis raised that you’d like to explore further in future assignments. Consider questions about spatial patterns, time trends, or other demographic factors.]"
  },
  {
    "objectID": "labs/lab1/assignment1.html#submission-checklist",
    "href": "labs/lab1/assignment1.html#submission-checklist",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\nAll code chunks run without errors\nAll “[Fill this in]” prompts have been completed\nTables are properly formatted and readable\nExecutive summary addresses all four required components\nPortfolio navigation includes this assignment\nCensus API key is properly set\nDocument renders correctly to HTML\n\nRemember: Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/your_file_name.html"
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html",
    "href": "assignments/assignment_1/assignment1.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Ohio Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders\n\n\n\n\nSubmit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#scenario",
    "href": "assignments/assignment_1/assignment1.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Ohio Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#learning-objectives",
    "href": "assignments/assignment_1/assignment1.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#submission-instructions",
    "href": "assignments/assignment_1/assignment1.html#submission-instructions",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Submit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#data-retrieval",
    "href": "assignments/assignment_1/assignment1.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\ncounty_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    median_income = \"B19013_001\",   # Median household income\n    total_pop = \"B01003_001\"       # Total population\n  ),\n  state = my_state,\n  year = 2022,\n  output = \"wide\"\n)\n\n# Clean the county names to remove state name and \"County\" \nwords_to_remove &lt;- c(\"County\", \"Ohio\", ', ', ' ')\npattern &lt;- paste0(\"\\\\b(\", paste(words_to_remove, collapse = \"|\"), \")\\\\b\")\n\ncounty_data &lt;- county_data %&gt;%\n  mutate(county_name = str_remove_all(NAME, pattern))\n\n# Display the first few rows\nglimpse(county_data)\n\nRows: 88\nColumns: 7\n$ GEOID          &lt;chr&gt; \"39001\", \"39003\", \"39005\", \"39007\", \"39009\", \"39011\", \"…\n$ NAME           &lt;chr&gt; \"Adams County, Ohio\", \"Allen County, Ohio\", \"Ashland Co…\n$ median_incomeE &lt;dbl&gt; 46234, 58976, 62254, 53663, 48750, 75231, 56943, 66677,…\n$ median_incomeM &lt;dbl&gt; 4072, 2172, 2954, 2186, 3695, 2795, 2417, 3077, 1604, 3…\n$ total_popE     &lt;dbl&gt; 27509, 102087, 52522, 97666, 61276, 46263, 66554, 43715…\n$ total_popM     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ county_name    &lt;chr&gt; \"Adams\", \"Allen\", \"Ashland\", \"Ashtabula\", \"Athens\", \"Au…"
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#data-quality-assessment",
    "href": "assignments/assignment_1/assignment1.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Calculate MOE percentage and reliability categories using mutate()\n# Calculate MOE percentages\ncounty_data &lt;- county_data %&gt;%\n  mutate(\n      median_income_moe_pct = (median_incomeM / median_incomeE) * 100,\n  )\n\ncounty_data &lt;- county_data %&gt;%\n  mutate(\n    median_income_moe_cat = case_when(\n      median_income_moe_pct &gt; 10 ~ \"Low Confidence: MOE &gt; 10%\",\n      median_income_moe_pct &gt;= 5 & median_income_moe_pct &lt;= 10 ~ \"Moderate Confidence: MOE 5-10%\",\n      TRUE ~ \"High Confidence: MOE &lt; 5%\"\n    ),\n  )\n\n# Create a summary showing count of counties in each reliability category\n# Hint: use count() and mutate() to add percentages\n\nmoe_cat_summary_table &lt;- county_data %&gt;%\n  count(median_income_moe_cat, name = \"number_of_counties\")\n\nmoe_cat_summary_table\n\n# A tibble: 3 × 2\n  median_income_moe_cat          number_of_counties\n  &lt;chr&gt;                                       &lt;int&gt;\n1 High Confidence: MOE &lt; 5%                      57\n2 Low Confidence: MOE &gt; 10%                       2\n3 Moderate Confidence: MOE 5-10%                 29"
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#high-uncertainty-counties",
    "href": "assignments/assignment_1/assignment1.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\ntop_moe_counties &lt;- county_data %&gt;%\n   slice_max(median_income_moe_pct, n = 5) %&gt;%\n    select(county_name, median_incomeE, median_incomeM, median_income_moe_pct, median_income_moe_cat)\n\n# Format as table with kable() - include appropriate column names and caption\nkable(\n  top_moe_counties,\n  caption = \"Top 5 Counties in Ohio by Median Income Margin of Error (MOE)\",\n  col.names = c(\"County Name\", \"Median Income Estimate $\", \"Median Income MOE $\", \"Median Income MOE %\", \"Median Income MOE Category\")\n)\n\n\nTop 5 Counties in Ohio by Median Income Margin of Error (MOE)\n\n\n\n\n\n\n\n\n\nCounty Name\nMedian Income Estimate $\nMedian Income MOE $\nMedian Income MOE %\nMedian Income MOE Category\n\n\n\n\nVinton\n50967\n6264\n12.290306\nLow Confidence: MOE &gt; 10%\n\n\nNoble\n51547\n5566\n10.797913\nLow Confidence: MOE &gt; 10%\n\n\nMonroe\n55381\n5415\n9.777722\nModerate Confidence: MOE 5-10%\n\n\nAdams\n46234\n4072\n8.807371\nModerate Confidence: MOE 5-10%\n\n\nPike\n50832\n4258\n8.376613\nModerate Confidence: MOE 5-10%\n\n\n\n\n\nData Quality Commentary:\nThese results are not surprising given that the counties are among the least populous in the state (Vinton, Noble, and Monroe have less than 15,000 people according to 2024 estimates). Since the ACS relies on sample data, rather than the total enumeration provided by the Census, small population groups will result in small sample sizes.\nIn this scenario: Any algorithmic system should account for the data inaccuracies arising from Ohio’s smaller counties."
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#focus-area-selection",
    "href": "assignments/assignment_1/assignment1.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\nselected_counties &lt;- county_data %&gt;%\n  filter(county_name %in% c(\"Cuyahoga\", \"Marion\", \"Vinton\"))\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\nkable(\n  caption = \"Selected Counties (1 in each reliability category)\",\n  selected_counties %&gt;% select(county_name, median_incomeE, median_income_moe_pct, median_income_moe_cat),\n  col.names = c(\"County Name\", \"Median Income Estimate $\", \"Median Income MOE %\", \"Median Income MOE Category\")\n)\n\n\nSelected Counties (1 in each reliability category)\n\n\n\n\n\n\n\n\nCounty Name\nMedian Income Estimate $\nMedian Income MOE %\nMedian Income MOE Category\n\n\n\n\nCuyahoga\n60074\n1.163565\nHigh Confidence: MOE &lt; 5%\n\n\nMarion\n55106\n8.313069\nModerate Confidence: MOE 5-10%\n\n\nVinton\n50967\n12.290306\nLow Confidence: MOE &gt; 10%\n\n\n\n\n\nComment on the output: Cuyahoga county is one of the most populous counties in Ohio, so the sample sizes used lent to its low MOE of ~1.16%. Although this can not be extrapolated to all counties, it is interesting to see a decline of median income as the MOE increases among the selected counties."
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#tract-level-demographics",
    "href": "assignments/assignment_1/assignment1.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You’ll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\nrace_vars &lt;- c(\n  white    = \"B03002_003\",\n  black    = \"B03002_004\",\n  hispanic = \"B03002_012\",\n  total    = \"B03002_001\"\n)\n\n# Use get_acs() to retrieve tract-level data\n# Hint: You may need to specify county codes in the county parameter\n\n# Based on https://www.census.gov/programs-surveys/geography/guidance/geo-identifiers.html,\n# GEOIDs for counties are made of two digit state codes and three digit county codes. This is visible\n# in how GEOID in county_data stays constant for the first two digits, and is unique for the last 3.\n# We can parse this and create a list:\nselected_counties &lt;- selected_counties %&gt;%\n  mutate(county_code = str_sub(GEOID, 3, 5))\n\ncounty_codes &lt;- selected_counties %&gt;% pull(county_code)\n\ntract_demo_selected_counties = get_acs(\n  geography = \"tract\",\n  state     = my_state,\n  county    = county_codes,\n  variables = race_vars,\n  output    = \"wide\"\n)\n\n# Calculate percentage of each group using mutate()\n# Create percentages for white, Black, and Hispanic populations\ntract_demo_selected_counties &lt;- tract_demo_selected_counties %&gt;%\n  mutate(\n    pct_white    = 100 * whiteE    / totalE,\n    pct_black    = 100 * blackE    / totalE,\n    pct_hispanic = 100 * hispanicE / totalE\n  )\n\n# Add readable tract and county name columns using str_extract() or similar\ntract_demo_selected_counties &lt;- tract_demo_selected_counties %&gt;%\n  mutate(\n    tract_name  = str_extract(NAME, \"^[^;]+\"),\n    county_name = str_extract(NAME, \"(?&lt;=;\\\\s).*\")\n  )\n\n# We can clean up the county name as we did before:\nwords_to_remove &lt;- c(\"County\", \"Ohio\", '; ', ' ')\npattern &lt;- paste0(\"\\\\b(\", paste(words_to_remove, collapse = \"|\"), \")\\\\b\")\ntract_demo_selected_counties &lt;- tract_demo_selected_counties %&gt;%\n  mutate(county_name = str_remove_all(county_name, pattern))"
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#demographic-analysis",
    "href": "assignments/assignment_1/assignment1.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\n# Find the tract with the highest percentage of Hispanic/Latino residents\n# Hint: use arrange() and slice() to get the top tract\ntop_hispanic &lt;- tract_demo_selected_counties %&gt;%\n  slice_max(order_by = pct_hispanic, n = 1)\n\ntop_hispanic %&gt;%\n  select(county_name, tract_name, pct_white, pct_black, pct_hispanic) %&gt;%\n  kable(\n    caption = \"Details of Census Tract with Highest Percentage Hispanic Residents\",\n    col.names = c(\"County Name\", \"Tract\", \"% White\", \"% Black\", \"% Hispanic\")\n  )\n\n\nDetails of Census Tract with Highest Percentage Hispanic Residents\n\n\nCounty Name\nTract\n% White\n% Black\n% Hispanic\n\n\n\n\nCuyahoga\nCensus Tract 1029\n18.89952\n19.13876\n61.48325\n\n\n\n\n# Calculate average demographics by county using group_by() and summarize()\n# Show: number of tracts, average percentage for each racial/ethnic group\n\nselected_counties_summary &lt;- tract_demo_selected_counties %&gt;%\n  group_by(county_name) %&gt;%\n  summarize(\n    n_of_tracts      = n(),\n    avg_pct_white = mean(pct_white,    na.rm = TRUE),\n    avg_pct_black = mean(pct_black,    na.rm = TRUE),\n    avg_pct_hisp  = mean(pct_hispanic, na.rm = TRUE),\n  )\n\n# Create a nicely formatted table of your results using kable()\nselected_counties_summary %&gt;%\n  kable(\n    caption = \"Average Demographics for Selected Ohio Counties\",\n    digits = 2,\n    col.names = c(\"County Name\", \"Number of Tracts\", \"Avg % White\", \"Avg % Black\", \"Avg % Hispanic\")\n  )\n\n\nAverage Demographics for Selected Ohio Counties\n\n\n\n\n\n\n\n\n\nCounty Name\nNumber of Tracts\nAvg % White\nAvg % Black\nAvg % Hispanic\n\n\n\n\nCuyahoga\n428\n51.12\n34.98\n6.77\n\n\nMarion\n18\n86.13\n4.75\n3.50\n\n\nVinton\n3\n96.42\n0.49\n0.07"
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#moe-analysis-for-demographic-variables",
    "href": "assignments/assignment_1/assignment1.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\n# Calculate MOE percentages for white, Black, and Hispanic variables\n# Hint: use the same formula as before (margin/estimate * 100)\n\n# I noticed some of the estimates or margins were 0s, or NaNs, so I'm accounting for that here as well:\ntract_demo_selected_counties &lt;- tract_demo_selected_counties %&gt;%\n  mutate(\n    white_moe_pct    = if_else(whiteE    &gt; 0, (whiteM    / whiteE), NA_real_) * 100,\n    black_moe_pct    = if_else(blackE    &gt; 0, (blackM    / blackE), NA_real_) * 100,\n    hispanic_moe_pct = if_else(hispanicE &gt; 0, (hispanicM / hispanicE), NA_real_) * 100\n  )\n\n# Create a flag for tracts with high MOE on any demographic variable\n# Use logical operators (| for OR) in an ifelse() statement\ntract_demo_selected_counties &lt;- tract_demo_selected_counties %&gt;%\n  mutate(\n    high_moe_flag = ifelse(\n      white_moe_pct &gt; 15 | black_moe_pct &gt; 15 | hispanic_moe_pct &gt; 15,\n      TRUE,\n      FALSE\n    )\n  )\n\n# Create summary statistics showing how many tracts have data quality issues\n\ntract_demo_selected_counties %&gt;%\n  group_by(county_name) %&gt;%\n  summarize(\n    total_tracts    = sum(!is.na(high_moe_flag), na.rm = TRUE),\n    high_moe_tracts = sum(high_moe_flag, na.rm = TRUE),\n  ) %&gt;%\n  kable(\n    caption = \"Tract Data Quality by County\",\n    digits = 0,\n    col.names = c(\"County Name\", \"Number of Tracts (excluding those without data)\", \"Tracts with Quality Issues (&gt;15% MOE)\")\n  )\n\n\nTract Data Quality by County\n\n\n\n\n\n\n\nCounty Name\nNumber of Tracts (excluding those without data)\nTracts with Quality Issues (&gt;15% MOE)\n\n\n\n\nCuyahoga\n421\n421\n\n\nMarion\n18\n18\n\n\nVinton\n2\n2\n\n\n\n\n\nI noticed that there were some tracts with no data, so the sum of tracts excludes those."
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#pattern-analysis",
    "href": "assignments/assignment_1/assignment1.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\n\n# Use group_by() and summarize() to create this comparison\n# Create a professional table showing the patterns\n\ntract_demo_selected_counties %&gt;%\n  group_by(high_moe_flag) %&gt;%\n  summarize(\n    avg_pct_white  = mean(pct_white, na.rm = TRUE),\n    avg_pct_black  = mean(pct_black, na.rm = TRUE),\n    avg_pct_hisp   = mean(pct_hispanic, na.rm = TRUE),\n    n_tracts       = n(),\n    .groups = \"drop\"\n  ) %&gt;%\n  kable(\n    caption = \"Average Demographics by High MOE Status\",\n    digits = 1,\n    col.names = c(\n      \"High MOE Flag\",\n      \"Avg % White\",\n      \"Avg % Black\",\n      \"Avg % Hispanic\",\n      \"Number of Tracts\"\n    )\n  )\n\n\nAverage Demographics by High MOE Status\n\n\n\n\n\n\n\n\n\nHigh MOE Flag\nAvg % White\nAvg % Black\nAvg % Hispanic\nNumber of Tracts\n\n\n\n\nTRUE\n52.6\n33.7\n6.6\n441\n\n\nNA\n98.9\n0.0\n0.0\n8\n\n\n\n\n\nThis summary was simple, but when I did a visual inspection of the data, I see there’s some patterns that this table analysis is obscuring. I see that the MOE for Hispanic residents is above 15% for all tracts! I’ll do a second summary table, this time showing\n\ntract_demo_selected_counties %&gt;%\n  summarize(\n    n_tracts          = n(),\n    white_high_moe    = sum(white_moe_pct    &gt; 15, na.rm = TRUE),\n    black_high_moe    = sum(black_moe_pct    &gt; 15, na.rm = TRUE),\n    hispanic_high_moe = sum(hispanic_moe_pct &gt; 15, na.rm = TRUE),\n    pct_white_high    = 100 * white_high_moe    / n_tracts,\n    pct_black_high    = 100 * black_high_moe    / n_tracts,\n    pct_hispanic_high = 100 * hispanic_high_moe / n_tracts\n  ) %&gt;%\n  kable(\n    caption = \"Tracts with MOE &gt; 15% by Demographic Group\",\n    digits = 1,\n    col.names = c(\n      \"Total Tracts\",\n      \"White High MOE Tracts\",\n      \"Black High MOE Tracts\",\n      \"Hispanic High MOE Tracts\",\n      \"% of Tracts with White High MOE\",\n      \"% of Tracts with Black High MOE\",\n      \"% of Tracts with Hispanic High MOE\"\n    )\n  )\n\n\nTracts with MOE &gt; 15% by Demographic Group\n\n\n\n\n\n\n\n\n\n\n\nTotal Tracts\nWhite High MOE Tracts\nBlack High MOE Tracts\nHispanic High MOE Tracts\n% of Tracts with White High MOE\n% of Tracts with Black High MOE\n% of Tracts with Hispanic High MOE\n\n\n\n\n449\n306\n416\n403\n68.2\n92.7\n89.8\n\n\n\n\n\n\ntract_demo_selected_counties %&gt;%\n  group_by(county_name) %&gt;%\n  summarize(\n    n_tracts          = n(),\n    white_high_moe    = sum(white_moe_pct    &gt; 15, na.rm = TRUE),\n    black_high_moe    = sum(black_moe_pct    &gt; 15, na.rm = TRUE),\n    hispanic_high_moe = sum(hispanic_moe_pct &gt; 15, na.rm = TRUE),\n    pct_white_high    = 100 * white_high_moe    / n_tracts,\n    pct_black_high    = 100 * black_high_moe    / n_tracts,\n    pct_hispanic_high = 100 * hispanic_high_moe / n_tracts,\n    .groups = \"drop\"\n  ) %&gt;%\n  kable(\n    caption = \"County-Level Breakdown of Tracts with MOE &gt; 15%, by Demographic Group\",\n    digits = 1,\n    col.names = c(\n      \"County\",\n      \"Total Tracts\",\n      \"White High MOE Tracts\",\n      \"Black High MOE Tracts\",\n      \"Hispanic High MOE Tracts\",\n      \"% of Tracts with White High MOE\",\n      \"% of Tracts with Black High MOE\",\n      \"% of Tracts with Hispanic High MOE\"\n    )\n  )\n\n\nCounty-Level Breakdown of Tracts with MOE &gt; 15%, by Demographic Group\n\n\n\n\n\n\n\n\n\n\n\n\nCounty\nTotal Tracts\nWhite High MOE Tracts\nBlack High MOE Tracts\nHispanic High MOE Tracts\n% of Tracts with White High MOE\n% of Tracts with Black High MOE\n% of Tracts with Hispanic High MOE\n\n\n\n\nCuyahoga\n428\n298\n402\n383\n69.6\n93.9\n89.5\n\n\nMarion\n18\n8\n12\n18\n44.4\n66.7\n100.0\n\n\nVinton\n3\n0\n2\n2\n0.0\n66.7\n66.7\n\n\n\n\n\nPattern Analysis: A larger share of census tracts flagged as High MOE (&gt;15%) are driven primarily by high median income margins of error among Black or Hispanic populations. The last chart highlights that, with both communities having a higher MOE as a percentage of tracts being marked.\nFor Marion and Vinton counties, this is potentially explained by Black and Hispanic populations being a smaller percentage of the total. This would make the sample sizes for both communities fairly small in these counties. In Cuyahoga county, a visual inspection reveals tracts being flagged when they have small percentages of Hispanic, Black, or White population."
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#analysis-integration-and-professional-summary",
    "href": "assignments/assignment_1/assignment1.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements:\n\nOverall Pattern Identification: What are the systematic patterns across all your analyses?\nEquity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings?\nRoot Cause Analysis: What underlying factors drive both data quality issues and bias risk?\nStrategic Recommendations: What should the Department implement to address these systematic issues?\n\nExecutive Summary:\nAcross the analyses, a pattern emerges: the MOE for demographic data is highly sensitive to both the population size of census tracts and the demographic composition within them. Small-population counties, such as Vinton County, show particularly large MOEs due to limited sample sizes in the ACS. Similarly, tracts with highly diverse populations tend to have inflated MOEs across multiple demographic groups, making nearly every group susceptible to high uncertainty in the estimates.\nIn examining equity implications, Black and Hispanic populations face the highest risk of unreliable data. While the largest MOE observed for White populations is around 200%, Black and Hispanic populations frequently have MOEs in the hundreds of percent and, in extreme cases, as high as 2000%. This suggests that algorithmic decisions relying on this demographic data can be highly unreliable for these communities.\nWe can classify the root causes of these data issues into two categories. First, small sample sizes in low-population tracts and counties amplify MOEs for all estimates. Second, demographic variability can take the form of both extremely small minority populations and highly heterogeneous tracts. This further inflates MOE. An example is Census Tract 1711.02, which is nearly 87% Black but flagged as “High MOE” because the MOE for the White population exceeds 40%.\nTo address these systematic issues, decision-makers should incorporate demographic composition into analyses, weighting MOEs appropriately when interpreting the demographic data. Underrepresented communities should be prioritized for follow-up investigations at the tract or county level to better understand them. By acknowledging and adjusting for these data limitations, we can improve the equity and reliability of policy decisions."
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#specific-recommendations",
    "href": "assignments/assignment_1/assignment1.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\n\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\n\n# Format as a professional table with kable()\n\ncounty_data %&gt;%\n  mutate(\n    algorithm_recommendation = case_when(\n      median_income_moe_cat == \"High Confidence: MOE &lt; 5%\"  ~ \"Safe for algorithmic decisions\",\n      median_income_moe_cat == \"Moderate Confidence: MOE 5-10%\" ~ \"Use with caution - monitor outcomes\",\n      median_income_moe_cat == \"Low Confidence: MOE &gt; 10%\" ~ \"Requires manual review or additional data\",\n      TRUE ~ \"Check data\"\n    )\n  ) %&gt;%\n  select(county_name, median_incomeE, median_income_moe_pct, median_income_moe_cat, algorithm_recommendation) %&gt;%\n  kable(\n    caption = \"County-Level Median Income Reliability and Algorithmic Recommendations\",\n    digits = 1,\n    col.names = c(\n      \"County Name\",\n      \"Median Income\",\n      \"MOE % Median Income\",\n      \"Reliability Category\",\n      \"Algorithm Recommendation\"\n    )\n  )\n\n\nCounty-Level Median Income Reliability and Algorithmic Recommendations\n\n\n\n\n\n\n\n\n\nCounty Name\nMedian Income\nMOE % Median Income\nReliability Category\nAlgorithm Recommendation\n\n\n\n\nAdams\n46234\n8.8\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nAllen\n58976\n3.7\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nAshland\n62254\n4.7\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nAshtabula\n53663\n4.1\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nAthens\n48750\n7.6\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nAuglaize\n75231\n3.7\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nBelmont\n56943\n4.2\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nBrown\n66677\n4.6\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nButler\n77062\n2.1\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nCarroll\n59872\n5.9\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nChampaign\n70486\n5.5\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nClark\n58954\n2.6\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nClermont\n79573\n2.9\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nClinton\n64210\n7.1\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nColumbiana\n55473\n4.0\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nCoshocton\n52048\n5.9\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nCrawford\n52486\n4.2\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nCuyahoga\n60074\n1.2\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nDarke\n60237\n6.5\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nDefiance\n69302\n4.8\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nDelaware\n123995\n1.9\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nErie\n65171\n4.4\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nFairfield\n82969\n3.9\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nFayette\n56773\n6.2\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nFranklin\n71070\n1.3\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nFulton\n71453\n4.5\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nGallia\n55533\n6.4\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nGeauga\n97162\n3.9\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nGreene\n81243\n3.0\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nGuernsey\n53901\n7.7\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nHamilton\n68249\n1.6\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nHancock\n67006\n4.6\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nHardin\n55876\n5.1\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nHarrison\n54056\n6.7\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nHenry\n71616\n5.9\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nHighland\n60522\n5.0\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nHocking\n59007\n4.9\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nHolmes\n72987\n4.1\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nHuron\n64144\n4.1\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nJackson\n56549\n7.8\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nJefferson\n53124\n4.4\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nKnox\n71246\n5.2\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nLake\n76835\n1.7\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nLawrence\n51846\n5.2\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nLicking\n78505\n2.8\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nLogan\n69125\n4.8\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nLorain\n67272\n1.7\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nLucas\n57265\n1.7\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nMadison\n77062\n4.7\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nMahoning\n54279\n2.0\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nMarion\n55106\n8.3\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nMedina\n89968\n2.7\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nMeigs\n46255\n6.9\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nMercer\n73278\n4.5\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nMiami\n71440\n3.2\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nMonroe\n55381\n9.8\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nMontgomery\n61942\n1.6\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nMorgan\n51056\n7.8\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nMorrow\n70412\n6.4\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nMuskingum\n56810\n4.9\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nNoble\n51547\n10.8\nLow Confidence: MOE &gt; 10%\nRequires manual review or additional data\n\n\nOttawa\n69515\n5.0\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nPaulding\n65331\n5.7\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nPerry\n62899\n7.3\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nPickaway\n67600\n4.7\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nPike\n50832\n8.4\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nPortage\n69796\n3.2\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nPreble\n66355\n5.9\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nPutnam\n79453\n4.0\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nRichland\n56557\n3.5\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nRoss\n58048\n4.1\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nSandusky\n60814\n5.0\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nScioto\n46360\n3.6\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nSeneca\n62476\n3.9\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nShelby\n73502\n4.6\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nStark\n63130\n1.8\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nSummit\n68360\n1.3\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nTrumbull\n53537\n3.6\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nTuscarawas\n61953\n3.7\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nUnion\n104496\n2.9\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nVanWert\n64841\n5.4\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\nVinton\n50967\n12.3\nLow Confidence: MOE &gt; 10%\nRequires manual review or additional data\n\n\nWarren\n103128\n2.3\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nWashington\n59053\n4.2\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nWayne\n70320\n2.7\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nWilliams\n60632\n3.6\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nWood\n70537\n3.2\nHigh Confidence: MOE &lt; 5%\nSafe for algorithmic decisions\n\n\nWyandot\n68552\n7.6\nModerate Confidence: MOE 5-10%\nUse with caution - monitor outcomes\n\n\n\n\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nCounties suitable for immediate algorithmic implementation: Most Ohio counties are appropriate for immediate algorithmic use; however, some caveats apply. In counties with small or underrepresented populations, certain communities may face higher margins of error, and in highly diverse counties, careful attention or additional algorithmic analysis is needed.\nCounties requiring additional oversight: Adams, Athens, Carroll, Champaign, Clinton, Coshocton, Darke, Fayette, Gallia, Guernsey, Hardin, Harrison, Henry, Highland, Jackson, Knox, Lawrence, Marion, Meigs, Monroe, Morgan, Morrow, Paulding, Perry, Pike, Preble, Sandusky, Van Wert, Wyandot.\n\n\nThese counties can be further categorized based on population and demographic characteristics. In counties with very small populations, targeted local outreach may be sufficient to understand community needs. In larger or more diverse counties, more granular analysis at sub-county levels may be necessary to ensure accurate insights and equitable decision-making.\n\n\nCounties needing alternative approaches: Noble, Vinton.\n\n\nThese counties have some of the lowest populations in Ohio, making traditional ACS estimates highly uncertain. Given the small population size, targeted local outreach—allowing for larger effective sample sizes than the ACS—may be the most feasible approach to accurately assess community needs."
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#questions-for-further-investigation",
    "href": "assignments/assignment_1/assignment1.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\nThe relationship between demographics and population size in determining MOE warrants further investigation. The current analysis, which applies MOE cutoffs based on either demographic or county-level data, involves tradeoffs and may exclude or underrepresent certain populations. Developing a cutoff that incorporates both population size and demographic characteristics could provide a more accurate and equitable assessment, rather than relying single, uniform metrics."
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#submission-checklist",
    "href": "assignments/assignment_1/assignment1.html#submission-checklist",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\nAll code chunks run without errors\nAll “[Fill this in]” prompts have been completed\nTables are properly formatted and readable\nExecutive summary addresses all four required components\nPortfolio navigation includes this assignment\nCensus API key is properly set\nDocument renders correctly to HTML\n\nRemember: Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/your_file_name.html"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html",
    "href": "weekly-notes/week-04-notes.html",
    "title": "Week 4 Notes",
    "section": "",
    "text": "This week’s class is the last piece of the puzzle before we can start building models: doing GIS/spatial visualizations in R.\nRemember: Spatial-autocorrelation violates independence assumptions! Also, training data may under-represent certain areas.\nVector Data Model (one simplified geometric rep. of the real world)\n\nPoints, Lines, Polygons\nEach of the above will have a geometry (shape + location), and attributes (data about that feature)\n\nCoordinate Reference Systems (CRS)\n\nSince the Earth is round, and maps are flat, we need projections! They can’t preserve area, distance, and angles though.\nAlready, we abstract the Earth’s shape. The Earth is a geoid (imperfect surface), and we approximate it using an ellipsoid. There are multiple ellipsoids that can be used to approximate and some can be better depending on where you live.\nThen we must tie the ellipsoid to the real Earth. This is the datum. Ex: Clarke’s 1866 Ellipsoid, centered on Meade’s Ranch, KS.\nThen we can put down a Lat/Long grid. Ex: The North American Datum of 1927 (NAD27) was formed from Clarke’s work.\nNAD83 is another 3D model of the earth that was mapped and uses GRS80.\nWGS84 is what GPS uses.\nAll of the above standards are Geographic Coordinate Systems (GCS).\nThese must then be projected onto a computer screen/flat surface! Recall example of cylindrical projections and the corresponding distortions (Mercator is an example). Can remember how projections mess with things with SADD (Shape, Area, Distance, Direction)\nWe then use Project Coordinate System to provide a localized coordinate system built on a regular non-distorted grid (as opposed to lat/long whihc are in degrees with respect to center of the earth).\n\nOne is UTM, dividing the earth into 60 zones, each of 6 degrees of long. Each zone is projected individually, and is referenced using the Zone (because coordinates are defined from the bottom-left origin of each zone). No negatives, and in meters (with some false northing or easting value added).\nAnother is State Plane (in the US, used in PA for example). Each state has its own projection, reducing distortions at the state level. Similar system as UTM where there’s an origin corner, but the distance can be feet or meters.\n\nGCS (lat/long coordinates, decimal degrees, good for global datasets + web mapping, bad for area/distance)\nPCS ()\nSee below for programmatic considerations"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#key-concepts-learned-notes",
    "href": "weekly-notes/week-04-notes.html#key-concepts-learned-notes",
    "title": "Week 4 Notes",
    "section": "",
    "text": "This week’s class is the last piece of the puzzle before we can start building models: doing GIS/spatial visualizations in R.\nRemember: Spatial-autocorrelation violates independence assumptions! Also, training data may under-represent certain areas.\nVector Data Model (one simplified geometric rep. of the real world)\n\nPoints, Lines, Polygons\nEach of the above will have a geometry (shape + location), and attributes (data about that feature)\n\nCoordinate Reference Systems (CRS)\n\nSince the Earth is round, and maps are flat, we need projections! They can’t preserve area, distance, and angles though.\nAlready, we abstract the Earth’s shape. The Earth is a geoid (imperfect surface), and we approximate it using an ellipsoid. There are multiple ellipsoids that can be used to approximate and some can be better depending on where you live.\nThen we must tie the ellipsoid to the real Earth. This is the datum. Ex: Clarke’s 1866 Ellipsoid, centered on Meade’s Ranch, KS.\nThen we can put down a Lat/Long grid. Ex: The North American Datum of 1927 (NAD27) was formed from Clarke’s work.\nNAD83 is another 3D model of the earth that was mapped and uses GRS80.\nWGS84 is what GPS uses.\nAll of the above standards are Geographic Coordinate Systems (GCS).\nThese must then be projected onto a computer screen/flat surface! Recall example of cylindrical projections and the corresponding distortions (Mercator is an example). Can remember how projections mess with things with SADD (Shape, Area, Distance, Direction)\nWe then use Project Coordinate System to provide a localized coordinate system built on a regular non-distorted grid (as opposed to lat/long whihc are in degrees with respect to center of the earth).\n\nOne is UTM, dividing the earth into 60 zones, each of 6 degrees of long. Each zone is projected individually, and is referenced using the Zone (because coordinates are defined from the bottom-left origin of each zone). No negatives, and in meters (with some false northing or easting value added).\nAnother is State Plane (in the US, used in PA for example). Each state has its own projection, reducing distortions at the state level. Similar system as UTM where there’s an origin corner, but the distance can be feet or meters.\n\nGCS (lat/long coordinates, decimal degrees, good for global datasets + web mapping, bad for area/distance)\nPCS ()\nSee below for programmatic considerations"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#coding-techniquestechnical-notes",
    "href": "weekly-notes/week-04-notes.html#coding-techniquestechnical-notes",
    "title": "Week 4 Notes",
    "section": "Coding Techniques/Technical Notes",
    "text": "Coding Techniques/Technical Notes\n\nsf (simple features) Package\n\nAllows you to read in multiple types of spatial data (Shapefiles, GeoJSON, KML/KMZ, DB connections, etc.) as an sf (similar to how we read csv data as df or tibbles)\n\ntigris Package\n\nContains shape data for the census (as opposed to tidycensus which contains attribute data)\nst_drop_geometry: will drop the geometry column from your table, so you just have attribute data.\n\nYou can use ggplot() to map using + geom_sf()\nYou can filter your data using st_filter(), and add the argument of .predicate = relationshop, where relationship can be something like st_touches, st_intersects, etc.\nAlso important st_ functions, like st_dissolve!\nChecking and Setting CRS\n\nuse st_crs(data) to check the current CRS\niff missing, st_set_crs(data, crs_id).\n\nIFF!!!\n\ntransform CRS using data_transformed &lt;- data %&gt;% st_transform(crs = id)"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#questions-challenges",
    "href": "weekly-notes/week-04-notes.html#questions-challenges",
    "title": "Week 4 Notes",
    "section": "## Questions & Challenges",
    "text": "## Questions & Challenges"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#connections-to-policy",
    "href": "weekly-notes/week-04-notes.html#connections-to-policy",
    "title": "Week 4 Notes",
    "section": "## Connections to Policy",
    "text": "## Connections to Policy"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#reflection",
    "href": "weekly-notes/week-04-notes.html#reflection",
    "title": "Week 4 Notes",
    "section": "## Reflection",
    "text": "## Reflection"
  }
]